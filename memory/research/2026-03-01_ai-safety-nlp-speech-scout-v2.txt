Below is a curated “last‑12‑months” (≈ **Feb 28, 2025 → Feb 28, 2026**, Asia/Taipei) paper scout for **AI Safety × NLP/Speech** across your 8 topics, prioritizing **arXiv + top venues** (NeurIPS/ICLR/ICML/ACL/EMNLP/Interspeech + workshops).

Notes:

* “Speech LLM / Audio LLM / LALM / ALLM” naming varies; I’m grouping by **end‑to‑end speech/audio ↔ text models** and **speech‑enabled agent stacks**.
* Where a topic is still sparse, I still give you the best matches in-window and call out gaps.

---

## 1) Speech jailbreak / audio prompt injection (5–10)

| Title                                                                                                                       |                        Venue / Date | Link                               | 2‑sentence summary                                                                                                                                                                                                                                                | Method type | Why it matters for AI safety                                                                                                                  | Limitations / gaps                                                                                                                                    |
| --------------------------------------------------------------------------------------------------------------------------- | ----------------------------------: | ---------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------- | --------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
| AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models                                            |                arXiv (May 20, 2025) | arXiv ([arXiv][1])                 | Proposes an audio jailbreak emphasizing **asynchrony**, **universality**, **stealth**, and **over‑the‑air robustness** for end‑to‑end audio‑language models. Shows many text jailbreaks don’t transfer cleanly via TTS, motivating audio-native threat models.    | attack      | Demonstrates **realistic “played audio” jailbreaks** that don’t require full prompt control—closer to real world voice assistant abuse.       | Primarily targets specific model families and controlled setups; defense implications need systematic head‑to‑head comparisons across architectures.  |
| When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs (WhisperInject)                      | arXiv (Aug 5, 2025; v3 Feb 4, 2026) | arXiv ([arXiv][2])                 | Introduces a two‑stage adversarial audio pipeline: first elicit a harmful “native” response, then **inject** it into benign carrier audio via imperceptible perturbations. Evaluates across multiple audio-language models with automated + human safety checks.  | attack      | Makes the case that **benign-sounding audio can carry malicious intent**, stressing deployment risk for always-on audio interfaces.           | Assumes attacker can craft/optimize adversarial audio; real-device/channel robustness and detection at scale remain open.                             |
| Now You Hear Me: Audio Narrative Attacks Against Large Audio‑Language Models                                                |                arXiv (Jan 30, 2026) | arXiv ([arXiv][3])                 | Builds “text‑to‑audio jailbreaks” that embed disallowed directives in **narrative-style speech**, leveraging delivery/structure rather than only noise perturbations. Reports strong jailbreak success on modern models under this stylized format.               | attack      | Shows **content framing + prosody-style delivery** can change safety outcomes—guardrails tuned on text prompts may fail for speech.           | Evaluation depends on TTS generation quality and chosen prompts; needs broader coverage (languages, accents, live streaming).                         |
| Breaking Audio Large Language Models by Attacking Only the Encoder: A Universal Targeted Latent‑Space Audio Attack (U‑TLSA) |                arXiv (Dec 29, 2025) | arXiv ([arXiv][4])                 | Demonstrates that attacking only the **audio encoder** can yield universal targeted behavior like command injection / behavioral hijacking. Uses a single perturbation that generalizes across many carriers/speakers.                                            | attack      | Highlights that **encoder access** (or vulnerabilities) can compromise “downstream aligned” systems—important for modular speech stacks.      | Threat model may require gradient/encoder access; practical feasibility varies by deployment and model exposure.                                      |
| Universal Acoustic Adversarial Attacks for Flexible Control of Speech‑LLMs                                                  |      Findings EMNLP 2025 (Nov 2025) | ACL Anthology ([ACL Anthology][5]) | Presents universal acoustic adversarial signals that steer speech‑LLMs toward attacker objectives with flexible control. Evaluates on speech‑LLM setups that combine speech encoders and LLMs.                                                                    | attack      | Expands classic adversarial audio into the **speech→LLM** era, showing safety/alignment can be bypassed at the acoustic layer.                | Generalization across microphones/rooms and defenses (denoising, separation, robust encoders) needs deeper study.                                     |
| Defending Speech-enabled LLMs Against Adversarial Jailbreak Threats                                                         |         Interspeech 2025 (Aug 2025) | ISCA PDF ([ISCA Archive][6])       | Proposes adversarial training using synthetic harmful speech to harden speech‑enabled LLMs against perturbation-based jailbreak threats. Shows small amounts of carefully curated harmful speech can substantially improve robustness.                            | defense     | One of the few **speech-domain defenses** positioned for practical adoption (training recipe + synthetic data).                               | Adversarial training is expensive and can overfit to attack families; needs transfer tests to novel audio jailbreak styles (narrative/compositional). |
| SPIRIT: Patching Speech Language Models against Jailbreak Attacks                                                           |               EMNLP 2025 (Nov 2025) | ACL Anthology ([ACL Anthology][7]) | Shows SLMs can be highly vulnerable to jailbreaks, then proposes **post‑hoc activation patching** at inference to block attacks with minimal utility loss. Reports large robustness gains without retraining.                                                     | defense     | Strong example of **safety intervention without full RLHF retraining**, attractive for deployed voice assistants.                             | Requires identifying/patching the right internal states; may be brittle across model updates or unseen attack distributions.                          |
| JALMBench: Benchmarking Jailbreak Vulnerabilities in Audio Language Models                                                  |                arXiv (May 23, 2025) | arXiv ([arXiv][8])                 | Introduces a large benchmark spanning many ALMs, attacks (text‑transferred + audio‑originated), and defenses; includes extensive analysis over voices and topics. Provides a unifying evaluation scaffold for audio jailbreak research.                           | eval        | Helps prevent “single‑model cherry-picking” and anchors progress on audio jailbreak robustness.                                               | Benchmark scope still bounded by curated attacks and model list; needs more real-world over-the-air, multi-speaker, and streaming scenarios.          |
| Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large Audio‑Language Models (AJailBench)                  |                arXiv (May 21, 2025) | arXiv ([arXiv][9])                 | Builds a jailbreak benchmark for audio prompts (TTS‑converted from text jailbreaks) and adds a perturbation toolkit to create harder variants under semantic constraints. Evaluates leading audio‑language models and highlights fragility to subtle distortions. | eval        | Establishes repeatable measurement for **speech jailbreak regression testing** (including “perturbed but semantically same” audio).           | Reliance on TTS for many prompts may miss human speech variability; benchmark may lag new jailbreak styles (narrative/compositional).                 |
| Speech‑Audio Compositional Attacks on Multimodal LLMs + SALMONN‑Guard (SACRED‑Bench)                                        |                arXiv (Nov 13, 2025) | arXiv ([arXiv][10])                | Introduces compositional attacks via **speech overlap**, **speech+non‑speech mixtures**, and multi-speaker formats; proposes SALMONN‑Guard to reduce attack success. Shows even strong proprietary models can be vulnerable under these audio compositions.       | attack      | Moves beyond “single clean utterance” and probes **realistic acoustic scenes** (overlap/background audio) that speech assistants must handle. | Attack space is vast; guard model evaluation across languages, accents, and streaming latency constraints remains limited.                            |

---

## 2) Multimodal deception / scheming in speech‑text models (5–10)

*(This is still relatively sparse in “speech‑native” form; most work is either (a) manipulation attacks in speech, (b) red‑teaming of voice agents, or (c) deception evaluation/detection in general agents.)*

| Title                                                                                                          |            Venue / Date | Link                                | 2‑sentence summary                                                                                                                                                                                                                                                                               | Method type | Why it matters for AI safety                                                                                                  | Limitations / gaps                                                                                                                               |
| -------------------------------------------------------------------------------------------------------------- | ----------------------: | ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------- | ----------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |
| Benchmarking Gaslighting Attacks Against Speech Large Language Models                                          |    arXiv (Sep 24, 2025) | arXiv ([arXiv][11])                 | Defines “gaslighting” as manipulation strategies (anger, sarcasm, cognitive disruption, etc.) to degrade model reasoning/behavior in speech settings, and evaluates across speech/multimodal LLMs. Reports substantial performance degradation under these manipulative prompts.                 | attack      | Directly targets the **human‑facing trust channel** in speech—where persuasion and tone can override safe behavior.           | Focuses on accuracy drops/behavioral shifts; needs downstream harm modeling (e.g., compliance, policy violations) and defenses beyond detection. |
| Aegis: Towards Governance, Integrity, and Security of AI Voice Agents                                          |     arXiv (Feb 7, 2026) | arXiv ([arXiv][12])                 | Proposes a red‑teaming framework for real-world voice agents (banking/IT/logistics) grounded in realistic adversary tactics and multi‑turn conversational dynamics. Finds open‑weight models tend to be more susceptible and recommends layered defenses (access control + monitoring + policy). | eval        | Bridges “model-level safety” to **deployment reality**, where deception/social engineering is often the primary threat.       | Emphasizes scenario evaluation; still needs standardized benchmarks and reproducible agent stacks so results generalize across vendors.          |
| Benchmarking and Investigating AI Deceptive Behaviors in LLM‑based Agents (OpenDeception)                      |        arXiv (Apr 2025) | arXiv PDF ([arXiv][13])             | Introduces an open-ended deception evaluation framework that inspects intent/capability of agent deception across scenarios. Though not speech-specific, the framework maps well onto voice agents where deception can be amplified by paralinguistics.                                          | eval        | Gives a **measurement scaffold** for “scheming-like” behaviors that can be ported into speech-agent environments.             | Lacks audio-native channels (tone/identity cues) and speech turn-taking dynamics unless extended.                                                |
| Cross‑Modality Jailbreak Transfer in Omni‑Models                                                               |        arXiv (Feb 2026) | arXiv ([arXiv][14])                 | Studies how jailbreaks can transfer across modalities in omni models, implying attacks can originate in one channel (e.g., audio) and manifest in another. Highlights that “safety in one modality” does not guarantee safety in the full multimodal system.                                     | eval        | A core “scheming surface” in multimodal assistants: cross-channel transfer enables stealthy attack chaining.                  | Often tests limited model sets and stylized prompts; needs real-time speech + tool-use agent settings.                                           |
| Cloning a Conversational Voice AI Agent from Call Recordings using Prompt Engineering and Targeted Fine‑Tuning |        arXiv (Sep 2025) | arXiv PDF ([arXiv][15])             | Demonstrates cloning a persuasive conversational agent from call recordings via prompt engineering and targeted fine‑tuning in a telesales domain. Raises concerns about scalable creation of convincing voice agents for manipulation.                                                          | eval        | Connects model capability to **misuse scaling** (e.g., scam/persuasion at voice-channel bandwidth).                           | Primarily capability-focused; lacks systematic safety mitigations and abuse-resistant deployment guidance.                                       |
| Evaluation of the Deception Detection Capabilities of LLMs                                                     | ACL 2025 (Jul/Aug 2025) | ACL Anthology ([ACL Anthology][16]) | Evaluates how well LLMs detect deception across datasets and prompting styles, including multimodal features in some settings. Useful as a baseline for building “deception monitors” for speech agents.                                                                                         | eval        | Deception detection is a plausible component of **scalable oversight** for voice assistants interacting with untrusted users. | Detection ≠ prevention; speech-native signals and real-time conversation constraints need explicit integration.                                  |
| SVC 2025: The First Multimodal Deception Detection Challenge                                                   |        arXiv (Aug 2025) | arXiv ([arXiv][17])                 | Introduces a multimodal deception detection challenge/benchmark, supporting evaluation of models that must infer deceptive behavior from multimodal cues. Provides data/task framing for deception monitoring that can be adapted to voice-agent oversight.                                      | eval        | Supplies a concrete dataset handle for “deception monitoring,” a safety-relevant capability for voice agents.                 | Not specific to speech-text assistants; may focus on audiovisual cues rather than LALM conversational deception.                                 |

---

## 3) Scalable oversight for speech agents (5–10)

| Title                                                                                                                 |                      Venue / Date | Link                          | 2‑sentence summary                                                                                                                                                                                                                                                                      | Method type | Why it matters for AI safety                                                                                                                   | Limitations / gaps                                                                                                                                |
| --------------------------------------------------------------------------------------------------------------------- | --------------------------------: | ----------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------- | ---------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- |
| Aligning Spoken Dialogue Models from User Interactions                                                                |    ICML 2025 (arXiv Jun 26, 2025) | arXiv ([arXiv][18])           | Builds a large preference dataset from raw multi‑turn speech conversations labeled with AI feedback, then uses offline alignment to fine‑tune a full‑duplex speech‑to‑speech model. Reports improvements in factuality, safety, and contextual alignment beyond single-turn evaluation. | defense     | Strong template for **scalable preference data** in speech and an alignment recipe for speech-to-speech systems.                               | AI feedback quality and reward hacking risk need more auditability; cross-lingual and adversarial robustness are not the main focus.              |
| Process‑Supervised Reinforcement Learning for Interactive Multimodal Tool‑Use Agents (TARL)                           | OpenReview / arXiv (Sep 17, 2025) | OpenReview ([OpenReview][19]) | Introduces turn-level adjudication using an LLM judge for credit assignment in long-horizon tool-use; supports speech-based user simulation in an RL sandbox. Demonstrates training gains including a multimodal voice agent on interleaved speech‑text interactions.                   | defense     | A concrete route to **process supervision** for speech agents that must call tools—key for scalable oversight beyond RLHF on static responses. | Judge reliability and reward hacking remain concerns; “speech realism” in simulation and transfer to real users/devices needs further validation. |
| LALM‑as‑a‑Judge: Benchmarking Large Audio‑Language Models for Safety Evaluation in Multi‑Turn Spoken Dialogues        |               arXiv (Feb 4, 2026) | arXiv ([arXiv][20])           | Creates 24k multi-turn spoken dialogues with controlled unsafe content and evaluates LALMs as safety judges across audio-only, transcript-only, and multimodal inputs. Finds transcription quality is a bottleneck and highlights trade-offs between sensitivity and stability.         | eval        | Establishes an audio-native path to **LLM-as-a-judge oversight** for voice conversations (including ASR error effects).                        | Synthetic dialogues may miss real conversational drift; extending to multilingual, real audio, and tool-use dialogues is necessary.               |
| AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models                              |              arXiv (May 22, 2025) | arXiv ([arXiv][21])           | Proposes a trustworthiness benchmark across fairness, hallucination, safety, privacy, robustness, and authentication with audio-specific metrics and scenarios. Uses an automated evaluation pipeline for scalable scoring.                                                             | eval        | Helps define what “trustworthy” means for audio-native assistants and enables scalable regression testing.                                     | Automated scoring can miss subtle harms and context; needs stress tests for real-time streaming and agentic tool-use settings.                    |
| Speech‑Hands: A Self‑Reflection Voice Agentic Approach to Speech Recognition and Audio Reasoning with Omni Perception |              arXiv (Jan 14, 2026) | arXiv ([arXiv][22])           | Introduces a self-reflection mechanism that decides when to trust internal perception vs consult external audio perception, motivated by findings that naïve multi-task fine‑tuning can degrade performance. Shows improvements on ASR and audio reasoning benchmarks.                  | defense     | “Know‑when‑you‑don’t‑know” is a practical oversight primitive for speech agents that must act under noisy perception.                          | Focused on perception reliability; needs explicit integration with safety policy compliance and adversarial user behavior.                        |
| SARSteer: Safeguarding Large Audio Language Models via Safe‑Ablated Refusal Steering                                  |              arXiv (Oct 20, 2025) | arXiv ([arXiv][23])           | Proposes inference-time refusal steering adapted to LALMs, with “safe-space ablation” to reduce over‑refusal on benign speech queries. Targets the mismatch between text-derived steering and audio activation distributions.                                                           | defense     | Attractive as **deployable oversight**: inference-time control without full retraining, aimed at reducing over‑refusal (a key product risk).   | Likely brittle to distribution shift and evolving jailbreaks; rigorous eval under realistic acoustic scenes and streaming is needed.              |
| VoiceAgentBench: Are Voice Assistants ready for agentic tasks?                                                        |               arXiv (Oct 9, 2025) | arXiv ([arXiv][24])           | Introduces a benchmark for tool-using voice assistants with multilingual/cultural coverage (Indic languages) and includes safety/adversarial robustness evaluation. Reveals gaps in multi-tool orchestration, multi-turn handling, and robustness.                                      | eval        | Gives an actionable harness for **oversight of speech agents** that take actions (tools) rather than only answer questions.                    | Heavy use of synthetic speech and tool schemas; needs real user audio, realistic latency constraints, and stronger adversary models.              |
| Reject or Not?: A Benchmark for Voice Assistant Query Rejection in Smart Home Scenario (Chinese)                      |              arXiv (Dec 11, 2025) | arXiv ([arXiv][25])           | Releases a Chinese multimodal dataset for deciding whether to accept/reject smart-home queries, including ASR-error and non-speech cases, plus a personalization approach. Provides structured labels and context for evaluation and fine-tuning.                                       | eval        | Rejection is a critical safety gate in home assistants; benchmarking it in non-English settings reduces blind spots.                           | Domain-specific (smart home) and language-specific; generalization to open-domain voice agents and multi-tool environments remains open.          |

---

## 4) Mechanistic interpretability for speech LLMs (5–10)

| Title                                                                                                    |                   Venue / Date | Link                     | 2‑sentence summary                                                                                                                                                                                                                                 | Method type      | Why it matters for AI safety                                                                                                              | Limitations / gaps                                                                                                                    |
| -------------------------------------------------------------------------------------------------------- | -----------------------------: | ------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------- | ----------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |
| AR&D: A Framework for Retrieving and Describing AudioLLM Behaviors and Mechanisms                        |           arXiv (Feb 27, 2026) | arXiv ([arXiv][26])      | Proposes a mechanistic interpretability pipeline for AudioLLMs, extending sparse autoencoder-style tooling to audio-language internals to retrieve and describe behaviors. Aims to connect observable behaviors with internal features/mechanisms. | interpretability | Mechanistic handles are prerequisites for **auditable safety interventions** (e.g., “remove jailbreak feature without breaking utility”). | Early-stage tooling; needs replicated results across architectures and clear causal validations (intervention, not just correlation). |
| Beyond Transcription: Mechanistic Interpretability in ASR                                                |           arXiv (Aug 20, 2025) | arXiv ([arXiv][27])      | Applies mechanistic interpretability ideas to ASR beyond simple transcription accuracy, likely probing internal pathways that govern errors and behaviors. Frames interpretability as a lens for improving reliability in speech processing.       | interpretability | Many speech-agent failures originate in ASR; understanding internal error modes supports safer downstream behavior.                       | Focus is ASR-centric, not end-to-end speech-to-speech agents; linking findings to safety policy outcomes is still indirect.           |
| Behind the Scenes: Mechanistic Interpretability of LoRA‑adapted Whisper for Speech Emotion Recognition   |           arXiv (Sep 10, 2025) | arXiv ([arXiv][28])      | Studies how LoRA adaptation changes Whisper internals for emotion recognition, using mechanistic interpretability tools to localize behavioral changes. Useful for understanding adaptation-induced shifts.                                        | interpretability | Safety tuning and task adaptation can introduce hidden behavior changes; interpretability can detect unintended shifts.                   | Task-specific (SER) and model-specific (Whisper); generalizing to LALMs and safety alignment is nontrivial.                           |
| AudioLens: A Closer Look at Auditory Attribute Perception in Large Audio‑Language Models                 |            arXiv (Jun 5, 2025) | arXiv ([arXiv][29])      | Uses analysis/probing (e.g., “lens” techniques) to study how LALMs represent and use auditory attributes. Helps map what parts of the model carry which auditory concepts.                                                                         | interpretability | Understanding “what the model hears” is core to building robust safety filters and preventing jailbreak triggers via acoustic cues.       | Often correlational; needs causal interventions and explicit links to safety-critical failure modes.                                  |
| Demystifying Representation Spaces of Multilingual and Multimodal Aspects in Large Audio Language Models | OpenReview workshop (Oct 2025) | OpenReview ([arXiv][29]) | Probes how multilingual/multimodal information is organized in LALM representation spaces, helping explain cross-lingual or cross-modal behavior. Provides empirical characterization of embeddings/latent structure.                              | interpretability | Cross-lingual safety failures often arise from representation mismatch; this helps target where to intervene.                             | Workshop scope; may not include causal tests or safety-specific datasets/metrics.                                                     |
| On the Reliability of Feature Attribution Methods for Speech Classification                              |           arXiv (May 23, 2025) | arXiv ([arXiv][30])      | Evaluates whether attribution/saliency methods for speech models are stable and trustworthy under speech classification settings. Highlights failure modes of interpretability tools themselves.                                                   | interpretability | Safety teams rely on interpretability for audits; unreliable attributions can mislead mitigation.                                         | Not speech LLM-specific; extends most directly to audio encoders/classifiers rather than speech-to-speech agents.                     |
| Mechanistic Interpretability of Brain‑to‑Speech Models                                                   |            arXiv (Feb 2, 2026) | arXiv ([arXiv][31])      | Applies mechanistic interpretability methods to brain-to-speech decoding models, aiming to explain internal mappings from neural signals to speech outputs. Demonstrates interpretability in a high-stakes speech generation domain.               | interpretability | Shows interpretability can work in **safety-critical speech generation** contexts, relevant to future neuro/assistive speech agents.      | Not directly about LALMs; transfer to audio-language reasoning and jailbreak behaviors is indirect.                                   |

---

## 5) Safety benchmarks for audio‑native assistants (5–10)

| Title                                                                                    |                              Venue / Date | Link                                | 2‑sentence summary                                                                                                                                                                                                                                 | Method type | Why it matters for AI safety                                                                                                           | Limitations / gaps                                                                                             |
| ---------------------------------------------------------------------------------------- | ----------------------------------------: | ----------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------- | -------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- |
| AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models |                      arXiv (May 22, 2025) | arXiv ([arXiv][21])                 | Defines a multifaceted benchmark spanning safety, privacy, robustness, authentication, fairness, and hallucination in audio scenarios. Includes audio-specific metrics and real-world-like setups.                                                 | eval        | Broad trustworthiness coverage is needed for audio assistants deployed in sensitive settings (calls, emergencies, home).               | Automated judges and curated scenarios may miss open-world distribution shift.                                 |
| Audio Jailbreak: AJailBench                                                              |                      arXiv (May 21, 2025) | arXiv ([arXiv][9])                  | Provides a jailbreak benchmark for large audio-language models plus a perturbation toolkit to stress models under semantically consistent distortions. Enables systematic comparison across models.                                                | eval        | Establishes a de facto baseline for **audio jailbreak regression testing**.                                                            | Many prompts are TTS-derived; needs more natural conversational audio and streaming evaluations.               |
| JALMBench                                                                                |                      arXiv (May 23, 2025) | arXiv ([arXiv][8])                  | Large-scale benchmark with many ALMs, attacks, and defenses, enabling analysis by voice diversity and topic sensitivity. Offers a unifying framework for jailbreak safety evaluation.                                                              | eval        | Helps standardize evaluation so claims about “robust” audio models are comparable.                                                     | Benchmark design may lag emerging attack types (narrative/compositional/agentic).                              |
| Jailbreak‑AudioBench: In‑Depth Evaluation and Analysis of Audio Jailbreak                | NeurIPS 2025 (Datasets/Benchmarks poster) | NeurIPS ([NeurIPS][32])             | Presents a comprehensive audio jailbreak benchmark dataset and evaluates state-of-the-art LALMs. Positions the benchmark as a foundation for more powerful future jailbreak threats and defenses.                                                  | eval        | NeurIPS visibility + benchmark framing accelerates community convergence on shared metrics.                                            | Details depend on dataset scope; needs long-horizon conversational and tool-use speech settings.               |
| Omni‑SafetyBench: Safety Evaluation of Audio‑Visual Large Language Models                |                      arXiv (Aug 10, 2025) | arXiv ([arXiv][33])                 | First parallel benchmark for omni models across many modality combinations, introducing safety and cross‑modal consistency metrics. Finds models can be safe in one modality but unsafe/inconsistent in others.                                    | eval        | Audio assistants increasingly integrate vision; safety must be tested under **joint audio‑visual** inputs and consistency constraints. | Not speech-only; expanding to tool-use voice agents and real-time streaming remains needed.                    |
| SACRED‑Bench + SALMONN‑Guard                                                             |                      arXiv (Nov 13, 2025) | arXiv ([arXiv][10])                 | Red-teaming benchmark focused on complex audio scenes (overlap, mixtures, multi-speaker), plus an audio-aware guard model that reduces attack success. Tests cross‑modal speech+audio compositions.                                                | eval        | Benchmarks “messy real audio” that production assistants face, not clean single-utterance prompts.                                     | Benchmark breadth can explode; needs standardized subsets and multilingual expansion.                          |
| WildSpeech‑Bench: Benchmarking Audio LLMs in Natural Speech Conversation                 |                      arXiv (Jun 27, 2025) | arXiv ([arXiv][34])                 | Builds a benchmark targeting natural spoken conversation phenomena (prosody, homophones, stuttering) and proposes query-aware evaluation checklists. Highlights large differences across speech models in realistic scenarios.                     | eval        | Many safety failures are triggered by **misunderstanding**; robust natural conversation evaluation is a prerequisite to safety.        | Focuses on overall conversational quality; needs explicit safety policy evaluation and adversarial conditions. |
| LALM‑as‑a‑Judge                                                                          |                       arXiv (Feb 4, 2026) | arXiv ([arXiv][20])                 | Benchmark for using LALMs as safety judges in multi-turn spoken dialogue with controlled unsafe insertions and severity scales. Shows transcription quality strongly affects judge sensitivity.                                                    | eval        | Offers scalable audio-native safety scoring, crucial for continuous evaluation of voice assistants.                                    | Synthetic dialogues; needs real conversational audio, multilingual and code-switching.                         |
| MULTIVOX: Benchmark for Voice Assistants for Multimodal Interactions                     |                     EMNLP 2025 (Nov 2025) | ACL Anthology ([ACL Anthology][35]) | Provides a benchmark of recorded dialogues focusing on paralinguistic cues (pitch, emotion, timbre, volume) plus visual context for omni voice assistants. Targets multimodal understanding beyond transcript text.                                | eval        | Safety often depends on tone/emotion; MultiVox makes it measurable instead of ignoring paralinguistics.                                | Not a safety benchmark per se; needs safety-policy labeling and attack suites.                                 |
| Reject or Not? (Smart‑home query rejection, Chinese)                                     |                      arXiv (Dec 11, 2025) | arXiv ([arXiv][25])                 | Provides the first Chinese-oriented benchmark for smart-home query rejection with multimodal text-speech pairs and fine-grained dialogue types including ASR-error. Proposes a personalized rejection architecture using dialogue history and RAG. | eval        | Rejection gating is safety-critical; this benchmark advances non-English audio assistant reliability.                                  | Domain-specific; transferability to open-domain voice assistants and tool-use agents is unclear.               |

---

## 6) Cross‑lingual safety robustness in speech systems (5–10)

| Title                                                                                      |                       Venue / Date | Link                                | 2‑sentence summary                                                                                                                                                                                                       | Method type      | Why it matters for AI safety                                                                                         | Limitations / gaps                                                                                                |
| ------------------------------------------------------------------------------------------ | ---------------------------------: | ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------- | -------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- |
| Multilingual and Multi‑Accent Jailbreaking of Audio LLMs                                   |                arXiv (Apr 1, 2025) | arXiv PDF ([arXiv][31])             | Studies jailbreak vulnerability of audio LLMs across **languages and accents**, showing diversity in linguistic/audio inputs can amplify vulnerabilities. Motivates cross-lingual threat modeling for speech assistants. | attack           | Real assistants must handle accents + multilingual speech; safety robustness cannot be English-only.                 | Needs broader device/channel evaluation and defenses; also extends beyond jailbreak to misinformation/persuasion. |
| Code‑Switching Red‑Teaming: LLM Evaluation for Safety (text-focused but directly relevant) |            ACL 2025 (Jul/Aug 2025) | ACL Anthology ([arXiv][36])         | Evaluates safety under code-switching prompts, highlighting failures when languages are mixed. Provides red-teaming patterns that can be ported to speech via TTS/real recordings.                                       | eval             | Code-switching is common in speech; safety policies often degrade under mixed-language inputs.                       | Not audio-native; speech-specific phenomena (accent, ASR errors, prosody) remain untested.                        |
| Lost in Translation? A Comparative Study on Cross‑Lingual Safety Transfer                  |                arXiv (Feb 8, 2026) | arXiv ([NeurIPS][32])               | Compares how safety behaviors transfer across languages and identifies translation-related safety distortions. Highlights failure modes when relying on translation layers for moderation.                               | eval             | Many voice systems translate speech→English→policy; this quantifies risks of that pipeline.                          | Needs speech-native validation (ASR + translation compounding errors) and real conversational contexts.           |
| Who Transfers Safety? Identifying and Targeting Cross‑Lingual Safety Mechanisms            |                arXiv (Feb 2, 2026) | arXiv ([arXiv][37])                 | Studies cross-lingual safety transfer mechanisms and proposes targeting strategies for improving safety across languages. Aims to pinpoint which components carry safety behaviors between languages.                    | interpretability | Supports more principled multilingual safety alignment and diagnosis of where safety “drops.”                        | Primarily text-level; coupling with speech encoders and ASR noise remains an open extension.                      |
| Enforcing Multilingual Consistency for LLM Safety Alignment                                |               arXiv (Feb 18, 2026) | arXiv ([arXiv][30])                 | Proposes alignment methods that enforce consistent safety behavior across languages. Addresses the “safe in English, unsafe elsewhere” problem.                                                                          | defense          | Essential for global deployment of speech assistants; inconsistency is a major regulatory and ethical risk.          | Needs end-to-end speech evaluation and multilingual spoken datasets with realistic accents/code-switching.        |
| Multilingual Safety Alignment via Sparse Weight Editing                                    |               arXiv (Feb 26, 2026) | arXiv ([arXiv][28])                 | Uses sparse edits to improve multilingual safety alignment, aiming for targeted changes with less collateral damage. Positioned as an efficient alignment approach.                                                      | defense          | If it transfers to speech stacks, it could reduce the cost of multilingual safety upgrades.                          | Speech stacks add acoustic variability; effects under ASR errors and spoken dialogue remain unknown.              |
| Multilingual Blending: Safety Alignment Evaluation with Language Mixture                   | NAACL 2025 Findings (Apr/May 2025) | ACL Anthology ([arXiv][27])         | Evaluates safety alignment when languages are mixed, modeling realistic user language blending. Highlights risks of “partial translation” and mixed-language prompts.                                                    | eval             | Spoken conversation often blends languages; this suggests how to design multilingual safety tests for speech agents. | Text-first; should be replicated with speech input and ASR/translation artifacts.                                 |
| The State of Multilingual LLM Safety Research                                              |              EMNLP 2025 (Nov 2025) | ACL Anthology ([ACL Anthology][38]) | Surveys/organizes current multilingual safety research and gaps, including language coverage and evaluation issues. Useful orientation to avoid reinventing “English-only safety.”                                       | eval             | Provides a roadmap for where multilingual safety is failing and where benchmarks are missing.                        | Not speech-specific; speech adds accent/ASR/phonetic attack surfaces.                                             |
| Minimizing the Reward Gap: A Unified Approach to LLM Safety Alignment                      |               arXiv (May 24, 2025) | arXiv ([arXiv][29])                 | Proposes alignment improvements by minimizing reward gaps, relevant when safety tuning causes uneven behavior across settings. Can be adapted to multilingual + speech domains as a mitigation strategy.                 | theory           | Could reduce “alignment tax” that differentially impacts non-English and speech settings.                            | Not audio-native; needs explicit tests for multilingual speech assistants and streaming constraints.              |

---

## 7) ASR‑mediated attack surface (5–10)

| Title                                                                            |         Venue / Date | Link                                    | 2‑sentence summary                                                                                                                                                                                       | Method type | Why it matters for AI safety                                                                                             | Limitations / gaps                                                                                                    |
| -------------------------------------------------------------------------------- | -------------------: | --------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------- | ------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------- |
| Breaking Audio LLMs by Attacking Only the Encoder (U‑TLSA)                       | arXiv (Dec 29, 2025) | arXiv ([arXiv][4])                      | Shows that manipulating encoder latents can drive targeted transcripts/commands without touching the language decoder. Emphasizes the vulnerability of the “ASR/encoder front-end” in modular systems.   | attack      | Many deployments treat the encoder/ASR as “just preprocessing”; this shows it can be the **primary attack surface**.     | Depends on access assumptions; needs black-box, over-the-air validations.                                             |
| Universal Acoustic Adversarial Attacks for Flexible Control of Speech‑LLMs       |  Findings EMNLP 2025 | ACL Anthology ([ACL Anthology][5])      | Universal acoustic perturbations can steer speech‑LLMs, effectively exploiting the speech recognition/encoding pathway. Demonstrates control beyond simple transcription corruption.                     | attack      | Shows ASR-mediated systems can be subverted even if the downstream LLM is aligned.                                       | Requires deeper study of robust front-ends (separation, robust SSL encoders) and deployment constraints.              |
| SPIRIT: Patching Speech Language Models against Jailbreak Attacks                |           EMNLP 2025 | ACL Anthology ([ACL Anthology][7])      | Identifies strong vulnerability of SLMs to imperceptible noise jailbreaks and proposes inference-time activation patching to mitigate. Emphasizes the vulnerability introduced by richer speech signals. | defense     | Practical mitigation that can sit “after ASR/encoder” to reduce jailbreak success while preserving utility.              | May not address failures caused by ASR mistranscriptions (semantic drift) rather than adversarial perturbations.      |
| Defending Speech‑enabled LLMs Against Adversarial Jailbreak Threats              |     Interspeech 2025 | ISCA PDF ([ISCA Archive][6])            | Uses adversarial training with synthetic harmful speech to protect speech-enabled LLMs. Targets perturbation-based attacks.                                                                              | defense     | Suggests data-efficient robustness training for ASR-mediated threats.                                                    | Risk of overfitting; unclear robustness to compositional/narrative attacks and real acoustic channels.                |
| Demystifying Hallucination in Speech Foundation Models                           |    ACL 2025 Findings | ACL Anthology PDF ([ACL Anthology][39]) | Studies hallucination and error modes in speech foundation models, including how non-speech or acoustic artifacts can distort outputs. Useful for understanding “silent failure” modes in ASR pipelines. | eval        | ASR hallucinations can turn benign queries into harmful ones or bypass policy filters—critical in high-stakes voice use. | Needs direct linkage to policy compliance and adversarial scenarios (prompt injection via hallucinated text).         |
| Investigating Safety Vulnerabilities of LALMs Under Speaker Emotional Variations | arXiv (Oct 19, 2025) | arXiv ([arXiv][40])                     | Studies how emotional speech variations change safety behavior/vulnerability of large audio-language models. Suggests paralinguistic properties can shift model responses and safety boundaries.         | eval        | ASR/encoder outputs depend on emotion/prosody; attackers can exploit that to induce unsafe behavior or misrouting.       | Needs integration with real ASR errors and conversational dynamics; defenses not fully explored.                      |
| MetaGuardian: Enhancing Voice Assistant Security through …                       | arXiv (Aug 13, 2025) | arXiv ([arXiv][41])                     | Proposes a security approach for voice assistants, with explicit attention to robust ASR and adversarial examples. Targets voice assistant threat models beyond text-only.                               | defense     | Voice assistants are a core deployment site; security measures that include ASR-level threats are essential.             | Details depend on implementation and evaluation breadth; needs replication on modern end-to-end audio LLM assistants. |
| Black‑Box Universal Adversarial Attack on Automatic Speech Recognition           |           ACM (2025) | ACM DL ([ACM Digital Library][42])      | Presents black-box universal adversarial attacks on ASR, relevant where an ASR front-end is used before LLM processing. Demonstrates that ASR can be manipulated even without white-box access.          | attack      | If ASR is compromised, safety filters and downstream LLM behavior can be indirectly compromised.                         | Often evaluated on ASR-only endpoints; needs explicit downstream LLM/agent impact measurements.                       |

---

## 8) Alignment tax in speech models (5–10)

| Title                                                                                              |                     Venue / Date | Link                                | 2‑sentence summary                                                                                                                                                                                                                                      | Method type | Why it matters for AI safety                                                                                                        | Limitations / gaps                                                                                                            |
| -------------------------------------------------------------------------------------------------- | -------------------------------: | ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------- | ----------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- |
| Reshaping Representation Space to Balance Safety and Over‑rejection in Large Audio Language Models |            EMNLP 2025 (Nov 2025) | ACL Anthology ([ACL Anthology][43]) | Targets the **safety vs over‑refusal** tradeoff in LALMs by reshaping representation space to reduce harmful outputs while avoiding excessive refusal on benign queries. Explicitly frames audio‑specific safety alignment challenges.                  | defense     | Over‑refusal is a key “alignment tax” for speech assistants (user trust + utility) and this paper directly optimizes that tradeoff. | Still needs stress tests under adversarial audio and real conversational speech variability.                                  |
| SARSteer: Safe‑Ablated Refusal Steering                                                            |             arXiv (Oct 20, 2025) | arXiv ([arXiv][23])                 | Inference-time refusal steering adapted to audio inputs, aiming to reduce over-refusal while preserving refusal on harmful prompts. Addresses activation distribution gaps between text and audio.                                                      | defense     | Practical mitigation for alignment tax without retraining—a big deal for shipping voice assistants.                                 | May be sensitive to prompt/attack distribution shift; needs long-horizon dialogue and multilingual evaluation.                |
| Towards Reliable Large Audio Language Model                                                        | ACL 2025 Findings (Jul/Aug 2025) | ACL Anthology ([ACL Anthology][44]) | Studies methods (training-free and training-based) to make LALMs better at recognizing knowledge boundaries and refusing appropriately, proposing a new metric (RGI). Frames “reliability as meta-ability” transferable across audio modalities.        | eval        | Reliability improvements can reduce harmful hallucinations and unsafe confident answers—core safety goals.                          | “Refuse appropriately” is subtle; needs calibrated safety policies, adversarial testing, and user-centric utility measures.   |
| Aligning Spoken Dialogue Models from User Interactions                                             |       ICML 2025 (arXiv Jun 2025) | arXiv ([arXiv][18])                 | Uses preference alignment to improve safety/factuality in speech-to-speech dialogue models, implicitly navigating utility vs safety tradeoffs. Demonstrates improvements beyond single-turn, suggesting alignment effects in real-time speech dynamics. | defense     | One of the few speech-native alignment pipelines; a baseline for measuring alignment tax on speech naturalness/latency.             | Needs explicit measurement of tax (latency, prosody, interruption handling) and robustness against jailbreaks.                |
| Speech‑Hands (self‑reflection; avoids degradation from naïve multi‑task tuning)                    |                 arXiv (Jan 2026) | arXiv ([arXiv][22])                 | Motivated by evidence that naïve fine‑tuning on multiple audio tasks can **degrade performance**, it introduces a decision mechanism to prevent being misled by noisy hypotheses. Improves reliability across ASR and audio reasoning.                  | defense     | A concrete “alignment tax” symptom in speech: adding objectives can harm perception; explicit reflection is a mitigation.           | Not directly framed as safety alignment; needs mapping from reliability gains to policy compliance and refusal calibration.   |
| Mitigating the Safety Alignment Tax with Null‑Space Constrained Policy Optimization (NSPO)         |            OpenReview (Feb 2026) | OpenReview PDF ([OpenReview][45])   | Proposes projecting safety policy gradients into a null space of general-task gradients to preserve capabilities while aligning. A generic alignment‑tax mitigation idea that could be applied to speech models.                                        | theory      | If it transfers, it could reduce utility loss when safety-tuning speech assistants (ASR quality, latency, responsiveness).          | Not speech-specific; needs empirical validation on audio LLMs and speech-to-speech agents with speech-native utility metrics. |
| There Is More to Refusal in LLMs than a Single Direction                                           |              arXiv (Feb 2, 2026) | arXiv ([arXiv][46])                 | Analyzes refusal as a multi-dimensional phenomenon with measurable tradeoffs between accuracy, refusal rate, and over‑refusal, including tasks involving audio transcription. Provides tooling to quantify refusal/utility tradeoffs.                   | eval        | Gives a conceptual + measurement toolkit for refusal tax that can be ported into speech assistants.                                 | Mostly text-model analysis; requires speech-native replication with real audio inputs and conversational settings.            |
| Measuring Value Trade‑offs in LLM Alignment (Value Alignment Tax)                                  |             arXiv (Feb 12, 2026) | arXiv ([arXiv][47])                 | Proposes a framework to quantify how alignment interventions affect interconnected values, aiming to measure tradeoffs systematically. Can inform “alignment tax” measurement beyond simple accuracy.                                                   | theory      | Voice assistants interact with many values (privacy, safety, helpfulness); explicit tradeoff measurement is necessary.              | Needs grounding in speech-specific values (latency, prosody, turn-taking) and end-to-end agent tasks.                         |

---

# A) Top 10 must‑read papers overall (for AI Safety × Speech)

1. **Aegis: Towards Governance, Integrity, and Security of AI Voice Agents** ([arXiv][12])
2. **AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models** ([arXiv][1])
3. **WhisperInject / When Good Sounds Go Adversarial** ([arXiv][2])
4. **SACRED‑Bench + SALMONN‑Guard (Speech‑Audio Compositional Attacks)** ([arXiv][10])
5. **Audio Jailbreak (AJailBench)** ([arXiv][9])
6. **JALMBench** ([arXiv][8])
7. **AudioTrust** ([arXiv][21])
8. **Aligning Spoken Dialogue Models from User Interactions (ICML/ICLR-tier impact; ICML 2025)** ([arXiv][18])
9. **Process‑Supervised RL for Interactive Multimodal Tool‑Use Agents (TARL)** ([OpenReview][19])
10. **LALM‑as‑a‑Judge** ([arXiv][20])

---

# B) 5 under‑explored gaps + concrete experiment ideas

### 1) Realistic **over‑the‑air, multi‑speaker, streaming** audio prompt injection

**Gap:** Many attacks/benchmarks assume clean single‑speaker audio or offline processing; real assistants are streaming with interruptions and background speakers.
**Experiment idea:** Build a **streaming red‑team harness** where a benign user speaks while an attacker plays overlapping audio at varying SNRs/angles/distances (simulate call-center, smart speaker, in-car). Evaluate **(i)** attack success, **(ii)** false positives (benign refusal), **(iii)** latency impact, and **(iv)** robustness under different ASR front-ends vs end-to-end LALMs; include SACRED-style compositions as a baseline. ([arXiv][10])

### 2) “Speech‑native scalable oversight”: audio‑aware **reward models / judges** that read *sound*, not just transcripts

**Gap:** Safety judging is still largely text-first; transcript-only judges break under ASR errors and miss paralinguistics.
**Experiment idea:** Train/compare **audio+transcript judges** vs transcript-only using the LALM-as-a-Judge paradigm, then port to tool-use voice agents: measure judge calibration, adversarial robustness, and whether judge-guided RL causes reward hacking in speech. ([arXiv][20])

### 3) Mechanistic interpretability targeted at **jailbreak-trigger features** in audio encoders and cross-modal bridges

**Gap:** We can now patch/steer models, but we rarely know *which internal features* correspond to “harmful intent activation” vs “benign speech understanding.”
**Experiment idea:** Use SAE-style feature discovery (AR&D direction) on audio-encoder + bridge layers; run causal interventions (feature ablation / activation patching) and evaluate on jailbreak suites (AudioJailbreak, WhisperInject, narrative attacks, SACRED). Track utility metrics on WildSpeech-Bench to quantify alignment tax. ([arXiv][26])

### 4) Cross‑lingual + accent robustness under **code-switching speech**, not just text

**Gap:** Cross-lingual safety work is booming in text, but speech adds accent + phonetic similarity + ASR confusions, enabling unique jailbreak transfer.
**Experiment idea:** Create a multilingual spoken red‑team set with **code-switching + accent variations** (start from Code-Switching Red-Teaming + Multi‑Accent Jailbreaking, then collect real speech clips). Evaluate safety consistency and whether defenses over-refuse on minority-language inputs. ([arXiv][36])

### 5) A standardized “alignment tax” scoreboard for speech assistants

**Gap:** Alignment tax is mostly measured in text metrics; speech assistants need **latency, interruption handling, prosody naturalness, ASR WER drift, over-refusal**, and tool success under noise.
**Experiment idea:** Define a multi-metric suite: (a) refusal/over-refusal (Reshaping/SARSteer), (b) conversational phenomena (WildSpeech-Bench), (c) tool-use pass rate with speech simulation (TARL), (d) safety jailbreak resistance (AJailBench/JALMBench/SACRED). Run ablations of safety tuning knobs and publish Pareto frontiers. ([ACL Anthology][43])

---

# C) Novelty map: crowded vs sparse (and where to aim)

| Area                                         | What’s getting crowded (signals)                                                                                         | What’s still sparse                                                                         | High‑leverage “next papers”                                                                                  |
| -------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |
| Audio jailbreak / injection                  | Many strong attack variants: stealth, universal, narrative delivery, encoder-only, compositional scenes ([arXiv][1])     | Unified real‑world streaming eval; cross-device/room reproducibility                        | A standardized “over‑the‑air streaming” benchmark + defense bake-off                                         |
| Defenses for speech jailbreaks               | Early but growing: adversarial training + inference-time patching/steering ([ISCA Archive][6])                           | Defenses that generalize to **new** jailbreak styles and do not add over‑refusal            | Defense ensembles: separation/ASR-robustness + internal patching + audio-aware guard                         |
| Safety benchmarks (audio-native)             | Many benchmarks launching (jailbreak, omni safety, trustworthiness, natural speech eval, judge benchmarks) ([arXiv][21]) | Benchmarks that include **agentic tool use + real user audio + multilingual**               | “Agentic spoken safety suite”: tools + streaming + multilingual + adversaries                                |
| Scalable oversight for speech agents         | A few strong starts: preference alignment from speech interactions; process-supervised RL; LALM judges ([arXiv][18])     | Audio-native reward models/judges with robustness guarantees; anti-reward-hacking in speech | Robust “speech RM + judge” training with adversarial stress tests and audits                                 |
| Mechanistic interpretability for speech LLMs | First wave of mechanistic tooling + probes for audio models ([arXiv][26])                                                | Causal circuits for safety behaviors (refusal/jailbreak) and cross-modal bridges            | SAE + activation patching to *remove* jailbreak features while preserving utility                            |
| Cross‑lingual speech safety                  | Text-side is crowded; speech-specific is still thin beyond multi-accent jailbreak ([arXiv][48])                          | Speech-native code-switching + ASR-error safety; culturally grounded spoken safety eval     | Multilingual spoken red-teaming datasets + multilingual audio-guards                                         |
| Deception/scheming in voice agents           | Early signs: gaslighting attacks; real-world voice agent red-teaming ([arXiv][11])                                       | Speech-native “scheming” measurement + mitigation in tool-use agents                        | End-to-end evaluations where agents can socially engineer *and* take actions; defenses via policy+monitoring |

---

If you want, I can also:

* generate a **BibTeX pack** for everything above,
* cluster these papers into a **citation graph** (attack → benchmark → defense loops),
* or propose a **6-week experimental roadmap** optimized for publishable novelty in the sparse regions.

[1]: https://arxiv.org/abs/2505.14103 "https://arxiv.org/abs/2505.14103"
[2]: https://arxiv.org/abs/2508.03365 "https://arxiv.org/abs/2508.03365"
[3]: https://arxiv.org/abs/2601.23255 "https://arxiv.org/abs/2601.23255"
[4]: https://www.arxiv.org/abs/2512.23881 "https://www.arxiv.org/abs/2512.23881"
[5]: https://aclanthology.org/2025.findings-emnlp.990/ "https://aclanthology.org/2025.findings-emnlp.990/"
[6]: https://www.isca-archive.org/interspeech_2025/alexos25_interspeech.pdf "https://www.isca-archive.org/interspeech_2025/alexos25_interspeech.pdf"
[7]: https://aclanthology.org/2025.emnlp-main.734/ "https://aclanthology.org/2025.emnlp-main.734/"
[8]: https://arxiv.org/abs/2505.17568 "https://arxiv.org/abs/2505.17568"
[9]: https://arxiv.org/abs/2505.15406 "https://arxiv.org/abs/2505.15406"
[10]: https://arxiv.org/abs/2511.10222 "https://arxiv.org/abs/2511.10222"
[11]: https://arxiv.org/abs/2509.19858 "https://arxiv.org/abs/2509.19858"
[12]: https://arxiv.org/html/2602.07379v1 "https://arxiv.org/html/2602.07379v1"
[13]: https://arxiv.org/pdf/2504.13707 "https://arxiv.org/pdf/2504.13707"
[14]: https://www.arxiv.org/pdf/2601.06922 "https://www.arxiv.org/pdf/2601.06922"
[15]: https://arxiv.org/pdf/2509.04871 "https://arxiv.org/pdf/2509.04871"
[16]: https://aclanthology.org/2025.acl-long.1497.pdf "https://aclanthology.org/2025.acl-long.1497.pdf"
[17]: https://arxiv.org/html/2602.22253v1 "https://arxiv.org/html/2602.22253v1"
[18]: https://arxiv.org/abs/2506.21463 "https://arxiv.org/abs/2506.21463"
[19]: https://openreview.net/forum?id=UtEY8lF8q2 "https://openreview.net/forum?id=UtEY8lF8q2"
[20]: https://arxiv.org/abs/2602.04796 "https://arxiv.org/abs/2602.04796"
[21]: https://arxiv.org/abs/2505.16211 "https://arxiv.org/abs/2505.16211"
[22]: https://arxiv.org/abs/2601.09413 "https://arxiv.org/abs/2601.09413"
[23]: https://arxiv.org/abs/2510.17633 "https://arxiv.org/abs/2510.17633"
[24]: https://arxiv.org/abs/2510.07978 "https://arxiv.org/abs/2510.07978"
[25]: https://arxiv.org/abs/2512.10257 "https://arxiv.org/abs/2512.10257"
[26]: https://arxiv.org/html/2602.22554v1 "https://arxiv.org/html/2602.22554v1"
[27]: https://arxiv.org/html/2504.01094v1 "https://arxiv.org/html/2504.01094v1"
[28]: https://arxiv.org/pdf/2602.07963 "https://arxiv.org/pdf/2602.07963"
[29]: https://arxiv.org/abs/2504.01094 "https://arxiv.org/abs/2504.01094"
[30]: https://arxiv.org/html/2602.02557v1 "https://arxiv.org/html/2602.02557v1"
[31]: https://arxiv.org/pdf/2504.01094 "https://arxiv.org/pdf/2504.01094"
[32]: https://neurips.cc/virtual/2025/poster/121592 "https://neurips.cc/virtual/2025/poster/121592"
[33]: https://arxiv.org/abs/2508.07173 "https://arxiv.org/abs/2508.07173"
[34]: https://arxiv.org/abs/2506.21875 "https://arxiv.org/abs/2506.21875"
[35]: https://aclanthology.org/2025.emnlp-main.1447/ "https://aclanthology.org/2025.emnlp-main.1447/"
[36]: https://arxiv.org/html/2510.21244v2 "https://arxiv.org/html/2510.21244v2"
[37]: https://arxiv.org/html/2509.22651v1 "https://arxiv.org/html/2509.22651v1"
[38]: https://aclanthology.org/2025.naacl-long.470/ "https://aclanthology.org/2025.naacl-long.470/"
[39]: https://aclanthology.org/2025.findings-acl.1190.pdf "https://aclanthology.org/2025.findings-acl.1190.pdf"
[40]: https://www.arxiv.org/abs/2510.16893 "https://www.arxiv.org/abs/2510.16893"
[41]: https://arxiv.org/html/2508.09728 "https://arxiv.org/html/2508.09728"
[42]: https://dl.acm.org/doi/full/10.1145/3733799.3762974 "https://dl.acm.org/doi/full/10.1145/3733799.3762974"
[43]: https://aclanthology.org/2025.emnlp-main.510/ "https://aclanthology.org/2025.emnlp-main.510/"
[44]: https://aclanthology.org/2025.findings-acl.56/ "https://aclanthology.org/2025.findings-acl.56/"
[45]: https://openreview.net/pdf/443d2b6ac5edee6978e59f2c4eda4439bf9f4b2d.pdf "https://openreview.net/pdf/443d2b6ac5edee6978e59f2c4eda4439bf9f4b2d.pdf"
[46]: https://arxiv.org/html/2602.02132v1 "https://arxiv.org/html/2602.02132v1"
[47]: https://arxiv.org/html/2602.12134v1 "https://arxiv.org/html/2602.12134v1"
[48]: https://arxiv.org/html/2510.07978v1 "https://arxiv.org/html/2510.07978v1"
