# ðŸ§  Cycle #22 â€” 2026-02-27 01:00
## Action: learn
## Context: Post-Causal Abstraction cycle. 1:00 AM but Leo approved nighttime research. Multimodal MI Survey (Lin 2025) was the last remaining item on the must-read list above "optional" threshold. High value: landscape mapping for the speech-shaped hole in MMFM interpretability literature.

## Content: Lin et al. "A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models" (arXiv:2502.17516)
**Authors:** Zihao Lin, Samyadeep Basu et al. â€” UC Davis, UMD, Adobe, UIUC, Virginia Tech (Feb 22, 2025)
**Scope:** 30 pages, 4 Figures, 10 Tables. Covers contrastive VLMs (CLIP), generative VLMs (LLaVA, Qwen, Pixtral), text-to-image diffusion models (SD, DALL-E).

### Taxonomy (3 dimensions):
1. **Model family**: non-generative VLMs, text-to-image diffusion, generative VLMs
2. **Technique type**: LLM-adapted methods vs. multimodal-native methods
3. **Applications**: downstream tasks enhanced by interpretability

### LLM-Adapted Methods Covered (Section 4):
1. **Linear Probing** â€” extended to multimodal; KEY FINDING: intermediate layers capture global cross-modal interactions (not upper layers as in LLMs); upper layers emphasize local details or textual biases
2. **Logit Lens** â€” extended to VLMs; KEY FINDING: earlier layers = more robust to misleading inputs; anomalous inputs alter trajectory (anomaly detection use case); adaptive early exiting feasible
3. **Causal Tracing / Circuit Analysis** â€” active causal intervention; FFN layers store factual knowledge (mid-layer MLPs)
4. **Representation Decomposition** â€” SVD, ICA, NMF applied to multimodal activations
5. **Task Vectors** â€” arithmetic on activations to edit behavior
6. **SAE** â€” sparse autoencoders as "unsupervised task vectors" for granular manipulation
7. **Neuron-level descriptions** â€” concept-specific neurons, hallucination attention patterns

### Key Survey Findings (directly relevant):
- **LLM methods transfer to MMFMs with moderate adjustments**, especially when visual tokens are treated like text tokens
- **Hallucination mitigation + model editing remain UNDERDEVELOPED** in multimodal models vs. LLMs â€” explicit future research direction
- **Speech/audio models are COMPLETELY ABSENT** from this survey â€” confirms Leo's positioning: the survey covers vision (CLIP, LLaVA, SD) only; speech = genuine white space in the landscape

### What this survey does NOT cover (= Leo's opportunity space):
- No audio-language models (Whisper, HuBERT, Qwen-Audio, LLaMA-Omni)
- No speech encoder SAEs
- No audio activation patching
- No ASR mechanistic circuits
- No temporal audio interpretability (audio â‰  static image)
- No cross-modal audio-text grounding coefficient

### Cross-paper synthesis:
- Survey confirms: **probing + logit lens + causal tracing are established transfer methods** â€” exactly what Beyond Transcription + AudioLens did; Leo's work fits the right paradigm
- Survey's "hallucination mitigation underdeveloped in MMFMs" = directly addresses Beyond Transcription's 93.4% hallucination detection + AudioSAE's 70% FPR reduction = Leo's research already targets the sharpest open problem the survey identifies
- Survey's intermediate-layer cross-modal interaction finding (probing) is CONSISTENT with Triple Convergence (~50% depth = semantic crystallization zone) â€” applies to audio too

### New gap identified:
- **No speech-specific survey exists** â€” "MMFM interpretability Ã— speech" is a valid survey paper in itself (Track 1 variant: community resource). This could be a nature/ACL survey target in ~2 years once Leo has enough work to survey.

### Assessment:
- **Lower information density than expected** â€” survey focuses on vision; speech is absent
- But this absence is itself informative: it confirms Leo's space is unmapped and validates Track 1 (InterpBench for audio) as a genuine community contribution
- The taxonomy (probing â†’ logit lens â†’ causal tracing â†’ SAE) also serves as a rough checklist: Leo has done/plans all 4 methods but in the audio domain

## Next: 
- Must-read list now EXHAUSTED (all 10 items read or planned)
- Day 2 priority: **real speech test** â€” whisper_hook_demo.py with actual .wav â†’ validate Triple Convergence Hypothesis with real data (synthetic sine wave confirmed, real speech TBD)
- **NNsight API check** â€” 5-min assessment vs pyvene for encoder interventions
- Optional: check if there's a follow-up to this survey or a speech-specific survey we missed

## Tags: #survey #multimodal-MI #landscape #vision-language #speech-gap #hallucination
