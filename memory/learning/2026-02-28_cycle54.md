# üß† Cycle #54 ‚Äî 2026-02-28 02:31
## Action: plan (AudioSAEBench Design)
## Context: Execution-blocked on experiments (no arXiv until 14:00, no venv yet). Meta-audit #50-53 complete. P5 (AudioSAEBench Design) has zero prerequisites ‚Äî pure conceptual synthesis. Ideal 2:30 AM task: connect accumulated reading into a concrete benchmark protocol draft.

---

## Content: AudioSAEBench ‚Äî Draft Protocol Design

### Motivation
SAEBench (Karvonen, Nanda et al., ICML 2025) established the first multi-metric SAE evaluation framework for text LLMs, exposing that proxy metrics (L0 sparsity, reconstruction loss) poorly predict real-world quality. **No equivalent exists for speech/audio models.** AudioSAEBench fills this gap and introduces a novel audio-native metric: **Grounding Sensitivity**.

Three audio SAE papers now exist (AudioSAE, Mariotte, AR&D) ‚Äî enough for a benchmarking meta-study.

---

### AudioSAEBench: Design Draft v0.1

#### Core Philosophy (from SAEBench lessons)
1. **Multi-metric**: No single proxy metric predicts quality. Need ‚â•4 orthogonal dimensions.
2. **Causal validation required**: Interpretability claims must be testable via intervention (steering/patching), not just correlation.
3. **Audio-native**: Metrics must reflect audio-specific challenges (temporal structure, distributed phonetic encoding, multi-level representations).
4. **Reproducibility**: Fixed stimuli + evaluation harness ‚Üí reproducible numbers across labs.

---

#### Proposed Evaluation Dimensions (5 categories)

**Category 1: Acoustic Concept Detection**
- *What it measures*: Does each SAE feature detect interpretable acoustic concepts?
- *Method*: Given SAE features, find max-activating audio clips ‚Üí label with audio captioner (AR&D pipeline) ‚Üí human rater agreement score
- *Novel aspect vs SAEBench*: Labels must be time-resolved (not just "speech" but "phoneme /s/ in position 3-5")
- *Baseline stimuli*: Use VocalSet (singing techniques), LibriSpeech (phonemes), ESC-50 (environmental sounds)
- *Metric*: `concept_f1` = F1 of feature-level concept detection vs human-labeled ground truth

**Category 2: Disentanglement / Completeness**
- *What it measures*: Are semantically-related audio attributes encoded in separate, non-overlapping features?
- *Method*: Mariotte's completeness metric ‚Äî train linear probe on sparse codes for each attribute (pitch, gender, accent, noise); measure independence via correlation matrix
- *Metric*: `disentanglement_score` = average pairwise correlation between attribute probes on sparse codes (lower = better)
- *Baseline*: Compare dense hidden states (d=0 disentanglement) vs SAE features
- *SAEBench equivalent*: Feature Disentanglement category

**Category 3: Reconstruction Fidelity**
- *What it measures*: Does the SAE preserve downstream task performance?
- *Method*: Replace hidden states with SAE-reconstructed states; measure WER (Whisper), phoneme accuracy, emotion classification F1
- *Metric*: `task_preservation_ratio` = (accuracy_with_SAE) / (accuracy_without_SAE)
- *Note*: AudioSAE showed WER cost +0.4% for 70% hallucination reduction ‚Äî this metric captures that tradeoff
- *SAEBench equivalent*: Reconstruction category

**Category 4: Causal Controllability (Steering)**
- *What it measures*: Can features be used to causally intervene on model behavior?
- *Method*: Feature ablation (suppress top-K features for concept C ‚Üí measure loss of C-related accuracy) + Feature amplification (gain > 1 ‚Üí measure controlled behavior shift)
- *Requires both*: necessity test (ablation) + controllability test (gain)
- *Metric*: `causal_effect_size` = Cohen's d between ablated vs baseline on target task; `steering_precision` = target metric gain / off-target metric gain ratio
- *SAEBench equivalent*: Sparse Probing + Steering categories

**Category 5 (NOVEL): Grounding Sensitivity** ‚≠ê
- *What it measures*: Is each SAE feature causally responding to audio input, or is it predicted from text context?
- *Why it matters*: Audio-LLMs can encode features that look "audio-like" but are actually driven by text priors. AudioSAE and AR&D cannot distinguish these. This is the gap.
- *Method*:
  1. Take a feature F with high activation on audio concept C (e.g., "speaker emotion = sad")
  2. Create minimal pair: (audio=sad, text=neutral) vs (audio=neutral, text=sad)
  3. Measure `gc(F)` = feature activation(audio=C, text=neutral) / [activation(audio=C, text=neutral) + activation(audio=neutral, text=C)]
  4. `gc = 1.0` ‚Üí pure audio grounding; `gc = 0.0` ‚Üí pure text prediction; `gc = 0.5` ‚Üí multimodal blend
- *Stimuli*: ALME conflict stimuli (57K already built, arXiv:2602.11488) ‚Äî perfect for this!
- *Metric*: `grounding_score` = mean gc across all features in Category 1 that have concept labels. Distribution: "grounding histogram" (what fraction of features are audio-grounded vs text-grounded?)
- *Why novel*: SAEBench has no equivalent (text LLMs have no multimodal input). This is AudioSAEBench's unique contribution.

---

#### Benchmark Configuration

**Models to evaluate**:
- Encoder-only: Whisper-base / small (MacBook-feasible), Whisper-large-v3, HuBERT, WavLM
- Audio-LLMs: Qwen2-Audio-7B (via NDIF), SALMONN (if accessible)

**SAE configurations to compare**:
- Existing: AudioSAE (TopK, BatchTopK), Mariotte SAE (TopK), AR&D framework
- Expansion ratios: 4x, 8x, 16x (ablation)
- Layer coverage: per-layer vs end-to-end (single SAE on last hidden layer)

**Stimuli sets**:
- LibriSpeech dev-clean (ASR/phoneme)
- IEMOCAP (emotion, 4-class) 
- ESC-50 (environmental sounds, 50 classes)
- VocalSet (singing technique, 20 techniques)
- ALME conflict stimuli (57K pairs for grounding sensitivity)

---

#### Key Contributions vs Existing Work

| Dimension | SAEBench (text) | AudioSAE | Mariotte | AR&D | **AudioSAEBench** |
|-----------|----------------|----------|----------|------|-------------------|
| Multi-metric | ‚úÖ (8 metrics) | ‚ùå (2-3) | ‚ùå (3) | ‚ùå (2) | ‚úÖ (5 categories) |
| Acoustic concept detection | N/A | partial | partial | ‚úÖ | ‚úÖ (time-resolved) |
| Disentanglement | ‚úÖ | ‚ùå | ‚úÖ (completeness) | ‚ùå | ‚úÖ (attr-specific) |
| Causal controllability | ‚úÖ | ‚úÖ (steering) | ‚ùå | ‚úÖ (steering) | ‚úÖ (both tests) |
| Grounding sensitivity | N/A | ‚ùå | ‚ùå | ‚ùå | ‚úÖ **NOVEL** |
| Temporal resolution | N/A | partial | ‚ùå (mean-pool) | ‚ùå | ‚úÖ (per-timestep) |

---

#### Research Positioning

**Title candidate**: *"AudioSAEBench: Multi-Metric Evaluation of Sparse Autoencoders for Speech and Audio Language Models"*

**Venue target**: INTERSPEECH 2027 or NeurIPS 2026 Datasets & Benchmarks track

**Timing**: 3 papers in field now; paper will appear within 6 months that does partial benchmarking ‚Üí move fast on defining grounding sensitivity before it's claimed

**Collaboration opportunity**: 
- AR&D authors (Chowdhury et al.) have concept labeling pipeline ‚Üí ask if they want to co-author AudioSAEBench as Section 1 component
- AudioSAE authors (Aparin et al., EACL 2026) have baseline SAE infrastructure

---

#### Open Questions / Risks

1. **Temporal SAE**: current audio SAEs use mean-pooled features (Mariotte) or frame-level but not analyzed temporally (AudioSAE). Need to define: is temporal resolution a requirement or bonus?
2. **ALME stimuli access**: arXiv:2602.11488 ‚Äî are stimuli publicly available? Need to verify data release.
3. **Text modality in encoder models**: Grounding sensitivity requires text input; encoder-only models (HuBERT, WavLM) have no text ‚Üí grounding sensitivity only applies to audio-LLMs. Acknowledge this in scope.
4. **Matryoshka SAE**: SAEBench found Matryoshka SAE wins on disentanglement ‚Äî should AudioSAEBench include it? Probably yes as one configuration to test.

---

## Next: 
Present AudioSAEBench design to Leo for feedback. The "Grounding Sensitivity" metric design is the novel contribution worth prototyping quickly (does not require ALME stimuli ‚Äî can use any audio-text conflict pair). Leo's Track 2 now has a concrete paper design, not just a gap description.

## Tags: #Track2 #AudioSAEBench #saebench #benchmark-design #grounding-sensitivity #plan
