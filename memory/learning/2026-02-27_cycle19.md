# üß† Cycle #19 ‚Äî 2026-02-27 00:00
## Action: learn
## Context: Day 2 opening cycle. Top priority from Day 1 wrap = Causal Abstraction (Geiger et al.) ‚Äî theoretical foundation before writing any pyvene/patching code. Last session recommended this explicitly.

## Content: Deep-read "Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability"
**Paper**: Geiger, Ibeling, Zur, Chaudhary, Chauhan, Huang, Arora, Wu, Goodman, Potts, Icard  
**arXiv**: 2301.04709 (v4, May 2025)  
**Venue**: cs.AI

### Problem
What does it mean for a high-level, human-intelligible explanation to be *faithful* to the low-level causal structure of a neural network? Existing XAI (LIME, SHAP) is model-agnostic and cannot make claims about internal reasoning. Probes tell you if a concept is *present*, not if it's *causally active*.

### Core Framework: Causal Abstraction
A **causal model** M = (V, F): variables V, structural functions F. Each variable V has a structural function f_V: Val(V) ‚Üí Val(V).

**The key operation: Interchange Intervention (IIT)**  
Set an internal representation h to the value it would have taken for a *different* input (source input), while running the model on the original input (base input). This tests: "does this representation causally implement this concept?"

**Constructive Abstraction**: A high-level model H is a causal abstraction of low-level model L when:
1. There exists a mapping œÑ from clusters of L's variables ‚Üí H's variables
2. Interchange interventions on L that match H's intervention structure ‚Üí same output changes in both models
3. Three operations: marginalization (remove variables), variable-merge (cluster low-level vars ‚Üí one high-level var), value-merge (cluster low-level values ‚Üí one high-level value)

**Graded faithfulness**: Approximate causal abstraction = interchange intervention *accuracy* (fraction of cases where L's behavior after patching matches H's prediction). This is the key metric.

### What Interchange Intervention = Activation Patching
The paper makes explicit: **activation patching IS interchange intervention analysis**. When we patch layer k activations from a clean input into a corrupted run, we're testing whether layer k causally mediates the behavior difference.

Key distinction that connects to Heimersheim & Nanda (already read):
- **Noising** (base=clean, source=corrupt): test NECESSITY
- **Denoising** (base=corrupt, source=clean): test SUFFICIENCY  
‚Üê Causal Abstraction theory says: both are valid IIT; choose based on what you want to claim

### Methods Unified Under Causal Abstraction
The paper formally shows all these are special cases of IIT:
1. Activation patching (source activations ‚Üí base run)
2. Path patching (only patch along specific paths)
3. Causal tracing (Meng et al. ROME paper)
4. Circuit analysis (Elhage induction heads)
5. Causal mediation analysis (Vig gender bias)
6. Concept erasure / nullspace projection
7. Sparse autoencoders (SAE features = high-level variables in abstraction)
8. Distributed alignment search (DAS / linear representations)
9. Steering (feature steering = soft intervention on high-level var)
10. Differential binary masking

**This means**: When Leo does AudioSAE feature steering + SPIRIT activation patching + AudioLens logit lens, these are all versions of the same underlying operation ‚Äî causal abstraction / IIT. They can be evaluated on the same metric: interchange intervention accuracy.

### Key Concepts for Leo's Research

**Polysemanticity** = formally: a neural variable that value-merges multiple high-level variables. When a neuron encodes multiple concepts, finding its causal role requires disentangling the merged values. SAE is the attempt to undo value-merge.

**Linear Representation Hypothesis** = formally: variable-merge where the mapping œÑ is linear (the cluster of low-level variables is a linear subspace). DAS/pyvene test this.

**Modular features** = sets of low-level variables that causally implement exactly one high-level variable. This is what SAE features aspire to be.

**Grounded Faithfulness** = IIT accuracy. For AudioLens: grounding coefficient = IIT accuracy of audio tokens vs text tokens. High audio IIT accuracy = model "is listening". 

### Connection to Grounding Coefficient (Track 3)
The grounding coefficient Leo defined:
```
gc = Œîacc(audio patch) / (Œîacc(audio patch) + Œîacc(text patch))
```
Is formally: the **relative interchange intervention accuracy** of audio tokens vs text tokens as high-level explanatory variables. The paper's framework legitimizes this as a rigorous causal claim.

### Connection to Triple Convergence (Layers 6-7)
Causal abstraction predicts that if layers 6-7 are a "transition zone", then:
- IIT interventions at layers 0-5 should have low accuracy (wrong abstraction level)
- IIT interventions at layers 6-7 should have *peak* accuracy  
- IIT interventions at layers 8-12 should have high but plateau accuracy

This is testable with pyvene. It would be the first causal validation of the Triple Convergence Hypothesis.

### Key Gap Identified
**No one has done IIT-based evaluation of the Triple Convergence Hypothesis.** AudioSAE, Beyond Transcription, and AudioLens all use *observational* methods (probing, logit lens, feature activity). None have validated with interchange interventions. This is the gap Leo's Track 1 (Audio Causal Benchmark) and Track 3 (Causal AudioLens) can fill.

**New framing**: Leo's research is not just "adding patching to AudioLens" ‚Äî it's **validating audio representations as causal abstractions**. This is a stronger, more theoretically grounded claim.

### Practical Takeaway for Experiments
When designing experiments with pyvene:
1. Define high-level model H: "Whisper layer 6 causally implements phoneme identity"
2. Run IIT: patch layer 6 activations from source (different phoneme) ‚Üí base run
3. Measure: does output change match what H predicts?
4. IIT accuracy = graded faithfulness metric ‚Üí publishable result

## Next: 
- NNsight API check (5-min) ‚Äî simpler than pyvene for Whisper encoder?
- Begin designing the Triple Convergence IIT experiment (conceptual sketch only, no code yet)
- Optional: Multimodal MI Survey (Lin 2025, arXiv:2502.17516)

## Tags: #causal-abstraction #IIT #activation-patching #theory #foundations #grounding-coefficient #triple-convergence
