# üß† Cycle #93 ‚Äî 2026-02-28 22:01
## Action: learn (pre-digest ‚Äî Neuronpedia + SAELens for Paper B AudioSAEBench)
## Context: Execution-blocked (no experiments, no arXiv Saturday batch). Cycle #92 suggested neuronpedia.org pre-digest for Paper B SAE feature visualization. All meta-board Qs answered. This pre-digest gives Leo a ~40% headstart for using Neuronpedia + SAELens when AudioSAEBench experiments begin.

---

## Content: Neuronpedia + SAELens Pre-Digest for AudioSAEBench

### What is Neuronpedia?

**Neuronpedia** (neuronpedia.org) = open platform for mech interp research. Primary use:
- Host SAE feature dashboards (activation histograms, top-activating examples, logit distributions)
- Feature testing with custom prompts (live inference via hosted model servers)
- UMAP visualization of feature space
- Auto-generated LLM interpretations of features
- Supports: probes, concepts, transcoders, SAEs

**Relevance to AudioSAEBench (Paper B):**
> Neuronpedia is the de-facto visualization/hosting layer for SAE features. If Leo trains audio SAEs (Whisper/HuBERT) and uploads them, every feature gets a dashboard showing its top-activating examples + activation histogram. This is exactly what AudioSAEBench needs for human interpretability scoring.

**Status check:** Docs say September 2024 (outdated), but platform is live.
- ‚úÖ Confirmed API endpoint live: `GET /api/feature/{modelId}/{layer}/{featureIndex}`
- Returns: `pos_str`, `neg_str` (top/bottom logit tokens), `maxActApprox`, `freq_hist_data_*`, `topkCosSimIndices` (nearest features), `neuron_alignment_indices`
- Example: `neuronpedia.org/api/feature/gpt2-small/0-res-jb/0` ‚Üí Feature 0 = "Brigade/battalion/regiment" concept (military units)

---

### SAELens v6 Key API Patterns (for AudioSAEBench setup)

```python
# 1. Load pre-trained SAE (text models only for now ‚Äî audio SAEs not yet available)
from sae_lens import SAE
sae, cfg_dict, log_feature_sparsity = SAE.from_pretrained(
    release="gpt2-small-res-jb",  # or "gemma-scope-2b-pt-res"
    sae_id="blocks.8.hook_resid_pre"
)

# 2. SAE works with ANY PyTorch model (not just TransformerLens)
# Key: extract activations ‚Üí pass to sae.encode() / sae.decode()
activations = model(input_ids)["hidden_states"]  # e.g., Whisper encoder layer k
feature_acts = sae.encode(activations)   # sparse feature activations
reconstructed = sae.decode(feature_acts)  # reconstruction

# 3. NNsight integration (confirmed in SAELens v6 README)
# Use NNsight hooks to extract Whisper activations ‚Üí pass to custom SAE
# This is the correct pipeline for audio SAE training
```

**Training API (from tutorials):**
```python
# Training SAE on custom model (relevant for audio SAEs)
from sae_lens import SAETrainingRunner, LanguageModelSAERunnerConfig
cfg = LanguageModelSAERunnerConfig(
    model_name="whisper-small",  # ‚Üê Leo would plug in here
    hook_name="encoder.layers.5.fc2",  # layer to intercept
    d_in=512,  # whisper-small hidden dim
    expansion_factor=16,  # 16√ó ‚Üí 8192 features (AudioSAE used 4096)
    training_tokens=1_000_000,
    ...
)
runner = SAETrainingRunner(cfg)
runner.run()
```

**SAELens Gap #19 confirmed (cycle #87):**
- `pip install sae-lens` + `sae.pretrained_saes()` ‚Üí 25 models, all text-only
- No audio/speech models in the HuggingFace hub with `saelens` tag
- **AudioSAEBench contribution:** train Whisper/HuBERT SAEs with SAELens ‚Üí upload to HF + Neuronpedia ‚Üí first audio SAE dashboards ever

---

### Neuronpedia Upload Pipeline (for Paper B)

**Step 1: Train audio SAE** (SAELens config for Whisper-small, ~1h on A100)
**Step 2: Generate feature dashboards** using `sae_vis` library (Callum McDougall)
```python
# pip install sae-vis
from sae_vis.data_storing_fns import SaeVisData
vis_data = SaeVisData.create(
    encoder=sae,
    model=model,
    tokens=dataset_tokens,  # audio token IDs or mel features
    cfg=SaeVisConfig(features=range(0, 4096))
)
vis_data.save_feature_centric_vis(save_path="vis_output/")
```
**Step 3: Upload to Neuronpedia** via 5-minute application form (forms.gle/Yg51TYFutJysiyDP7)
- Gets hosted with UMAP + auto-explanations
- Community can test features on custom audio prompts
- **Paper B claims:** "World's first publicly accessible audio SAE feature dashboard"

**Step 4: Cite in AudioSAEBench paper** as community contribution
- `sae_vis` + Neuronpedia = standard for text SAEs ‚Üí audio adoption = field-building

---

### Key Insight for Paper B: Neuronpedia as Evaluation Infrastructure

AudioSAEBench needs **5 metric categories:**
1. Feature Disentanglement (quantitative: TCS(F), L0 sparsity)
2. Human Interpretability (qualitative: Neuronpedia dashboards + crowd scoring)
3. Causal Controllability (patching success rate via pyvene/NNsight)
4. Coverage (do features span phoneme √ó speaker √ó emotion space?)
5. Cross-model Alignment (feature correspondence across Whisper/HuBERT/WavLM)

**Neuronpedia enables #2 (Human Interpretability):**
- Host audio SAE features with top-activating audio clips (instead of text tokens)
- Crowd-source interpretability labels: "Does this feature represent /f/ phoneme?"
- Compare human agreement on audio vs text features ‚Üí gap metric for paper
- `topkCosSimIndices` = find nearest phoneme/acoustic feature automatically

**SAE-Vis enables #2 locally:**
- Can run without Neuronpedia (saves to HTML files)
- Use for phoneme features: show waveform segments + spectrogram for top-activating examples
- Novel metric: "interpretability agreement rate" (IAR) = % features where ‚â•2 raters agree on a label

---

### Open Question for Paper B (Q10 ‚Äî new meta-board candidate)

**Q10:** Audio SAE feature dashboards require different visualization primitives than text:
- Text: `pos_str`/`neg_str` = top/bottom logit tokens ‚Üí directly interpretable
- Audio: activation peaks on time-frequency spectrograms ‚Üí need waveform + phoneme alignment
- **Gap:** `sae_vis` is text-only ‚Üí need audio-vis extension for Paper B

**Options:**
1. Fork `sae_vis` ‚Üí add audio spectrogram visualization (minimal; ~100 LoC change)
2. Use existing `librosa.display.specshow` ‚Üí generate PNGs per feature ‚Üí manual Neuronpedia upload
3. Contact Neuronpedia/sae_vis maintainers about audio support (Q10 = community contribution pitch)

**Recommendation:** Option 2 for first paper; Option 3 as community engagement. Flag for Leo (build = needs approval).

---

## Key Takeaways for AudioSAEBench

| Item | Status | Action |
|------|--------|--------|
| SAELens training pipeline | ‚úÖ Ready | `pip install sae-lens`, config for whisper-small |
| Neuronpedia upload process | ‚úÖ Live | 5-min form, then dashboards auto-generated |
| `sae_vis` feature dashboards | ‚úÖ Ready for text | Audio extension = optional build |
| Audio feature visualization | ‚ùå No tool exists | Gap ‚Üí Paper B community contribution |
| Neuronpedia API | ‚úÖ Live JSON | `GET /api/feature/{model}/{layer}/{index}` |

## Next: 
- All pre-digest targets completed (ARENA [1.3.1] ‚úÖ #89, Circuit Tracing ‚úÖ #90, Biology ‚úÖ #91, Neuronpedia+SAELens ‚úÖ this cycle)
- Next high-value action: arXiv Monday batch (~14:00) OR Leo unblock
- Q10 (audio SAE visualization) = new meta-board question candidate
- Cycle #94: consider skip or Q10 meta-board update (if no new arXiv/Leo)

## Tags: #learn #predigest #neuronpedia #saelens #paper-b #audiosae #visualization #sae-vis #gap19
