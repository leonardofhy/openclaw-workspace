# ðŸ§  Cycle #83 â€” 2026-02-28 17:01
## Action: learn (method synthesis)
## Context: arXiv Feb 28 batch still not posted (verified: cs.SD shows Feb 27 max). No new papers in Leo's space. Choosing highest-value alternative: synthesize IIT (Geiger et al., arXiv:2112.00826) + pyvene (Wu et al., arXiv:2403.07809) into a concrete operationalization of grounding_coefficient for Paper A. This bridges the gap between "idea level" and "experiment blueprint" without requiring Leo approval to document.

## Content

### IIT + pyvene â†’ Paper A Experiment Blueprint

**What IIT (Interchange Intervention Training) is:**
- IIT extends vanilla activation patching: instead of just measuring *effect* of patching, you can *train* the neural model to align its internal representations with a high-level causal model
- Key operation: take base input (e.g., audio with conflicting text context), take source input (audio without context), set model's aligned subspace to source's value â†’ measure output
- When IIT loss = 0, the target causal model is PROVABLY a causal abstraction of the neural model (the theoretical guarantee from arXiv:2301.04709)
- This upgrades grounding_coefficient from "empirical ratio" to "causal abstraction test"

**What pyvene adds:**
- Open-source library (PyPI: `pyvene`) for arbitrary PyTorch interventions
- Supports STATIC (analysis) and TRAINABLE (IIT/DAS) interventions
- Works on any PyTorch module â†’ directly usable with Whisper encoder, Qwen2-Audio
- Unified API: `IntervenableModel(config, model)` wraps any model; `model.intervene(...)` swaps activations

**Concrete mapping to Paper A ("Localizing the Listen Layer"):**

```
High-level causal model H (for Paper A):
  - Variable A = audio semantic content (source of "listening")
  - Variable T = text/context semantic content (source of "guessing")
  - H predicts: output comes from A XOR T (not both)

Neural model L = Qwen2-Audio or Whisper decoder:
  - Aligned subspace candidate = residual stream at layer k, audio token positions
  
Experiment flow (DAS-style, pyvene):
  1. Base input: [conflicting audio + text context] (ALME stimuli, arXiv:2602.11488)
  2. Source input: [same audio + neutral text]
  3. Sweep k from 0..N: patch base's layer-k audio subspace with source's value
  4. Measure: does base output shift toward source's prediction?
  5. Layer k* where IIT accuracy peaks = the "Listen Layer"
  
grounding_coefficient(layer k) = IIT accuracy using audio subspace
  â€” high gc(k) means: at this layer, audio representations causally determine output
  â€” low gc(k) means: text context dominates, model is "guessing"
  â€” the LAYER PROFILE of gc is the paper's core finding
```

**Why this is better than vanilla patching:**
1. DAS learns the *optimal linear subspace* within each layer, not just the full hidden state â†’ more surgical, less OOD patching artifacts
2. Trainable alignment = stronger evidence (not just correlation)
3. pyvene handles the bookkeeping; code would be ~50 lines on top of existing whisper_hook_demo.py infrastructure

**Key prerequisites for this experiment:**
1. `pip install pyvene` (add to venv setup checklist)
2. ALME stimuli set (arXiv:2602.11488) â€” already identified, 57K conflict pairs
3. Qwen2-Audio access via NDIF (NNsight handles this) OR use Whisper-small on MacBook as pilot
4. DAS training loop (pyvene tutorial at github.com/stanfordnlp/pyvene â€” "DAS" example notebook)

**New synthesis: DAS gc(k) = grounding_coefficient operationalized**
- Previous formulation: gc = Î”acc(audio patch) / (Î”acc(audio patch) + Î”acc(text patch))
- Upgraded formulation: gc(k) = IIT accuracy at layer k using learned audio subspace (DAS-style)
- The upgrade makes it publishable as a **novel metric with theoretical grounding** (literally: causal abstraction theory backs the claim)
- Paper A Figure 2 candidate: gc(k) curve across layers, showing peak "Listen Layer" location

**Connection to known results:**
- FCCT (arXiv:2511.05923, vision) found middle layers = cross-modal aggregation via causal tracing (no DAS)
- Leo's Paper A using DAS gc(k) on speech = stronger method + novel modality â†’ differentiates clearly

## Next: 
arXiv Feb 28 batch to post eventually â€” check at ~18:00 Taipei. If nothing new: consider daily-consolidate for Day 3 (cycles #70-83 â†’ 2026-02-28-digest.md). Cycle #84 = learn (arXiv) OR daily-consolidate if batch confirmed empty.

## Tags: #paper-a #das #iit #pyvene #grounding-coefficient #listen-layer #method-synthesis
