# ðŸ§  Cycle #70 â€” 2026-02-28 10:31
## Action: learn (triage + citation scan)
## Context: Execution-blocked 18h+; arXiv Feb 28 batch still not posted (expected ~14:00 Taipei). Last meta cycle was #66. Consecutive skip guard triggered (cycles #42-69 with few exceptions). Chose targeted learn over redundant meta-audit. Scanned cs.SD (18 items) + cs.CL (127 items) RSS.

## Content

### cs.SD scan (18 items)
- AR&D (2602.22253) â€” already deep-read cycle #37 âœ…
- Absorbing Diffusion for Speech Enhancement â€” TTS/enhancement, SKIP
- mmWave radar speech â€” SKIP
- "Relating Neural Representations of Vocalized, Mimed, Imagined Speech" (2602.22597) â€” ECoG (brain) study, not audio MI for LLMs, SKIP
- "Same Words, Different Judgments" (2602.22710) â€” preference alignment in speech vs text PbRL; cross-modal study (100 prompts, audio vs text annotations); PERIPHERALLY relevant (modality alignment) but not mechanistic, SKIP
- TADA (2602.23068) â€” TTS/generation, SKIP
- WaveSSM, speaker separation, Hakka ASR, spatial segmentation â€” all SKIP
- **0 new papers** in cs.SD relevant to Leo's research

### cs.CL scan (127 items)
- Bengali, Hakka ASR â€” SKIP
- Modality Collapse (2602.23136) â€” already in knowledge-graph (cycle #40) âœ…
- MiSTER-E (2602.23300) â€” already scanned (cycle #39) âœ…
- Deepfake detection via Whisper (2602.22658) â€” fine-tuning, not interpretability, SKIP
- OmniGAIA (2602.22897) â€” omni-modal agent, not MI, SKIP

### ðŸ†• KEY FIND: Temporal Sparse Autoencoders (T-SAEs)
**arXiv:2511.05541** â€” "Temporal SAEs: Leveraging Sequential Nature of Language for Interpretability"
- Harvard/MIT group (Bhalla, Oesterling, Lakkaraju, Calmon), Oct 2025
- **Core idea**: Standard SAEs train on individual token activations â†’ recover token-specific, noisy, local features; MISS higher-level semantic concepts that span multiple tokens
- **T-SAE fix**: Adds contrastive loss encouraging CONSISTENT activations of high-level features over ADJACENT tokens (temporal smoothness)
- **Results**: Recovers smoother, more coherent semantic concepts; disentangles semantic from syntactic features; works without explicit semantic signal
- **Direct relevance**: This is **Gap #12's text-side solution**! Mariotte identified the same problem for audio (mean-pooled SAEs lose temporal info); T-SAE addresses it for text. **Leo's opportunity: apply T-SAE approach to audio â†’ Temporal Audio SAE = Gap #12 filled**
- **Connection**: T-SAE contrastive loss on adjacent tokens â†’ for audio, this maps to adjacent frames; phoneme-level features should be consistent within a phoneme but change at phoneme boundaries â†’ T-SAE could discover phoneme-level features without supervision
- **New Insight**: If T-SAE works on text because semantic content evolves smoothly â†’ audio has STRONGER temporal structure (phonemes have fixed durations, formants are smooth) â†’ T-SAE should work BETTER on audio than text
- **Strategic value**: Adds methodological backbone to Track 2 AudioSAEBench â†’ "Temporal Audio SAE" = concrete novel contribution within AudioSAEBench

### Citation Checks
- Modality Collapse (2602.23136): 0 citations (too new, Feb 26)
- Cascade Equivalence (2602.17598): 0 citations (too new, Feb 27)

## Next: arXiv Feb 28 batch at ~14:00 Taipei. T-SAE â†’ add to knowledge-graph as Track 2 methodology.
## Tags: #temporal-SAE #audio-SAE #track2 #gap12 #arxiv-scan
