# üß† Cycle #86 ‚Äî 2026-02-28 18:31
## Action: skill-up (ARENA curriculum mapping)
## Context: Saturday evening. arXiv no weekend batch. Meta-board saturated. Execution-blocked 40h. Cycle #85 surfaced ARENA alignment-science branch with new exercises directly relevant to Leo's skill gaps. High-value action: map the actual curriculum content to Leo's research portfolio so he can study efficiently ‚Äî no GPU required, and this dramatically improves experiment quality before running Paper A's IIT experiment.

## Content

### ARENA `alignment-science` Branch ‚Äî Leo's Curriculum Map

**Branch confirmed live:** `callummcdougall/ARENA_3.0`, branch `alignment-science` (merging to main ~Sun Mar 1)

**Full chapter structure (confirmed):**
- Chapter 1 (Transformer Interp) ‚Äî 10 exercise sets, 4 NEW in this branch:
  - `[1.3.1] Linear Probes` ‚≠ê NEW
  - `[1.3.1] Probing for Deception` ‚≠ê NEW
  - `[1.3.3] Interpretability with SAEs`
  - `[1.3.4] Activation Oracles` ‚≠ê NEW
  - `[1.4.1] Indirect Object Identification`
  - `[1.4.2] SAE Circuits` ‚≠ê NEW (replaces old "Function Vectors")
  - `[1.5.X] Various toy models`
- Chapter 4 (Alignment Science) ‚Äî ALL NEW:
  - `[4.1] Emergent Misalignment`
  - `[4.2] Science of Misalignment`
  - `[4.3] Interpreting Reasoning Models`
  - `[4.4] LLM Psychology & Persona Vectors`
  - `[4.5] Investigator Agents`

---

### Exercise Set Priority for Leo (ordered by research impact)

#### ü•á Priority 1: `[1.3.1] Linear Probes`
**URL:** https://colab.research.google.com/github/callummcdougall/arena-pragmatic-interp/blob/main/chapter1_transformer_interp/exercises/part31_linear_probes/1.3.1_Linear_Probes_exercises.ipynb

**Covers:**
1. Extract hidden state activations + PCA visualization ‚Üí truth is linearly separable
2. Train MM (difference-of-means) probes vs LR (logistic regression) probes
3. **Causal interventions** ‚Äî activation patching with probe directions to flip predictions
4. Probing for deception (Apollo Research method)
5. **Attention probes** ‚Äî single learned query vector over full sequence; beats last-token/mean-pool

**Key insight:** MM probes find MORE causally implicated directions than LR probes (despite lower classification accuracy). This is directly relevant to Leo's grounding_coefficient ‚Äî the causal version matters more than the predictive version.

**Models used:** `meta-llama/Llama-2-13b-hf` (needs HF token + gated model access) ‚Äî but concept applies to any model. MacBook may be slow but feasible with smaller variants.

**Direct relevance to Leo's research:**
- Section 3 (causal interventions) = backbone of Paper A's patching methodology
- Attention probe = directly applicable to audio token positions (which audio frames are diagnostically important?)
- DAS (learned linear subspace) from Paper A gc(k) is a natural extension of MM probe approach
- **Methodology transfer**: Leo can apply "attention probe on audio frame positions" to ask "which frames drive the model's response?" ‚Äî directly operationalizes Listen vs Guess per audio segment

**Estimated time:** 3-4h (Colab). Can use smaller models than LLaMA-13B (GPT-2 for concept validation).

---

#### ü•á Priority 2: `[1.4.2] SAE Circuits` (Attribution Graphs)
**URL:** https://colab.research.google.com/github/callummcdougall/arena-pragmatic-interp/blob/main/chapter1_transformer_interp/exercises/part42_sae_circuits/1.4.2_SAE_Circuits_exercises.ipynb

**4 sections:**
1. **Latent Gradients** ‚Äî latent-to-latent, token-to-latent, latent-to-logit gradients ‚Üí linear proxy for how SAE features in different layers communicate
2. **Transcoders** ‚Äî variant of SAEs that reconstruct MLP layer computation (not just activations); "blind case study" using only weights
3. **Attribution graphs** ‚Äî from scratch implementation using Gemma 3-1B + GemmaScope 2 transcoders; full Anthropic circuit-tracing methodology
4. **`circuit-tracer` library** ‚Äî use Anthropic's published tool; Dallas/Austin two-hop factual recall circuit; feature swapping; generation with interventions

**Core papers replicated:**
- "Circuit Tracing: Revealing Computational Graphs in LMs" (Anthropic, 2025) ‚Äî the methodology paper behind Claude biology work
- "Transcoders Find Interpretable LLM Feature Circuits" (Dunefsky et al., 2024)
- SAELens library + TransformerLens

**Key insight for Leo:** Attribution graphs = the "Audio IOI" (Track 1) methodology. The ARENA exercise builds this from scratch, then uses `circuit-tracer` library. Leo needs this for Track 1 (Audio Causal Benchmark). Also: latent-to-latent gradients = direct tool for AudioSAEBench cross-layer feature connectivity analysis.

**Direct relevance to Leo's research:**
- Track 1 (Audio IOI) uses exactly this methodology (circuit-level analysis)
- Paper B (AudioSAEBench) Category 4 (Causal Controllability) uses SAE steering from this
- `circuit-tracer` = ready-made tool; once Leo learns it on text, audio extension is ~2h of adaptation
- Transcoders: understanding MLP computation decomposition ‚Üí Track 4 (LoRA mechanistic analysis)

**Models used:** GPT-2 (sections 1-2) + Gemma 2-2B + GemmaScope (sections 3-4). Colab Pro or A100.

**Estimated time:** 4-6h (Colab Pro recommended for sections 3-4)

---

#### ü•à Priority 3: `[4.1] Emergent Misalignment`
**Direct relevance to Track 4 (LoRA/adapter mechanistic interp):**
- Studies LoRA finetunes that produce misalignment
- "What does fine-tuning change mechanistically?" = exact research question of Track 4
- Uses TransformerLens + SAEs on finetuned models
- Methods: model organisms, autoraters, behavioral evaluation

**Estimated time:** 3-4h (Colab)

---

#### ü•à Priority 4: `[1.3.4] Activation Oracles`
- Model diffing exercises
- Directly addresses "how to compare two models' internal representations"
- Relevant for Track 4 (compare LoRA-adapted vs base Whisper)
- CKA (we already use this!) is the basis of this exercise

---

### Recommended Study Path for Leo (before IIT experiment)

```
1. [1.3.1] Linear Probes (3-4h, any GPU or Colab free) 
   ‚Üí Learn: probe training, causal patching with probe directions, MM vs LR
   ‚Üí THEN: design Paper A's DAS-gc(k) as a "parameterized probe + causal validation"

2. [1.4.2] SAE Circuits sections 1-2 (2-3h, GPT-2 only)
   ‚Üí Learn: latent-to-latent gradients, feature circuits, transcoders
   ‚Üí THEN: implement Track 1 (Audio IOI) using circuit-tracer

3. Run IIT experiment (Priority 1 in experiment-queue.md)
   ‚Üí Now armed with probe methodology + patching methodology from ARENA
```

**Why this order matters:** Doing ARENA first = run the IIT experiment ONCE correctly, rather than 3 iterations debugging methodology. Estimated net time savings: ~6h experimental work.

---

### New Synthesis: ARENA `circuit-tracer` + Paper A Connection

The `circuit-tracer` library (Anthropic, 2025) is directly applicable to Leo's Paper A:
- Paper A needs to localize "Listen Layer" = the layer where audio info is causally consulted
- circuit-tracer does exactly this for text models
- **Audio extension approach:** swap text token embeddings ‚Üí audio frame embeddings in the input graph
- The ARENA [1.4.2] exercise builds this from scratch first ‚Üí Leo understands the algorithm ‚Üí extension to audio is a natural ~2h adaptation

This connection was not obvious from cycle #85's radar note ‚Äî the full exercise structure reveals that `circuit-tracer` is not just "related" but potentially the **direct implementation tool** for Paper A's core methodology.

### New Tool Note: SAELens
The SAE Circuits exercise uses `sae_lens` (SAELens library) for loading pre-trained SAEs ‚Äî this is the production library for SAE research, better than building from scratch. Check if SAELens has any pre-trained SAEs for Whisper/HuBERT before writing custom SAE training code.

## Next
- **Cycle #87:** arXiv scan or skip (Saturday evening, no batch expected)
- **Leo's updated action queue:** Add ARENA study as **pre-experiment step** before IIT experiment
- **Monday ~14:00:** arXiv Feb 28/Mar 1 batch scan

## Tags: #skill-up #ARENA #linear-probes #attribution-graphs #circuit-tracer #SAELens #transcoders
