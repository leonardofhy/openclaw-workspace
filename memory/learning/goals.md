# ğŸ¯ Autodidact Goals

> Last updated: 2026-02-26 14:45 by Leo (direct feedback)

## åŒ—æ¥µæ˜Ÿ (North Star)

**æˆç‚º Google DeepMind / Anthropic ç­‰ç´šçš„ AI Researcherã€‚**

### Thesis-level north star
> å»ºç«‹ä¸€å¥—å¯é©—è­‰çš„ audio æ©Ÿåˆ¶å–®å…ƒï¼ˆfeatures/circuitsï¼‰ï¼Œä¸¦ç”¨å®ƒå€‘åœ¨ ASR èˆ‡ audio-LLM ä¸­åŒæ™‚åšåˆ°ï¼š**å¯é å®šä½éŒ¯èª¤ä¾†æº + å¯æ§ä»‹å…¥æ”¹å–„è¡Œç‚ºï¼ˆå«å®‰å…¨/ç©©å¥æ€§ï¼‰**ã€‚

é€™å¥è©±ä¸²èµ·æ‰€æœ‰æ–¹å‘ï¼šSAEï¼ˆæ©Ÿåˆ¶å–®å…ƒï¼‰ã€patchingï¼ˆå¯é©—è­‰ï¼‰ã€ASRï¼ˆå¯é‡åŒ–è¡Œç‚ºï¼‰ã€audio-LLMï¼ˆèåˆèˆ‡å®‰å…¨ï¼‰ã€ä»¥åŠã€Œæ”¹å¾—å‹•ã€ã€‚

## ç•¶å‰ç ”ç©¶æ–¹å‘

### ä¸»æ–¹å‘ï¼šMechanistic Interpretability Ã— Speech/Multimodal LM
- **ç‚ºä»€éº¼é¸é€™å€‹**: é ˜åŸŸæ—©æœŸåŠ é€Ÿä¸­ï¼ˆ~20 ç¯‡ç›¸é—œå·¥ä½œï¼‰ï¼Œå…ˆé€²è€…å„ªå‹¢ä»åœ¨ä½†éœ€åŠ é€Ÿ
- **æ ¸å¿ƒå•é¡Œ**: Multimodal LMï¼ˆQwen-Audio, Gemini, GPT-4oï¼‰å¦‚ä½•åœ¨å…§éƒ¨è™•ç† speechï¼Ÿ
  - Speech tokens åœ¨å“ªä¸€å±¤è¢«è½‰åŒ–ç‚ºèªç¾©ï¼Ÿ
  - Emotion / speaker identity / phonetics åˆ†åˆ¥åœ¨å“ªè£¡è™•ç†ï¼Ÿ
  - Speech pathway å’Œ text pathway åœ¨å“ªè£¡äº¤æœƒï¼Ÿ
- **æ–¹æ³•è«–éœ€æ±‚**: activation patching, probing, SAE, logit lens â€” éœ€è¦å¾ text mech interp é·ç§»åˆ° speech

### æ¬¡æ–¹å‘ï¼šAI Safety Ã— Speech
- Audio adversarial attacks çš„æ©Ÿåˆ¶
- Speech-based jailbreak detection
- Speech modality æ˜¯å¦ç¹¼æ‰¿äº† text safety trainingï¼Ÿ

### é€²è¡Œä¸­ï¼šAudioMatters â€” Interspeech 2026
- ä¸€ä½œï¼ŒCMT å¡ä½æˆªæ­¢ 2026-02-26 19:00
- æœ€çµ‚ PDF 2026-03-05
- æŠ•ç¨¿å¾Œ â†’ æ³¨æ„åŠ›è½‰å‘ mech interp æ–¹å‘

## 5 Research Tracksï¼ˆä¸€å€‹ thesis çš„ä¸åŒåˆ‡é¢ï¼‰

**æˆ°ç•¥è€ƒé‡ï¼šAudioLens æ˜¯æ™ºå‡±å“¥çš„å·¥ä½œ â†’ Leo æœ‰ä¸»å ´å„ªå‹¢ï¼›5 tracks éƒ½æœå‹™åŒä¸€å€‹ thesis**

### Track 1ï¼šAudio Causal Benchmark / Protocol â†’ community resource
- å»ºç«‹ audio çš„ IOI â€” clean/corrupt æ¨™æº–ä»»å‹™ + patching protocol
- ç¬¬ä¸€ç¯‡ paper: 3-5 tasks (Speech Commands, ESC-50, çŸ­å¥ ASR) Ã— 3-5 corruptions
- **åšå‡ºä¾†æ‰€æœ‰äººå¼•ç”¨**

### Track 2ï¼šAudioSAE â†’ AudioSAEBench â†’ è©•ä¼°ç§‘å­¸åŒ–
- å° Whisper/HuBERT/WavLM åš SAE + audio-native è©•ä¼°æŒ‡æ¨™
- å› æœ steering/erasure æ¸¬è©¦ + å‰¯ä½œç”¨æ›²ç·š
- å»¶ä¼¸ï¼šfeature alignment across models/languages

### Track 3ï¼šListen vs Guess in Audio-LLMs â­ æœ€é«˜å„ªå…ˆ
- æ¥æ£’æ™ºå‡±å“¥ AudioLensï¼Œç”¨ minimal pairs + patching é‡åŒ– grounding
- å®šç¾© grounding coefficientï¼ˆaudio patching sensitivity vs context patching sensitivityï¼‰
- **å„ªå‹¢ï¼šæ™ºå‡±å“¥ = AudioLens ä½œè€… = æ¯å¤©ä¸€èµ·åƒé£¯çš„ labmateï¼Œå·²è«‡å¥½åˆä½œ**

### Track 4ï¼šMechanistic Interp of Adaptation (LoRA/adapters)
- è§£é‡‹ã€Œå¾®èª¿åˆ°åº•æ”¹äº†ä»€éº¼æ©Ÿåˆ¶ã€
- CKA/SVD + SAE drift + patching å®šä½è®ŠåŒ–
- å»¶ä¼¸ï¼šmechanistically guided fine-tuning

### Track 5ï¼šSafety Mechanistic Defenses (Listen-Layer Audit)
- **æ ¸å¿ƒææ¡ˆ**: Safety-Critical Listen-Layer Audit via gc(k) â€” é€å±¤å®‰å…¨è©•åˆ†
- Audio prompt injection benchmark + trigger subspace å®šä½
- æœ€å°å‰¯ä½œç”¨çš„ inference-time defense
- **Novelty verdict**: ğŸŸ¡ YELLOW â€” éœ€è¦å…©å€‹ crisp claim ä¹‹ä¸€æ¨åˆ° GREEN:
  1. Safety signal emergence: harmful intent åœ¨ audio encoder ç‰¹å®šå±¤å°±ç·šæ€§å¯åˆ†ï¼ˆtranscription å‰ï¼‰
  2. Audit â†’ intervention bridge: gc(k) æŒ‡å°åœ¨å“ªå±¤ patch/pruneï¼Œæ”¹å–„ SPIRIT/ALMGuard
- **æœ€è¿‘ overlap**: SPIRIT (layer patching), ALMGuard (shortcut localization), SALMONN-Guard (multimodal guard)
- **MVP**: 7-day plan in `memory/learning/research/listen-layer-audit-deep-research-2026-03.md`
- é¢¨éšªï¼šè² è²¬ä»»æ­éœ²ï¼Œdefense > attack
- **MATS Research Task é¦–é¸æ–¹å‘**ï¼ˆAudio Jailbreak è·¨æ¨¡æ…‹æ¢æ¸¬ï¼‰

## 10 Core Research Questionsï¼ˆautodidact è®€è«–æ–‡æ™‚åœç¹é€™äº›å•é¡Œæ€è€ƒï¼‰
1. Audio çš„ "clean/corrupt" æ€éº¼è¨­è¨ˆæ‰åªç ´å£ä½ è¦éš”é›¢çš„å› ç´ ï¼Ÿ
2. Patching çš„ OOD internal state æ€éº¼è¨ºæ–·/é¿å…ï¼Ÿ
3. ASR çš„ WER æ˜¯åºåˆ—æŒ‡æ¨™ â€” æ€éº¼å°é½Šåˆ°å±€éƒ¨æ©Ÿåˆ¶ï¼Ÿ
4. SAE features èƒ½è·¨èªè¨€/å™ªè²/æ¨¡å‹é·ç§»å—ï¼Ÿç”¨ä»€éº¼ alignmentï¼Ÿ
5. Audio SAE è©•ä¼°è©²ç”¨ä»€éº¼æŒ‡æ¨™ï¼Ÿå“ªäº›èˆ‡ã€Œå¯å› æœæ“æ§ã€ç›¸é—œï¼Ÿ
6. æ¨¡å‹ä½•æ™‚åœ¨ã€Œè½ã€ã€ä½•æ™‚åœ¨ã€ŒçŒœã€ï¼Ÿæ€éº¼é‡åŒ–ï¼Ÿ
7. Connector bottleneck è®“å“ªäº›ä¿¡æ¯ä¸å¯é€†ä¸Ÿå¤±ï¼Ÿ
8. Audio jailbreak çš„ trigger subspace åœ¨ encoder é‚„æ˜¯ LMï¼Ÿ
9. Neural codec çš„ codebook åˆ†å·¥ â€” å“ªäº›å° pitch/timbre/æ¸…æ™°åº¦è² è²¬ï¼Ÿ
10. Audio èƒ½åšè‡ªå‹• circuit graph å—ï¼Ÿå‰ç½®æ¢ä»¶æ˜¯ä»€éº¼ï¼Ÿ

## Skill Gapsï¼ˆæŠ€èƒ½å±¤é¢ï¼‰
- [ ] TransformerLens + pyvene å¯¦ä½œ
- [ ] SAE è¨“ç·´ + evaluation discipline
- [ ] AudioLens codebaseï¼ˆå•æ™ºå‡±å“¥ï¼‰
- [ ] Whisper/HuBERT/WavLM é€å±¤æ©Ÿåˆ¶
- [ ] EnCodec discrete tokens èˆ‡ MI çš„æ¥å£
- [ ] Causal abstraction ç†è«–åŸºç¤

## Must-Read Listï¼ˆæŒ‰å„ªå…ˆç´šï¼‰

### Tier 0: æœ€é«˜å„ªå…ˆï¼ˆListen-Layer Audit ç›´æ¥ç›¸é—œï¼Œ2026-03 deep research ç¢ºèªï¼‰
1. [ ] **SPIRIT** (EMNLP 2025) â€” ğŸ¥‡ activation patching for speech jailbreak defense; up to 99% robustness w/o retraining [ACL Anthology](https://aclanthology.org/2025.emnlp-main.734.pdf)
2. [ ] **SACRED-Bench + SALMONN-Guard** (arXiv 2511.10222, Nov 2025) â€” ğŸ¥ˆ compositional audio attacks + multimodal guard; Gemini 2.5 Pro = 66% ASR even with guardrails [arXiv](https://arxiv.org/abs/2511.10222)
3. [ ] **ALMGuard** (NeurIPS 2025 poster) â€” ğŸ¥‰ safety shortcut localization + mel-gradient sparse mask; cuts jailbreak ASR to 4.6% [NeurIPS](https://neurips.cc/virtual/2025/poster/115978)

### Tier 1: é«˜å„ªå…ˆï¼ˆattack surface + benchmarksï¼‰
4. [ ] **JALMBench** (ICLR 2026 poster) â€” æœ€å¤§ audio jailbreak benchmark: 12 LALMs Ã— 8 attacks Ã— 5 defenses [OpenReview](https://openreview.net/forum?id=DJkQ236C8B)
5. [ ] **AJailBench + APT** (arXiv 2505.15406, May 2025) â€” 1,495 adversarial audio prompts + Bayesian-optimized perturbations [arXiv](https://arxiv.org/abs/2505.15406)
6. [ ] **LALM-as-a-Judge** (arXiv 2602.04796, Feb 2026) â€” ~24k dialogues; audio-LM as safety judge; sensitivity/specificity analysis [arXiv](https://arxiv.org/pdf/2602.04796)

### Tier 2: é‡è¦è£œå……ï¼ˆattack families + defensesï¼‰
7. [ ] **AudioJailbreak** (TDSC accepted, May 2025 / rev Feb 2026) â€” weak adversary + over-the-air robustness; claims GPT-4o bypass [arXiv](https://arxiv.org/abs/2505.14103)
8. [ ] **Multi-AudioJail** (arXiv 2504.01094, Apr 2025) â€” multilingual/accent attacks; +57pp jailbreak success [arXiv](https://arxiv.org/abs/2504.01094)
9. [ ] **StyleBreak** (arXiv 2511.10692, Nov 2025) â€” style/voice conditioned attacks [arXiv](https://arxiv.org/html/2511.10692v1)
10. [ ] **Defending speech-enabled LLMs via adversarial training** (Interspeech 2025) â€” PGD-style defense + conformer architecture description [ISCA](https://www.isca-archive.org/interspeech_2025/alexos25_interspeech.pdf)

### Tier 3: åŸºç¤æ–¹æ³•è«–ï¼ˆä¿ç•™åŸæ¸…å–®ï¼‰
11. [ ] **AudioLens** (æ™ºå‡±å“¥ 2025, NTU) â€” lab è‡ªå·±çš„å·¥ä½œï¼[arXiv:2506.05140]
12. [x] **Beyond Transcription** (Glazer 2025) â€” ASR MI åŸºç¤æ–¹æ³•è«– [arXiv:2508.15882] âœ… 2026-02-26 deep read cycle #6
13. [ ] **AudioSAE** (Aparin 2026, EACL) â€” SAE for speech + steering [arXiv:2602.05027]
14. [ ] **Activation patching best practices** (Heimersheim & Nanda) â€” é¿å… pitfalls
15. [ ] **Causal abstraction** (Geiger et al.) â€” å› æœä»‹å…¥çš„ç†è«–åŸºç¤
16. [ ] Multimodal MI Survey (Lin 2025) [arXiv:2502.17516]
17. [x] **SAEBench** (Karvonen, Nanda et al., ICML 2025) â€” 8-metric multi-category evaluation âœ… 2026-02-27 cycle #38
18. [ ] ICML 2025 MI Tutorial materials
19. [ ] **Interspeech 2025 Tutorial** â€” "Interpretability for Speech Models"

## 6-12 Month Ramp Plan
- **Month 0-2**: Foundations
  - ç²¾è®€ AudioLens + Beyond Transcription + AudioSAEï¼ˆæ–¹æ³•ç´°ç¯€ï¼Œä¸åª abstractï¼‰
  - TransformerLens + pyvene å¯¦ä½œï¼ˆå…ˆåœ¨ text ä¸Šè·‘é€šï¼Œå†é·ç§»åˆ° audioï¼‰
  - Starter experiments 1-3ï¼ˆprobing, CKA, Whisper neuron atlasï¼‰â†’ MacBook å¯è·‘
  - ç†è§£ patching pitfalls + SAE evaluation methodology
- **Month 2-4**: å’Œæ™ºå‡±å“¥åˆä½œè¨­è¨ˆ counterfactual experimentsï¼ˆå·²è«‡å¥½åˆä½œï¼‰
  - Starter experiments 4-5ï¼ˆsingle-layer SAE, intervention on Speech Commandsï¼‰â†’ æˆ°è‰¦
  - Define "clean vs corrupt" protocols for audio
- **Month 4-8**: è·‘å¯¦é©— + å¯«ç¬¬ä¸€ç¯‡è«–æ–‡
- **Month 8-12**: æŠ•ç¨¿ + é–‹å§‹ç¬¬äºŒå€‹æ–¹å‘

## Key Deadlines
| Conference | Deadline | Target Paper |
|-----------|----------|-------------|
| Interspeech 2026 | PDF 2026-03-05 | AudioMatters |
| NeurIPS 2026 | ~2026-05 | Listen vs Guess (if ready) |
| EMNLP 2026 | ~2026-06 | Audio InterpBench |

## ğŸ“Œ ç‹€æ…‹æ›´æ–° (2026-02-26 19:00)

**AudioMatters CMT deadline passed** â†’ Leo's focus now shifts fully to mech interp.

**Immediate next steps (post-deadline):**
1. ğŸ“– Deep-read **AudioSAE** (arXiv:2602.05027) â€” Track 2 anchor paper
2. ğŸ“– Read **SPIRIT** (arXiv:2505.13541) â€” safety track anchor paper
3. ğŸ“– Read **Activation patching best practices** (Heimersheim & Nanda) â€” é¿å… pitfalls
4. ğŸ’¡ æ¯ç¯‡è®€å®Œç”¢å‡º 1-2 å€‹å…·é«” research ideaï¼ˆèˆ‡ 10 core questions å°ç…§ï¼‰
5. Contact æ™ºå‡±å“¥ about AudioLens codebase access

**âš ï¸ Leo æŒ‡ç¤º (2026-02-26 21:10)ï¼šä¸è¦å¯¦ä½œï¼Œå°ˆæ³¨æŒ–æ˜æ–°æƒ³æ³•ã€‚**
**è£œå……æŒ‡ç¤º (2026-02-27 00:35)ï¼šå¤œé–“ä¸éœ€è¦è‡ªå‹• skipï¼Œå¯æŒçºŒè‡ªä¸»ç ”ç©¶ï¼›åªæ˜¯ Leo å³æ™‚ feedback æ©Ÿç‡è¼ƒä½ã€‚**
**æ–°æŒ‡ç¤º (2026-02-28 01:04)ï¼šæ¢å¾© 30 åˆ†é˜ cadenceï¼Œè‡ªä¸»å­¸ç¿’è¦åŠ å…¥ã€Œmeta-awareness ç³»çµ±è‡ªæˆ‘ç ”ç©¶ã€ï¼šæ¯è¼ªå¯åˆ—å‡ºå€¼å¾—æ”¹é€²å•é¡Œï¼Œä¸¦åšæœ€å°å¯é€†æ”¹å–„ã€‚**
**Recommended next cycles:** `learn` + `reflect(meta-audit)` äº¤æ›¿ï¼Œé¿å… execution-blocked æ™‚é€£çºŒ skipã€‚

## Paper Idea #7: Audio T-SAE (æ–°å¢ 2026-02-28 cycle #72)
**"Phoneme-Aware Sparse Autoencoders for Speech Models via Temporal Contrastive Learning"**
- Apply T-SAE (Bhalla et al., ICLR 2026 Oral, arXiv:2511.05541) to Whisper/HuBERT
- Matryoshka partition: high-level (speaker/phoneme/emotion) + low-level (frame-level articulation)
- Multi-scale temporal contrastive loss: SHORT (adjacent frames, phoneme-level) + LONG (utterance-level for speaker identity)
- Evaluate with TCS(F) = within-phoneme variance / across-phoneme variance (uses MFA boundary ground truth)
- Audio has STRONGER temporal priors than text â†’ should work BETTER; T-SAE authors flag this gap explicitly
- Gap #17: No audio SAE exploits temporal structure. All existing audio SAEs (AudioSAE, Mariotte, AR&D) are i.i.d. across frames.
- Venue: INTERSPEECH 2027 or ICASSP 2027. Risk: T-SAE authors could extend first â†’ move fast.
- Relationship to AudioSAEBench: TCS(F) = Category 1 metric; Audio T-SAE = the model being benchmarked.

## Gap #19: No Standardized Audio SAE Training Pipeline (æ–°å¢ 2026-02-28 cycle #87)
- SAELens v6 (the de-facto SAE training/loading library, `decoderesearch/SAELens`) has **ZERO audio/speech pre-trained SAEs** â€” all 25 HuggingFace models = Gemma-scope / GPT-2 / LLaMA only
- All 5 audio SAE papers (AudioSAE, Mariotte, AR&D, Plantinga-PD, Paek et al.) use custom one-off training code
- **Implication for Paper B (AudioSAEBench)**: include a SAELens-compatible audio SAE training toolkit as a community contribution. This makes AudioSAEBench stronger (not just evaluation â†’ evaluation + training pipeline) and ensures results are `pip install`-able and reproducible.
- Connection: Leo uses SAELens training code with NNsight hooks for Whisper/HuBERT activation extraction â†’ upload trained SAEs with `saelens` tag â†’ field has first standardized audio SAE backbone

## Gap #18: Phonological Vector Geometry Through the Connector (æ–°å¢ 2026-02-28 cycle #81; experiment design cycle #82)
**"Does linear phonological structure in S3M encoders survive through the connector into speech LLMs?"**
- Choi et al. 2602.18899 confirms: phonological features are linear, compositional, scale-continuous in S3M representations (96 languages)
- What's unknown: Does this linear phonological geometry persist after passing through the connector into the LLM residual stream?
- If YES: LLM has direct access to phonological feature directions â†’ listening is phonologically structured
- If NO: connector destroys phonological geometry â†’ connector = modality bottleneck â†’ supports Modality Collapse (2602.23136)
- **Experiment (4 steps, cycle #82):**
  1. Extract voicing_vector = h([d]) - h([t]) from Whisper-small encoder (MacBook, Choi et al. stimuli)
  2. Hook connector via NNsight (DeSTA2 or NDIF Qwen2-Audio)
  3. Test arithmetic in LLM layer 0: `projected_h([b]) â‰ˆ projected_h([d]) - projected_h([t]) + projected_h([p])?`
  4. Layer-wise probe sweep: where does voicing direction become decodable?
- **Status:** Added as **Priority 0** in experiment-queue.md (prerequisite check before Paper A IIT experiment)
- **Idea gate:** ğŸŸ¢ GREEN â€” no competitors found; integrate as Figure 2 of Paper A or Category 0 of AudioSAEBench
- Connection: Paper A (Listen Layer â€” prerequisite), Paper B (AudioSAEBench TCS(F) validation), Idea #7 (Audio T-SAE), Gap #14 (Modality Collapse)

## Gap #20: Emotion-Modulated Safety (Track 5 Candidate â€” ğŸŸ¡ YELLOW gate, cycle #100)
**"Why does speaker emotion override LALM safety alignment?"**
- Feng et al. 2510.16893 (ICASSP 2026): emotion varies unsafe response rate non-monotonically; medium intensity = highest risk
- Mechanistic cause unknown: which layers/heads allow emotion to bypass safety neurons?
- Method: SPIRIT-style patching + Zhao et al. ESN cross-reference + SAE-guided feature attribution
- **Gate verdict: ğŸŸ¡ YELLOW** â€” genuine gap but Track 5 = lowest priority; Hung-yi Lee lab (same as AudioLens) may follow up
- **Action: HOLD** â€” do not develop until Papers A+B submitted. Monitor Feng et al. for mechanistic follow-up.

## å¾…è«‹æ±‚ Leo çš„ä»»å‹™éšŠåˆ—
1. ğŸ”¬ **Deep Research**: Mech Interp Ã— Speech é ˜åŸŸæ·±åº¦æƒæï¼ˆå·²è«‹æ±‚ 2/26ï¼‰
2. ğŸ”§ **Deep Research**: è‡ªä¸» AI agent ç³»çµ±çš„å¯æŒçºŒæ¶æ§‹ï¼ˆå·²è«‹æ±‚ 2/26ï¼‰
