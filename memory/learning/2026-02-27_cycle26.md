# ðŸ§  Cycle #26 â€” 2026-02-27 03:00
## Action: learn
## Context: Must-read list exhausted; Kawamura 2602.15307 was scanned in cycle #24 but not deep-read. First systematic neuron-level analysis of general-purpose audio SSL â€” directly relevant to Track 2 (AudioSAEBench) and Triple Convergence. High-value learn action at 3 AM: no Leo participation needed, pure paper reading.

## Content: Deep-read "What Do Neurons Listen To?" (Kawamura et al., arXiv:2602.15307, EUSIPCO 2026)

**Model studied:** M2D (Masked Modeling Duo) â€” ViT-based SSL audio model, 12 layers Ã— 3072 neurons per layer.
**Method:** AAPE (Audio Activation Probability Entropy) â€” adapts LAPE from NLP. Identifies neurons that activate selectively for a small subset of audio classes. Three-step filter: (1) exclude bottom 5% by activation rate, (2) keep lowest AAPE percentile (high selectivity), (3) top 5% by class-specific activation probability.

**RQ1 â€” Are class-specific neurons present?**
- SSL model: ~100% class coverage across ALL tasks (novel, unseen during training)
- SL model: much lower (e.g., 49% in VoxCeleb1 gender/nationality)
- SSL develops 2x more mean class-specific neurons than SL
- **Interpretation**: SSL's general-purpose representations are mechanistically grounded â€” specific neurons "specialize" for each class even without explicit supervision

**RQ2 â€” What do shared neurons encode?**
- **Gender neurons**: strong cross-task sharing (VC1 female â†” CREMA-D female; VC1 male â†” CREMA-D male). Even though SSL trained without gender labels.
- **Pitch neurons**: cross-task sharing in octave classes (Surge â†” NSynth), even across different synthesis methods
- **Arousal neurons**: ANG+HAP share neurons; DIS/FEA/NEU/SAD share differently â†’ aligns with Russell's circumplex model
- **Language neurons**: Germanic (de+en) share more; Romance (es/fr/it) share more â†’ phonetic family structure in activations
- **Genre neurons**: classical+jazz share most (instrumental structure vs vocal-dominant pop)
- **Polysemanticity finding**: "shared responses" = same neuron responds to multiple semantically/acoustically related classes = classic polysemanticity. SAE needed to disentangle.

**RQ3 â€” Functional impact of class-specific neurons?**
- Deactivating 26 "classical+jazz" neurons â†’ significant GTZAN classification drop
- Deactivating 22 "ANG+HAP" neurons â†’ significant CREMA-D emotion classification drop
- Both > random deactivation baseline â†’ neurons are causally necessary (noising = necessity test)

**Key methodological note:** Their "deactivation" is zero-ing activations = noising patching (necessity). They don't do denoising patching (sufficiency). Same limitation as Zhao et al. 2601.03115 â€” both papers stop at necessity, never test sufficiency. Leo can do both.

## Key Gaps Identified

**Gap #10 (previously counted, now confirmed more specifically):**
No paper combines:
1. Neuron-level localization (Kawamura's AAPE method)
2. SAE decomposition (to resolve polysemanticity in those neurons)
3. Audio-vs-text pathway test (Zhao's ESN deactivation + Leo's grounding_coefficient)
4. Both necessity AND sufficiency patching (Heimersheim & Nanda says you need both)

Leo's Track 2+3 synthesis can close all four simultaneously.

**New Gap #11:** Kawamura studies M2D (environmental/music/speech SSL) but doesn't ask "does the same neuron encoding gender/language activate differently when the information comes from audio vs linguistic context?" â€” only encoder model, so context = audio only. But in LALMs (Qwen2.5-Omni, Kimi-Audio), the same gender neuron could respond to audio cues OR to "she said" in the text. That's Track 3's question, unanswered here.

## Connections to Existing Framework

**Triple Convergence Hypothesis:**
Kawamura's class-specific neurons cluster in layers with high AAPE contrast. For 12-layer M2D, if pattern holds, should see transition ~layer 6 (50% depth). Paper doesn't explicitly report layer distribution of class-specific neurons â€” only reports âŸ¨layer, neuronâŸ© IDs in tables (not accessible from HTML). But the 3072 neurons per layer Ã— 12 layers analysis confirms deep layers develop more semantic specialization.

**Track 2 (AudioSAEBench) connection:**
Kawamura's class-specific neurons via AAPE = a "neuron-level SAE approximation" â€” they manually find selective neurons; SAE would automate this AND disentangle polysemanticity. AAPE + SAE features would be complementary metrics for AudioSAEBench: AAPE for human-interpretable neuron ID, SAE for quantified feature decomposition.

**Track 3 (Listen vs Guess) connection:**
The polysemanticity finding (gender neuron responds to male/female across datasets) is exactly what grounding_coefficient is designed to parse: does this neuron fire because of audio gender cues, or text gender cues? AAPE can't distinguish â€” patching can.

**Zhao et al. 2601.03115 connection:**
Both find specialized neurons (ESNs for emotion in Zhao; class-specific neurons in Kawamura). Both use similar deactivation ablation. Neither asks "is this activation caused by audio or text?" The convergence of two independent teams finding the same gap is strong motivation for Track 3.

## Synthesis: "Class-specific Neuron Grounding" Experiment Design (conceptual sketch for Leo to review)

Experiment concept (NO CODE YET â€” needs Leo approval):
1. Take Kawamura's AAPE method â†’ port to an LALM (e.g., Qwen2.5-Omni or Qwen2-Audio)
2. Find class-specific neurons for emotion (CREMA-D) and gender (VoxCeleb1)
3. For each identified neuron, run two patching experiments:
   a. **Audio pathway patch**: swap audio tokens from audio_emotion_A â†’ audio_emotion_B, hold text constant
   b. **Text pathway patch**: swap text context ("she is angry" â†’ "she is happy"), hold audio constant
4. Compute grounding_coefficient at neuron level = Î”AAPE(audio patch) / (Î”AAPE(audio) + Î”AAPE(text))
5. If GC > 0.7 â†’ "audio-grounded neuron"; GC < 0.3 â†’ "text-grounded neuron"; 0.3-0.7 â†’ "mixed"
6. Predict: emotion neurons are more audio-grounded (GC > 0.7); discourse-level features more text-grounded (GC < 0.3)

This design directly cites Kawamura + Zhao + Heimersheim & Nanda as methodological ancestry. One experiment, three papers cited, clear novelty.

## Next: 
- Still 3 AM â€” no more papers to deep-read at high priority; real action queue requires Leo or physical resources
- Next high-value actions when Leo wakes up: (1) real speech test for Triple Convergence, (2) present "Class-specific Neuron Grounding" experiment sketch to Leo for approval, (3) NNsight API check

## Tags: #kawamura #neuron-level #SSL #M2D #AAPE #polysemanticity #track2 #track3 #grounding-coefficient #triple-convergence #experiment-design
