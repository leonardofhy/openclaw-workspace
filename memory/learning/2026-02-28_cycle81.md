# üß† Cycle #81 ‚Äî 2026-02-28 16:01
## Action: learn (deep-scan)
## Context: arXiv Feb 28 batch still not posted (16:00 Taipei ‚Äî unusually late, confirmed across 3 queries). Paper 2602.18899 has appeared in 5+ searches but not yet analyzed. Abstract+context synthesis available. High-value action: extract structured insights from phonological vector arithmetic paper.

## Content

### Paper: Choi et al. arXiv:2602.18899 ‚Äî "Self-supervised Speech Models Discover Phonological Vector Arithmetic"
**Authors**: Kwanghee Choi, Eunjung Yeo, Cheol Jun Cho, David Harwath, David Mortensen  
**Venue**: Submitted to ACL (2026-02-21); code at github.com/juice500ml/phonetic-arithmetic  
**Scope**: 96 languages, multiple S3Ms (HuBERT/WavLM/wav2vec 2.0 class)

**Problem**: S3Ms encode rich phonetic information ‚Äî but is this information *structured*? What is the *geometry* of phonological representation?

**Method**:
1. Extract S3M representations of phoneme tokens (e.g., /d/, /t/, /p/, /b/)
2. Compute difference vectors: voicing_vector = [d] - [t], nasality_vector, etc.
3. Test arithmetic: [p] + voicing_vector ‚âà [b]? (analogous to word2vec: king - man + woman ‚âà queen)
4. Test scaling: larger voicing_vector magnitude ‚Üí stronger acoustic voicing?
5. Test across 96 languages for universality

**Key Findings**:
1. **Linear phonological directions exist** in S3M representation space (across all tested S3Ms)
2. **Compositionality confirmed**: [b] = [d] - [t] + [p] ‚Äî phonological features add/subtract linearly
3. **Continuous scaling**: magnitude of the phonological vector ‚àù degree of acoustic realization (e.g., partially voiced phones sit at intermediate positions on the voicing axis)
4. **96-language universality**: phonological vectors transfer cross-lingually ‚Äî the structure is language-agnostic

**Why this matters for Leo's research**:

### Connection 1: Audio T-SAE feasibility (Idea #7)
- Phonological features are LINEAR in S3M space ‚Üí SAE can capture them (SAE learns linear features by design)
- Compositionality = exactly what Matryoshka SAE is designed to handle (multi-scale hierarchical features)
- Audio T-SAE with temporal contrastive loss at phoneme boundaries will find these features cleanly
- The magnitude-scales-with-degree result means feature activation magnitude will be informative (not just binary presence)
- **Assessment**: Idea #7 (Audio T-SAE) is now even better motivated. The signal structure matches T-SAE's design assumptions perfectly.

### Connection 2: TCS(F) metric validation (Paper B AudioSAEBench)
- TCS(F) = within-phoneme variance / across-phoneme variance for SAE feature activations
- This paper confirms that phoneme boundaries ARE acoustically meaningful AND geometrically well-separated in S3M space
- MFA boundary ground truth ‚Üí phoneme segmentation is achievable
- 96-language coverage ‚Üí TCS(F) can be tested across languages (cross-lingual stability = one of Paper B's evaluation axes)
- **Assessment**: TCS(F) metric is now formally justified by this paper's evidence. Good citation.

### Connection 3: Minimal pair design for causal patching (Research Question #1)
- The paper constructs phonological contrast pairs by design: [d] vs [t] = same except voicing
- This IS the "clean/corrupt" paradigm needed for audio patching (Heimersheim & Nanda: minimal pairs = cleanest causal evidence)
- Method: take two speech tokens differing in exactly one phonological feature ‚Üí patch in S3M ‚Üí measure behavioral change
- **This paper provides the STIMULI DESIGN BLUEPRINT** for Leo's minimal-pair patching experiments
- Previously: Leo knew minimal pairs were needed (cycle #15 insight) but no concrete method
- Now: phonological vector arithmetic = the principled way to select/construct minimal pairs

### Connection 4: New Gap #18 ‚Äî Phonological vectors in S3M, but do they survive the connector into speech LLMs?
- This paper shows phonological features are linear and stable in S3M (encoder) representations
- **Critical unanswered question**: Does this linear structure SURVIVE through the connector module into the LLM?
  - If YES: LLM can use phonological features ‚Üí listening behavior is phonologically structured
  - If NO: connector = information bottleneck that destroys phonological geometry ‚Üí "modality collapse" (2602.23136) mechanism found
- Testing this: patch phonological direction vectors into LLM layers via NNsight ‚Üí measure if phonological distinction survives (behavioral change matches prediction)
- **Gap #18**: Nobody has tested whether phonological vector arithmetic persists through the connector in LALMs
- Venue: directly supports Paper A (Listen Layer) ‚Äî "where does the LLM stop consulting phonological structure?"

### Connection 5: Cross-language feature alignment
- 96 languages ‚Üí phonological features are universal across S3Ms
- This means AudioSAEBench features (if well-trained) should be cross-linguistically aligned
- Potential metric: "Cross-Lingual Phonological Alignment" ‚Äî do SAE features discovered on English align to those discovered on Mandarin? (using this paper's vector arithmetic approach)

## New Gap Formalized

**Gap #18: Phonological vector geometry in S3Ms ‚Äî does it survive the connector into speech LLMs?**
- What's known: phonological vectors are linear, compositional, scale-continuous in S3M encoders
- What's unknown: whether this structure is preserved by the connector module in LALMs like Qwen2-Audio
- Why it matters: if yes ‚Üí phonological grounding is tractable; if no ‚Üí connector is the bottleneck
- How to test: (1) extract phonological vectors from S3M encoder, (2) project through connector with NNsight, (3) test if phonological arithmetic still holds in LLM residual stream (layer 0)
- This is MacBook-feasible for Whisper-base + a simple connector

## Assessment: Is this paper a competitor?
- Track 2 (AudioSAEBench): NOT a direct competitor ‚Äî they don't build an SAE benchmark. But validates the approach.
- Idea #7 (Audio T-SAE): NOT a competitor ‚Äî they don't do T-SAE. Strongly motivates the idea.
- Paper A (Listen Layer): NOT a competitor ‚Äî they don't test LALMs or do causal patching at the LALM level.
- **Net assessment**: HIGH-VALUE SCAN. Provides (1) stimuli design blueprint, (2) metric justification, (3) new gap, (4) motivation for Idea #7.

## Next: 
- Update paper-b-pitch.md with phonological vector arithmetic citation (TCS(F) justification)
- Update goals.md with Gap #18
- arXiv Feb 28 batch: re-verify in 30 min (still delayed at 16:00)

## Tags: #S3M #phonology #linear-representation #AudioSAEBench #TCS #Audio-T-SAE #minimal-pairs #Gap18
