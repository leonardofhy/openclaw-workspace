# üß† Cycle #8 ‚Äî 2026-02-26 17:31
## Action: learn
## Context: Progress.md recommended AudioSAE as cycle #8 target. Completes the "3 anchors" trilogy (Beyond Transcription + AudioLens + AudioSAE). Fills SAE methodology gap for Track 2 and provides practical toolkit for Track 3 experiments.

## Content: Deep Read ‚Äî AudioSAE (Aparin et al., EACL 2026)
> "Towards Understanding of Audio-Processing Models with Sparse AutoEncoders"
> arXiv:2602.05027 | Huawei Noah's Ark Lab | EACL 2026 main track
> Code & checkpoints: https://github.com/audiosae/audiosae_demo

### Problem
SAEs are powerful for text LM interpretability (Anthropic, DeepMind) but their use in audio models is completely unexplored. Do audio SAEs find meaningful, stable, steerable features?

### Method
- Train SAEs on **all 12 encoder layers** of Whisper (base) and HuBERT (base/large)
- Architecture: TopK + BatchTopK variants (following Bussmann et al., Gao et al.)
- Expansion factor: 8x (hidden 768 ‚Üí SAE dict 6144 features)
- Novel metric: **cross-layer / cross-model feature stability** (% features consistent across random seeds)
- Evaluation suite (5 dimensions):
  1. **Stability**: seed-to-seed consistency (>50% stable!)
  2. **Classification-based probing**: top-k features, unlearning curves
  3. **Semantic Analysis**: phoneme classification via forced alignment, vowel unlearning
  4. **Steering**: suppress "speech hallucination" features to reduce false positives
  5. **EEG correlation**: check if SAE features align with brain response

### Results (key numbers)
| Finding | Number |
|---------|--------|
| Feature stability (consistent across seeds) | >50% |
| Phoneme classification acc (Whisper layer 12) | 0.92 |
| Phoneme classification acc (HuBERT layer 12) | 0.89 |
| Hallucination FPR reduction via steering | **70%** (0.37‚Üí0.11) |
| WER degradation from steering (moderate Œ±) | +0.4% (5.1‚Üí5.5%) |
| Features needed to erase 1 vowel phoneme | ~19% of features |
| Binary task features that capture most variance | k‚âà10-150 out of 6144 |
| Multi-class task features for task | k‚âà500-3000 |

### Layer-wise Domain Specialization (key insight)
- **Whisper layer 5**: peak music feature specialization (audio-level, 20-28%)
- **Whisper layer 6**: peak speech features (audio-level, ~13%), then sharp drop after layer 6
- **Whisper layer 7**: speech info transitions from audio-level ‚Üí frame-level (local encoding)
- Divergence point at layer 6-7 = critical transition: global semantic ‚Üí local phonetic
- **HuBERT layer 11**: features 3249, 3081 = speech-boundary detectors (start/end of speech)

### Concrete Feature Examples
- Laughter: detected by both models (HuBERT layers 1-6, Whisper layers 1,6,9)
- Whisper (the sound): HuBERT SAE feature 6106 (layer 4), F1=0.6
- Sigh, sneezing: distributed across layers 1-7
- Animal sounds, breathing: NOT reliably identified ‚Üí distributed encoding
- Auto-interpreted features: "ringing alarms", "birds chirping", "guitar playing"

### Key Limitation (important for Leo's work)
- **Erasing speech concepts requires FAR more features than text SAEs**
  - Text SAE: erasing "gender" ‚Üí tens of features
  - Audio SAE: erasing "accent" ‚Üí ~2000 features
  - Reason: phonetic info is highly redundant + distributed (not monosemantic like text)
- Auto-interpretation of phonetic features fails ‚Üí caption model was music-trained, not speech-trained
- Only base/small models tested; WavLM, large variants not covered

### Steering Pipeline (usable in Leo's work)
1. Identify "hallucination" features from non-speech datasets (FSD50k, WHAM, Musan)
2. Compute top-100 features by discriminative score
3. Inject SAE on last encoder layer; negate hallucination features with Œ± scale factor
4. Sweet spot: Œ±=1 ‚Üí 70% FPR reduction, only +0.4% WER degradation
5. Œ±=3 ‚Üí better FPR but severely degrades speech comprehension
‚Üí **This is the exact template for Leo's Track 5: Safety Mechanistic Defenses**

### EEG Correlation (bonus finding)
- Some SAE features correlate with Pz electrode activity during speech perception
- Lag range: 0-500ms; both positive and negative correlations found
- Many features active on specific vowels (but not all instances) ‚Üí partial phoneme specificity
- Implication: audio SAE features = cognitively plausible, not just model-internal artifacts

## Connection to Goals

| Track | Connection |
|-------|-----------|
| **Track 2: AudioSAE** | This IS the baseline Track 2 paper. Leo's contribution = extend evaluation (SAEBench-style), causal evaluation (not just probing), cross-model alignment metrics |
| **Track 3: Listen vs Guess** | Steering pipeline directly applicable: can steer out "audio" features to create corrupted conditions for patching experiments |
| **Track 5: Safety** | 70% hallucination reduction via feature steering = proof-of-concept for safety defenses; exact same framework applies to jailbreak defense |
| **Skill Gap: SAE training** | Paper gives concrete training recipe: TopK/BatchTopK, 8x expansion, all-layer coverage |

## Synthesis Across 3 Anchors (Beyond Transcription + AudioLens + AudioSAE)

```
                    ENCODER                      LALM
Beyond Transcription:  saturation layer ‚Üê‚Üí  AudioLens: critical layer
                       (where semantics resolve)
AudioSAE:              layer 6-7 transition point
                       (audio-level ‚Üí frame-level)
                       ‚Üì
                Unified picture: layers 6-7 in Whisper =
                semantic-acoustic transition zone
                = best target for SAE probing + patching experiments
```

**New synthesis insight**: AudioSAE's layer 6-7 transition point may be the SAME phenomenon as "saturation layer" (Beyond Transcription) and "critical layer" (AudioLens). Three papers independently found the same architectural feature from different angles. This unified picture = strong foundation for Leo's benchmark paper.

## Open Questions (for goals.md)
1. Does AudioSAE's layer 6-7 speech transition correspond to Beyond Transcription's saturation layer in Whisper?
2. Can the steering pipeline (suppress top-100 features) be adapted for causal patching experiments?
3. AudioSAE uses encoder-only models. What happens when you apply SAE to the full LALM pipeline (Qwen2-Audio, etc.)?
4. The phoneme auto-interpretation failed due to bad caption model ‚Üí Leo's opportunity: use ASR-trained auto-interpretation instead

## Next: Cycle #9 Recommendation
**skill-up**: TransformerLens + pyvene installation and first run on a text model.
Reason: 3 deep reads completed (foundations solid). Time to move from reading to doing.
The "3 anchors" are now understood; next knowledge gap is practical implementation.
MacBook-feasible. ~45-60 min setup.

## Tags: #AudioSAE #SAE #Whisper #HuBERT #steering #hallucination #layer-specialization #phoneme #EEG #track2 #track3 #track5
