# ðŸ§  Cycle #37 â€” 2026-02-27 10:04
## Action: learn
## Context: Queue was depleted as of cycle #36; checked if new papers appeared overnight. Found AR&D (arXiv:2602.22253, Feb 24, ICASSP 2026) â€” missed in previous scans. High-value read: "first MI framework for AudioLLMs using SAEs". Directly relevant to Tracks 2+3.

## Content: Deep-read AR&D (Chowdhury et al., ICASSP 2026)

**Paper**: AR&D â€” "A Framework for Retrieving and Describing Concepts for Interpreting AudioLLMs"
**Authors**: Townim Faisal Chowdhury, Ta Duc Huy, Siqi Pan, Jeremy Stoddard, Zhibin Liao
**Venue**: ICASSP 2026 (accepted)
**arXiv**: 2602.22253 (submitted Feb 24, 2026)
**Project page**: https://townim-faisal.github.io/AutoInterpret-AudioLLM/

### Core Problem
AudioLLMs are polysemantic: individual neurons fire for unrelated audio concepts â†’ opaque, hard to trust. Unlike text LLM interpretability (mature tooling), AudioLLMs have no interpretability framework yet.

### Method: AR&D Pipeline
1. **R (Retrieve)**: For each SAE feature, find representative audio clips (max-activating examples)
2. **D (Describe)**: Auto-caption these clips using an audio captioner â†’ assign meaningful names to features
3. **Validate**: Human evaluation of concept names + steering (ablation/activation) to confirm causality

### Key Findings
- AudioLLMs encode **structured and interpretable features** (not noise)
- SAE successfully disentangles polysemantic neurons into monosemantic features
- Automated concept naming pipeline achieves high human agreement
- Steering (concept activation/suppression) works â†’ causal validation

### Critical Distinction: AR&D vs AudioSAE
| | AudioSAE (Aparin 2026) | AR&D (Chowdhury 2026) |
|---|---|---|
| Model type | Encoder-only (Whisper, HuBERT) | AudioLLMs (multimodal: audio + text) |
| Focus | Layer-wise SAE + hallucination steering | Feature naming + human interpretability |
| Causal validation | Activation steering (suppress top-100 features) | Steering + human evaluation |
| Gap | No feature naming, no AudioLLMs | No causal patching, no grounding test |

### KEY GAPS (Leo's opportunity)
1. **No causal patching**: AR&D does steering (necessary condition), but not denoising (sufficient). Doesn't tell you if AudioLLM uses audio input or prior/text context â†’ grounding_coefficient untested
2. **No audio-vs-text pathway test**: Their SAE features fire for audio concepts, but nobody asks "does this feature activate because of the audio signal or because of the text prompt?" = Track 3's core question
3. **Automated naming is noisy**: They rely on auto-captioner for concept labels â€” accuracy unclear; SAE + minimal pair corruptions would be more precise

### Connection to Leo's Research
- **Track 2 (AudioSAEBench)**: AR&D = new benchmark target; their auto-interpretation pipeline should be part of AudioSAEBench evaluation
- **Track 3 (Listen vs Guess)**: Their monosemantic features are exactly what grounding_coefficient should test â€” does feature X activate from audio or text? Neither AR&D nor AudioSAE answers this.
- **Synthesis**: AR&D (what features exist) + Leo's patching (why features activate, audio vs text) = a very clean contribution split

### Field Status Update
Audio MI at AudioLLM level now has **4 papers**:
1. AudioLens (NTU, ASRU 2025) â€” logit lens, no patching
2. SPIRIT (MBZUAI, EMNLP 2025) â€” patching for safety defense, not interpretability
3. Zhao et al. (JHU, Jan 2026) â€” emotion neurons, necessity only
4. **AR&D (ICASSP 2026, Feb 24)** â€” SAE + auto-interpretation, no audio-vs-text test

**No paper has ever done denoising patching on AudioLLMs to test causal grounding.** Leo is still first.

## Next: queue is now fully updated. Cycle #38 â†’ Leo action queue or skip if no high-value learn available before 14:00 Taipei arXiv update.

## Tags: #AudioLLM #SAE #mechanistic-interpretability #AR&D #ICASSP2026 #polysemanticity #Track2 #Track3 #gap-found
