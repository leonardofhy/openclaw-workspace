# üß† Cycle #27 ‚Äî 2026-02-27 03:30
## Action: learn
## Context: 3:30 AM, post-cycle-26. Must-read list exhausted. Two remaining high-value tasks: (1) deep-read Mariotte 2509.24793 (SAE for audio SSL, only abstract-read so far), (2) quick arXiv scan for newly published papers. No Leo feedback to integrate. Night is not a skip reason per Leo's explicit instruction.

---

## Content

### Part A: Deep Read ‚Äî Mariotte et al. "Sparse Autoencoders Make Audio Foundation Models more Explainable" [arXiv:2509.24793, ICASSP 2026]

**Authors:** Th√©o Mariotte, Martin Lebourdais, Antonio Almud√©var, Marie Tahon, Alfonso Ortega, Nicolas Dugu√© (University of Le Mans + Zaragoza)

**Key Setup:**
- Models: 4 audio foundation models ‚Äî AST (sound), HuBERT (speech), WavLM (speech), MERT (music) ‚Äî all 13 layers, 768-dim hidden states
- SAE type: TopK SAE (N=2048 features, 8x expansion like AudioSAE)
- Task: singing technique classification on VocalSet (10 vocal techniques, 20 singers, 10 hours)
- Key method: mean-pooled hidden states ‚Üí SAE sparse codes ‚Üí linear probe + factor disentanglement analysis

**Layer Selection:**
- AST: best layers 6 and 12 (81.8% / 82%, stable across layers 3‚Äì12)
- WavLM: best layer 1 (72.5%) ‚Äî dramatically drops at layer 12 (55%) ‚Üí **information is EARLY not LATE**
- HuBERT: best layer 3 (73%) ‚Üí also drops at layer 12 (59.8%)
- MERT (music): best layers 4 and 7 (72.5% / 76.2%)
- **Key insight**: speech SSL models (WavLM, HuBERT) peak EARLY for this task because singing technique is a low-level acoustic attribute, not a linguistic one. This is the OPPOSITE of AST which is more stable across layers.

**SAE Performance:**
- SAE sparse codes retain task information across all models (slight drop at very high sparsity ‚â•90%)
- At 75-85% sparsity: SAE codes perform on par with dense linear probes on original representations
- Reconstruction quality degrades with sparsity as expected (consistent with LLM SAE behavior)
- WavLM shows optimal MSE at 85% sparsity (trade-off exists)

**Disentanglement Analysis (Key Contribution):**
- Method: 9 voice attributes (pitch, shimmer, loudness, spectral rolloff, centroid, pitch range, jitter, harmonic-to-noise ratio, median pitch)
- Metric: Informativeness (R¬≤) and Completeness (from "A Benchmark for Interpretability" framework)
- **Finding: SAEs significantly increase completeness for all 4 models** ‚Äî sparse codes encode voice attributes more independently/cleanly than dense hidden states
- Completeness = one sparse feature maps to one voice factor (not spread across many) ‚Üí "disentangled"
- This holds even at higher sparsity (90%+) for some models ‚Üí SAE robustly disentangles

**Comparison with AudioSAE:**
| Aspect | Mariotte 2509.24793 | AudioSAE (Aparin 2602.05027) |
|--------|---------------------|-------------------------------|
| Models | AST, HuBERT, WavLM, MERT | Whisper, HuBERT |
| Expansion | 8x (N=2048) | 8x (N=6144) |
| Task | Singing technique | Speech/phonetics/ASR |
| Steering | ‚ùå No causal interventions | ‚úÖ 70% hallucination reduction |
| Evaluation | Completeness + R¬≤ | Feature stability + WER |
| Layer coverage | Best layer only | All layers |
| Temporal pooling | Mean pool (loses time) | Frame-level |

**Key New Gap (from Mariotte):**
- Mean pooling LOSES temporal structure of sparse features ‚Üí no information about WHEN a feature activates during an utterance
- AudioSAE keeps frame-level structure ‚Üí can localize feature activation in time
- **Mariotte's limitation = opportunity**: a temporally-resolved SAE for audio SSL would show where in the temporal sequence each disentangled feature fires ‚Üí connects to "Listen vs Guess" (which audio token positions carry the critical information?)

**Code:** https://github.com/theomariotte/sae_audio_ssl

---

### Part B: New Paper Scan ‚Äî Plantinga et al. "From Black Box to Biomarker: Sparse Autoencoders for Interpreting Speech Models of Parkinson's Disease" [arXiv: July 2025]

**Authors:** Peter Plantinga, Jen-Kai Chen, Roozbeh Sattari, Mirco Ravanelli, Denise Klein (McGill/Mila)

**Abstract summary:**
- SAE applied to speech models (WavLM/similar) for Parkinson's disease detection
- Goal: find interpretable biomarkers from black-box speech model activations
- Contributions: clinical application of audio SAE methodology

**Relevance to Leo:**
- LOW direct relevance (clinical application domain = PD, not ASR/LLM grounding)
- MEDIUM methodology relevance: SAE for downstream task interpretation (same infrastructure as Leo's Track 2)
- Confirms SAE for audio SSL is a real movement ‚Äî 3 papers now: AudioSAE + Mariotte + Plantinga

**Scan verdict:** Abstract-level only sufficient. No deep read needed.

---

### Synthesis: Completing the AudioSAEBench Picture

With Mariotte now deep-read, the 3-paper AudioSAE landscape is:

| Paper | Models | Contribution | Gap |
|-------|--------|-------------|-----|
| AudioSAE (Aparin 2026) | Whisper, HuBERT | Layer-wise SAE, steering, phonetic features | Encoder-only, no LALMs |
| Mariotte (2026) | AST, HuBERT, WavLM, MERT | Disentanglement + completeness | No causal interventions, no temporal |
| Plantinga (2025) | WavLM-like | Clinical biomarkers | Narrow domain |

**Track 2 (AudioSAEBench) positioning:** Neither Mariotte nor AudioSAE evaluates cross-model feature alignment. Neither provides a unified evaluation framework. AudioSAEBench = fill this meta-evaluation gap. Leo can propose: (a) unified metrics across all 3 paper's methods, (b) temporal-resolution analysis (what Mariotte lacks), (c) causal steering as mandatory benchmark component (what Mariotte lacks but AudioSAE has).

**New Synthesis Connection:**
WavLM and HuBERT peak early (layers 1-3) for singing technique ‚Äî same models peak LATE (layer 9+) for phonetics. This layer-specificity is model+task dependent. For AudioSAEBench: must control for BOTH layer selection AND task type to make cross-model comparisons valid.

---

## Key Findings

1. **Mariotte deep-read complete** ‚úÖ ‚Äî singing technique SAE confirms disentanglement; but mean-pooled (no temporal), no causal steering = significant gap
2. **Layer specificity**: speech SSL models encode acoustic/low-level attributes EARLY (layer 1-3), NOT late ‚Äî opposite intuition from language models
3. **New gap identified (#12)**: Temporally-resolved SAE for audio = nobody has done this. Mariotte loses time; AudioSAE has it but doesn't analyze temporal activation patterns systematically
4. **3 audio SAE papers confirmed** ‚Äî enough for a field survey/benchmark paper (AudioSAEBench)
5. **Plantinga SAE PD paper** scanned: clinically relevant, methodologically similar, no deep read needed

## Next: 
- **CYCLE QUEUE IS DEPLETED** ‚Äî must-read fully done, all bonus papers read
- Next high-value actions require Leo/resources: (a) real speech test, (b) NNsight API, (c) experiment design for Leo review
- 3:30 AM ‚Äî natural breakpoint, no skip stigma per Leo's instruction
- **RECOMMEND SKIP** for next cycle (03:30+04:00 window) UNLESS new arXiv papers appear overnight

## Tags: #mariotte #audioSAE #Track2 #disentanglement #temporalSAE #gap #deepRead
