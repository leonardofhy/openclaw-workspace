# ðŸ§  Cycle #38 â€” 2026-02-27 11:07
## Action: learn
## Context: arXiv Feb 27 batch not yet posted (~14:00 Taipei). Must-read list items 1-9 done; item #8 (SAEBench) identified as unread. Last item on must-read list. Directly informs Track 2 AudioSAEBench design.

## Content: **SAEBench Deep Read** (Karvonen, Rager, Nanda et al., ICML 2025) [arXiv:2503.09532]

### Problem
Prior SAE work evaluates only with **proxy metrics** (sparsity + reconstruction fidelity).
These don't reliably predict practical utility. Need a multi-metric benchmark.

### Method (8-metric framework)
SAEBench evaluates SAEs across **4 capability categories**, 8 metrics total:

| Category | What it measures |
|----------|-----------------|
| **Concept Detection** | Do individual latents map to meaningful concepts? (Probing + ground truth labeling) |
| **Interpretability** | Feature comprehensibility via automated LLM explanation scoring |
| **Reconstruction** | Faithfulness of SAE to model activations (fidelity at sparsity) |
| **Feature Disentanglement** (2 novel) | Are independent concepts properly separated? (Feature Absorption, Composition) |

**Key metric details:**
- Concept Detection: uses sparse probing over labeled concept datasets
- Interpretability: auto-interp with LLM scoring (but struggles to differentiate architectures â€” a known weakness)
- Reconstruction: standard CE loss delta at given L0
- Feature Disentanglement: two NEW metrics â€” Feature Absorption (Chanin et al.) and independent composition test

### Results
- **Matryoshka SAEs**: slightly underperform on proxy metrics, but substantially OUTPERFORM on feature disentanglement â†’ disentanglement grows with scale
- **Proxy metrics fail**: gains on sparsity/fidelity â‰  gains on interpretability or disentanglement
- **Feature Absorption** is a real problem: high sparsity can mask fragmented features that are not actually monosemantic
- **200+ SAEs** benchmarked: ReLU, Gated, TopK, BatchTopK, JumpReLU, P-anneal, Matryoshka, Switch, Feature Choice

### Connection to Leo's Goals

**Track 2 (AudioSAEBench) â€” SAEBench is the template:**
- AudioSAEBench should NOT just use one metric (AudioSAE used feature stability + steering, Mariotte used completeness)
- AudioSAEBench should adopt SAEBench's multi-category structure, but with audio-native versions:
  
  | SAEBench Category | AudioSAEBench Analog |
  |-------------------|--------------------|
  | Concept Detection | Phoneme/emotion/accent probing on sparse features |
  | Interpretability | Auto-caption pipeline (like AR&D) â€” but audio is harder |
  | Reconstruction | CE delta on ASR task at given L0 |
  | Feature Disentanglement | **Grounding sensitivity**: does each feature respond to audio or text? NEW metric |

- **New metric idea**: "Grounding Sensitivity" = for each SAE feature, compute grounding_coefficient (audio patch sensitivity). Features with gc â‰ˆ 1 = truly audio-grounded; gc â‰ˆ 0 = text-context driven. This is an AUDIO-NATIVE disentanglement metric that SAEBench has no equivalent of.

**GAP #15 (NEW):** SAEBench exists for text LLMs (7 architectures Ã— 200+ SAEs benchmarked). NO equivalent for audio/speech models. AudioSAEBench = this gap filled. And "Grounding Sensitivity" = a metric that doesn't exist in SAEBench â†’ genuine novel contribution.

**Architecture guidance for Track 2:**
- Use TopK or BatchTopK SAE (standard in AudioSAE + Mariotte already)
- Consider Matryoshka SAE for better feature disentanglement (novel for audio SAE)
- Evaluate using SAEBench-style multi-metric â€” don't just report fidelity

**Must-read list status:** âœ… FULLY COMPLETED (all 10 items now read)
- Item #8 SAEBench = done in this cycle

## Open Questions
1. Can Matryoshka SAE's disentanglement advantage extend to audio (temporal structure)?
2. Does Feature Absorption happen in audio SAEs? (AudioSAE uses BatchTopK which partially mitigates it)
3. What's the right "ground truth" concept set for audio SAEBench? (Phonemes? Acoustic features? Emotion labels?)
4. Is "Grounding Sensitivity" metric computable with real audio + text minimal pairs? (Yes, with pyvene/NNsight)

## Next: arXiv Feb 27 batch available ~14:00 Taipei â†’ cycle #39 = arXiv scan. Queue otherwise depleted. Leo to approve IIT experiment.

## Tags: #SAE #SAEBench #AudioSAEBench #Track2 #evaluation #methodology #Matryoshka
