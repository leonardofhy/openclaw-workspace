# üß† Cycle #71 ‚Äî 2026-02-28 11:01
## Action: learn (deep-read)
## Context: T-SAE found in cycle #70 triage as direct methodology for Gap #12 (temporal audio SAE). arXiv Feb 28 batch not yet posted (~14:00). This is a genuine new read with Track 2 implications ‚Äî not redundant.

## Content

### Paper: "Temporal Sparse Autoencoders: Leveraging the Sequential Nature of Language for Interpretability"
- **arXiv:2511.05541v2** (updated Feb 25, 2026) ‚Äî Harvard/MIT (Bhalla, Oesterling et al.)
- **Venue: ICLR 2026 Oral** ‚≠ê ‚Äî meaning this is a landmark paper in the SAE literature
- **Code**: https://github.com/AI4LIFE-GROUP/temporal-saes

### Core Problem
Standard SAEs treat tokens as i.i.d. ‚Üí recover token-specific, noisy, LOCAL concepts (syntactic artifacts like "sentence ending" or "capitalized first word"). This misses HIGH-LEVEL semantic content that evolves SMOOTHLY over sequences.

### Key Insight
Language has two types of structure:
- **High-level / global (semantic)**: invariant over multiple tokens in a sequence ‚Äî "discussion of plant biology", "scientific explanation"
- **Low-level / local (syntactic)**: specific to individual token positions ‚Äî "plural noun", "sentence start"

Standard SAEs conflate these. T-SAE disentangles them.

### Method: T-SAE
1. **Data Generating Process formalized**: speaker produces token œÑ_t from latent h_t (high-level, time-invariant) + l_t (low-level, token-specific). Goal: recover h_t and l_t separately.
2. **Architecture**: partition SAE features into high-level (first 20%) and low-level (remaining 80%), Matryoshka-style hierarchical loss
3. **Temporal contrastive loss**: `‚Ñí_contr` = contrastive loss on HIGH-LEVEL features between ADJACENT tokens `(z_t, z_{t-1})`
   - Positive pair: same sequence ‚Üí should have similar h_t
   - Negative pairs: different sequences ‚Üí should differ
   - Prevents smoothness collapse: no all-same features
4. **Full loss**: `‚Ñí = ‚Ñí_matr + Œ± * ‚Ñí_contr` where Œ±=1.0

### Key Results
- **Semantics**: T-SAE high-level features cluster by TOPIC (biology, medicine, history) ‚Äî Matryoshka SAE doesn't
- **Context**: T-SAE high-level features cluster by SEQUENCE IDENTITY ‚Äî tokens from the same question look similar
- **Syntax**: T-SAE LOW-LEVEL features cluster by part-of-speech ‚Äî correctly relegated to local features
- **Reconstruction**: T-SAE matches BatchTopK/Matryoshka SAEs on standard benchmarks (not sacrificed)
- **Temporal consistency**: smoother activation trajectories over sequences vs noisy per-token activations
- **Safety case study**: T-SAE detects "jailbreak-related" semantic concepts more reliably than Matryoshka SAE ‚Äî practical safety benefit

### Audio Transfer Hypothesis (Gap #12 connection)
**Critical observation from the T-SAE authors**: "all of these works assume a fully unsupervised objective for learning SAEs, treating each token in the training data as i.i.d., without acknowledging the temporal aspect of language *and other sequential modalities*" (emphasis mine).

The paper explicitly notes this limitation extends to "other sequential modalities" ‚Äî they're pointing at audio without doing it.

**Why T-SAE should work BETTER on audio than text:**
1. **Phoneme structure = natural adjacent-pair signal**: a phoneme spans ~5-10 frames at 20ms resolution; consecutive frames within a phoneme should share the same high-level feature (the phoneme identity). T-SAE's adjacent-token contrastive loss = EXACTLY the right prior.
2. **Prosodic/speaker structure = long-range consistency**: speaker identity, emotion, accent persist across utterances ‚Üí long-range variant of T-SAE contrastive pairs
3. **Speech SAE problem diagnosis**: Mariotte et al. mean-pool over time ‚Üí loses temporal info; AudioSAE tracks frame-level but doesn't analyze temporal patterns ‚Äî BOTH have the same i.i.d. token problem T-SAE solves
4. **Method is drop-in**: T-SAE is standard SAE + contrastive loss. Can be applied on top of any Whisper/HuBERT encoder with minimal code change.

### Experiment sketch: Audio T-SAE
- **Setup**: Train T-SAE on Whisper-small encoder (layer 3-5, the semantic crystallization zone) on LibriSpeech audio
- **Contrastive pairs**: (frame_t, frame_{t-1}) = adjacent frames within same utterance; negatives = frames from different utterances
- **Hypothesis**: T-SAE high-level features should segment cleanly at phoneme boundaries; low-level = sub-phoneme variation
- **Extended pairs**: (phoneme-center, phoneme-neighbor) for longer-range prosodic features
- **Evaluation**: phoneme classification probe on high-level features vs low-level features (should cleanly separate)
- **Connection to Track 2**: AudioSAEBench 5th category "Temporal Resolution" could evaluate exactly this ‚Äî when does each feature fire during utterance, does it respect phoneme boundaries?

### NEW SYNTHESIS: T-SAE + AudioSAEBench
Gap #12 (temporal audio SAE) now has CONCRETE METHOD:
- **AudioSAEBench metric #5b**: "Temporal Coherence Score" = within-phoneme variance / across-phoneme variance of high-level feature activations (T-SAE should score higher than standard SAE)
- This adds a SECOND novel metric to AudioSAEBench beyond Grounding Sensitivity:
  - `gc(F)` = Grounding Sensitivity (from Paper A)
  - `TCS(F)` = Temporal Coherence Score (new, from T-SAE ‚Üí audio)
- Two novel metrics = stronger paper A claim for AudioSAEBench (Paper B)

### Connection to Listen Layer Hypothesis (Paper A)
- T-SAE identifies "when" a semantic feature is active within a sequence
- Applied to LALM encoder: when audio tokens are processed by the listen layer, T-SAE high-level features there should be stable within-phoneme and should exhibit audio-specific coherence (not text-rhythm coherence)
- Contrast: text-dominated layers ‚Üí T-SAE features follow text token rhythm, not phoneme boundaries
- **New diagnostic**: T-SAE temporal coherence as a PROXY for audio vs text processing ‚Äî if a layer's SAE features are temporally coherent at PHONEME timescale, that layer is "listening"; if coherent at TEXT TOKEN timescale, it's "guessing"
- This is a non-causal version of grounding_coefficient ‚Äî could be a validation/triangulation metric for Paper A

### Novelty Assessment
- T-SAE: ICLR 2026 Oral ‚Äî highly credible, well-vetted method
- Audio extension: explicitly suggested by authors ("other sequential modalities") but NOT done by them or anyone
- Two novel metrics for AudioSAEBench (+TCS)
- Triangulation metric for Paper A (phoneme-timescale T-SAE coherence = proxy for "listening")
- Rating: ‚≠ê‚≠ê‚≠ê HIGH VALUE ‚Äî adds concrete methodology + opens new metric

## Next: arXiv Feb 28 batch at ~14:00 ‚Üí check for new speech MI papers. T-SAE notes should be integrated into KG section B and AudioSAEBench plan (paper-b-pitch.md). Meta-board remains saturated ‚Äî no new meta work needed.
## Tags: #T-SAE #SAE #temporal #audio #Track2 #AudioSAEBench #ICLR2026 #methodology
