# ðŸ“‹ 2026-02-27 Learning Digest (Day 2)

> Digest created at 21:01 (cycle #46 daily-consolidate). 22 cycles total today (incl. 6 skips, 1 plan, 1 report, 1 reflect, 13 learn/skill-up/build).

---

## Morning Cycles (#19â€“#28): Foundations + Tooling Complete

### Cycle #19: Causal Abstraction Theory (Geiger et al., arXiv:2301.04709) â€” DEEP READ
- **Core**: IIT = activation patching formalized. H is a causal abstraction of L â†” interchange interventions match predictions.
- Noising = test NECESSITY; denoising = test SUFFICIENCY. Choose based on claim.
- 10 MI methods unified (activation patching, SAE, DAS, logit lens, steering, causal tracingâ€¦) â€” all special cases of IIT.
- **Grounding coefficient** = relative IIT accuracy: gc = Î”acc(audio patch) / Î”acc(all patches) â€” theoretically grounded.
- **New framing for Leo's research**: not "adding patching to AudioLens" â€” it's "validating audio representations as causal abstractions."
- **Triple Convergence IIT testable**: if transition zone exists, IIT accuracy should peak at layer ~50% depth. First causal validation.

### Cycle #22: Multimodal MI Survey (Lin 2025, arXiv:2502.17516) â€” DEEP READ (FINAL MUST-READ âœ…)
- Covers CLIP/LLaVA/Stable Diffusion only. **Speech COMPLETELY ABSENT** â€” confirms Leo's white space.
- Method progression ladder: probing â†’ logit lens â†’ causal tracing â†’ SAE â†’ circuits. Leo's research follows this correctly.
- Hallucination mitigation = "underdeveloped open problem" the survey calls out â†’ Leo's IIT experiment addresses it.
- **MUST-READ LIST NOW FULLY COMPLETED (10/10)** âœ…

### Cycle #24: arXiv cs.SD Feb 26 Scan
- 5 papers, 4 irrelevant. Key find: **EmoOmni (ICML 2026)** â€” Thinker-Talker architecture for emotional speech generation.
- **Gap #13**: nobody has mapped WHERE in Thinker-Talker emotional info is lost (connector bottleneck? early layers?).
- Leo can do this with logit-lens + patching on Thinker-Talker interface. Extends Track 3 + Track 5.

### Cycle #25: Zhao et al. 2601.03115 (ESNs in LALMs) â€” DEEP READ
- ESNs causally validated in Qwen2.5-Omni/Kimi-Audio/Audio Flamingo 3 via SwiGLU hook + MAD/CAS selectors.
- ESN clusters at layers 0, 6-8, 19-22 â†’ **matches Triple Convergence layer distribution**.
- **KEY GAP**: their ESN deactivation never asks "is this neuron responding to audio emotion or linguistic context?"
- Leo's grounding_coefficient applied at neuron level = Track 3's contribution at neuron granularity.
- ESNs are non-additive â†’ SAE decomposition needed â†’ Track 2 + Track 3 intersection.

### Cycle #26: Kawamura 2602.15307 (EUSIPCO 2026) â€” DEEP READ
- AAPE method finds class-specific neurons in M2D SSL (12L Ã— 3072 neurons); SSL achieves ~100% class coverage vs SL's 49%.
- Neurons encode gender/pitch/arousal/language-family/genre. "Shared responses" = polysemanticity â†’ SAE needed.
- Deactivation = functional impact confirmed (necessity test).
- **New paper idea**: "Class-specific Neuron Grounding" â€” AAPE + patching + gc on LALM = Track 2+3 synthesis.
- **Gap #11**: no audio-vs-text pathway test for class-specific neurons in LALMs.

### Cycle #27: Mariotte 2509.24793 (ICASSP 2026) â€” DEEP READ + Plantinga SAE-PD scan
- TopK SAE on AST/HuBERT/WavLM/MERT; speech SSL peaks EARLY (layer 1-3) vs LLMs (deep).
- Mean-pooled approach destroys temporal resolution.
- **Gap #12: temporally-resolved SAE** â€” when does each feature fire *during* the utterance? Nobody has done this.
- 3-paper audio SAE field (AudioSAE + Mariotte + AR&D) now complete.

### Cycle #28: NNsight API Assessment â€” SKILL-UP âœ…
- **NNsight wins vs pyvene**: cleaner syntax, NDIF remote execution (run Qwen2-Audio-7B without local GPU!).
- Used in "Behind the Scenes" (Whisper SER MI) â€” proven on encoder models.
- arXiv check: 0 new papers in target domain.
- Cheat sheet updated with NNsight section.
- Action: `pip install nnsight` when creating venv.

---

## Morning Coordination (#34â€“#36): Plans, Proposals, Knowledge-Graph

### Cycle #34: Triple Convergence IIT Experiment Proposal
- **Experiment 1** (MacBook-feasible, ~3h): IIT causal test of saturation layer.
  - 10 LibriSpeech clip pairs; NNsight patching; denoising sweep across layers.
  - Expected: IIT accuracy peaks at Whisper-small layer 6.
  - Metric: logit diff (per Heimersheim & Nanda).
  - Claim: "first causal confirmation of the Triple Convergence Hypothesis."
- **Experiment 2** (GPU needed): Class-specific Neuron Grounding at ESN level (AAPE + gc on Qwen2.5-Omni).
- Both proposals formalized â†’ awaiting Leo approval.

### Cycle #35: Morning Handoff Report
- arXiv Feb 27 not yet live. Queue depleted. Overnight summary: 3 deep reads, 2 gaps, NNsight assessed, IIT proposal ready.

### Cycle #36: Knowledge-Graph Tidy
- Added IIT Experiment 1 (cycle #34), Gap #13 (EmoOmni bottleneck), Paper Ideas #5-6 (Class-specific Neuron Grounding + Temporal Audio SAE).

---

## Afternoon Cycles (#37â€“#41): Field Acceleration Confirmed

### Cycle #37: AR&D (Chowdhury et al., ICASSP 2026, arXiv:2602.22253) â€” DEEP READ
- "First MI framework for AudioLLMs" â€” SAEs to disentangle polysemantic neurons + auto-concept naming (AR&D).
- **KEY GAP**: no causal patching, no audio-vs-text pathway test â†’ Track 3's gc still untested.
- Now 4 papers at AudioLLM level; none do denoising patching. Leo still first.

### Cycle #38: SAEBench (Karvonen, Nanda et al., ICML 2025, arXiv:2503.09532) â€” DEEP READ
- 8-metric framework across 4 categories: Concept Detection, Interpretability, Reconstruction, Feature Disentanglement.
- Matryoshka SAE wins disentanglement despite underperforming on proxy metrics.
- **Proxy metrics fail**: sparsity + fidelity don't predict practical utility.
- **AudioSAEBench template**: adopt 4-category structure + add "Grounding Sensitivity" (gc per feature) = novel audio-native metric.
- **Gap #15 (NEW)**: no SAEBench equivalent for audio/speech models.
- **MUST-READ LIST: COMPLETE âœ…** (10/10, SAEBench was final item)

### Cycle #39: arXiv Scan (13:01)
- API rate-limited; found MiSTER-E (2602.23300, IISc/Microsoft) â€” MoE gating weights measure "listen vs guess" behaviorally but not mechanistically. Strengthens Track 3 motivation: "behavior shows modality dominance â†’ mechanism unknown â†’ Leo localizes causally."
- Feb 27 batch not yet live.

### Cycle #40: Feb 27 arXiv Batch â€” 3 Major Track 3 Papers â­â­â­

**PAPER 1: Modality Collapse (Billa et al., 2602.23136)**
- Audio info (speaker, emotion, visual) survives EVERY LLM layer but decoder can't USE it.
- Formal: GMI bound â†’ decoder scoring rule is the bottleneck.
- Removing 64-71% of modality-specific variance IMPROVES decoder loss (!) â€” decoder has no learned use for these directions.
- **Gap #14**: GMI bottleneck not layer-localized. Leo's IIT patching answers this.
- NEW FRAMING: Leo's experiment asks "where does the decoder START being able to access audio?" not just "is it there?"

**PAPER 2: Cascade Equivalence Hypothesis (Billa et al., 2602.17598)**
- Most speech LLMs = implicit ASR cascades; confirmed by logit lens + LEACE erasure.
- **Exception: Qwen2-Audio genuinely diverges** (Îº=0.87 vs Ultravox Îº=0.93) â€” most interesting model for Track 3.
- LEACE ablating text representations â†’ accuracy collapses to near-zero (strong causal claim, but GLOBAL).
- **Gap #15 (revised)**: no layer-wise causal localization of cascade-equivalent behavior. LEACE is global; patching is local.

**PAPER 3: ALME Benchmark (Billa et al., 2602.11488)**
- 57,602 audio-text conflict stimuli, 8 languages, 4 models.
- Text dominance = 10Ã— higher for A-T conflict than T-T conflict.
- LoRA on LLM halves text dominance; training only audio projection INCREASES it â†’ text dominance lives in LLM reasoning, not audio encoder.
- **Gap #16**: no layer-wise causal patching on conflict stimuli. Leo can extend ALME directly.
- **BONUS**: 57K stimuli already exist â€” no need to generate own. Clean direct experiment.

### Cycle #41: Forced Reflect (15:01)
- **"Listen Layer Hypothesis" crystallized**: speech LLMs have a small set of attention heads at specific layers where audio is causally consulted. Below = audio encoded but not consulted; above = text dominates.
- Leo's denoising patching sweep = only method that localizes this. Paper title candidate: "Localizing the Listen Layer in Speech LLMs."
- **Competition velocity**: ~2 papers/week in Feb 2026; ALL are behavioral; NONE do layer-wise causal patching.
- **Bottleneck = Leo**: IIT experiment approval + real speech file + venv setup. Research is execution-blocked, not idea-blocked.

---

## Evening (#42â€“#45): Correctly Skipped
- arXiv re-verified multiple times (42, 43, 44, 45) â€” 0 new papers since cycle #40.
- Must-read complete. Reflect done. Execution-blocked. 5 consecutive correct skips.

---

## Day 2 Final Stats
| Metric | Count |
|--------|-------|
| Papers deep-read | **14 total** (12 prev + AR&D + SAEBench today) |
| Papers scanned | 26+ |
| Research gaps documented | **16** |
| Paper ideas crystallized | **6** |
| Code scripts (verified) | 2 (whisper_hook_demo.py 230L, whisper_logit_lens.py 300L) |
| Cheat sheets | 1 (TL + pyvene + NNsight updated) |
| Experiment proposals | **2** (IIT Triple Convergence + Class-specific Neuron Grounding) |
| Days active | 2 |

---

## Key Concepts Developed Today

### ðŸ”‘ "Listen Layer Hypothesis" (NEW synthesis, cycle #40-41)
Speech LLMs have a discrete "listen layer" â€” a small set of LLM attention heads where audio representations are actually causally consulted. Below this layer: audio info is linearly accessible but causally inert. Above: text dominates.

Motivation from 5 papers: AudioLens (critical layer), MiSTER-E (gating), Cascade Equivalence (LEACE global), ALME (LoRA ablation localizes to LLM), Modality Collapse (GMI theory).

Leo's denoising patching sweep = only method that localizes this.

### ðŸ”‘ "Grounding Sensitivity" â€” Novel AudioSAEBench Metric (cycle #38)
For each SAE feature: gc = audio_patch_Î” / (audio_patch_Î” + text_patch_Î”). Features with gc â‰ˆ 1 = audio-grounded; gc â‰ˆ 0 = text-context-driven. Not present in SAEBench. Audio-native disentanglement metric.

### ðŸ”‘ IIT = Everything (cycle #19)
Causal Abstraction theory unifies all 10 MI methods under one theoretical framework. Leo's grounding_coefficient = relative IIT accuracy â€” theoretically rigorous, not ad-hoc.

### ðŸ”‘ ALME stimuli = free experiment scaffold (cycle #40)
57K conflict stimuli (audio-text mismatch) across 8 languages, 4 models, publicly available. Leo can run layer-wise causal patching directly on these â€” no need to generate stimuli from scratch. Experiment #3 is practically ready once venv + NNsight set up.

---

## Papers Encountered Today
| Paper | arXiv | Read Depth | Relevance |
|-------|-------|-----------|-----------|
| Causal Abstraction (Geiger et al.) | 2301.04709 | **Deep** | **Core â€” theory foundation** |
| Multimodal MI Survey (Lin 2025) | 2502.17516 | **Deep** | Landscape |
| AR&D â€” AudioLLM MI (Chowdhury et al., ICASSP 2026) | 2602.22253 | **Deep** | **Core â€” Track 3** |
| SAEBench (Karvonen, Nanda et al., ICML 2025) | 2503.09532 | **Deep** | **Core â€” Track 2 template** |
| Zhao et al. (ESNs in LALMs) | 2601.03115 | **Deep** | **Core â€” Track 3+2** |
| Kawamura (neuron dissection) | 2602.15307 | **Deep** | Core â€” Track 2 |
| Mariotte (audio SAE) | 2509.24793 | **Deep** | Core â€” Track 2 |
| MiSTER-E | 2602.23300 | Scan | Track 3 |
| Modality Collapse (Billa et al.) | 2602.23136 | **Deep** | **Core â€” Track 3** |
| Cascade Equivalence (Billa et al.) | 2602.17598 | **Deep** | **Core â€” Track 3** |
| ALME Benchmark (Billa et al.) | 2602.11488 | **Deep** | **Core â€” Track 3 scaffold** |
| Phonological Vector Arithmetic (Choi et al.) | 2602.18899 | Scan | Background (Track 2) |
| EmoOmni (ICML 2026) | â€” | Scan | Track 5 |
| Plantinga SAE-PD | â€” | Scan | Track 2 |

---

## Leo's Action Queue (as of 21:00 Feb 27) â­ UPDATED
1. **Approve IIT experiment** â€” Experiment 1 in knowledge-graph (MacBook-feasible, NNsight, ~3h). 10 LibriSpeech clips + denoising sweep. Clear path to "Localizing the Listen Layer."
2. **Real speech test** â€” whisper_hook_demo.py with actual .wav â†’ validate Triple Convergence on real audio
3. **Create venv** â€” `python3 -m venv ~/audio-mi-env && source ~/audio-mi-env/bin/activate && pip install nnsight openai-whisper`
4. **Contact æ™ºå‡±å“¥** about AudioLens codebase access
5. â­ **"Listen Layer Hypothesis" paper framing** â€” Cascade Equivalence + ALME + Modality Collapse all point at Leo's gap. Act fast (~2 papers/week field velocity).
6. â­ **ALME stimuli** â€” 57K conflict stimuli, publicly available. Layer-wise patching on these = Track 3 experiment 3, ready to run with NNsight.

## Key Decisions
- NNsight > pyvene for encoder work (simpler API + NDIF remote for large models)
- Paper framing: "Localizing the Listen Layer" > "Causal AudioLens" (more specific, better motivated by 5 papers)
- AudioSAEBench: adopt SAEBench 4-category structure + "Grounding Sensitivity" as novel audio-native metric
- All reading complete â€” research is now execution-blocked, not idea-blocked
