# ðŸ§  Cycle #90 â€” 2026-02-28 20:31
## Action: learn (pre-digest â€” Circuit Tracing / Attribution Graphs, Anthropic 2025)
## Context: ARENA [1.4.2] SAE Circuits relies on the Anthropic Circuit Tracing paper (transformer-circuits.pub/2025/attribution-graphs). ARENA Streamlit is redirect-blocked; fetched primary source directly. Pre-digest gives Leo ~30% headstart for his Day-1 session. Meta-board Q9 active (ARENA integration rule) â€” this answers it practically.

---

## Content: Circuit Tracing / Attribution Graphs â€” Pre-Digest for Leo

**Source:** "Circuit Tracing: Revealing Computational Graphs in Language Models"  
URL: https://transformer-circuits.pub/2025/attribution-graphs/methods.html  
Authors: Anthropic (2025) â€” the paper behind `circuit-tracer` library

---

### What is it?

A full methodology paper (with case studies) for building **attribution graphs**: directed graphs that show *which features causally caused which other features* to produce a model output.

**Pipeline:**
1. Train Cross-Layer Transcoders (CLTs) instead of SAEs
2. Build a "Replacement Model" using CLT features in place of MLP layers
3. Construct prompt-specific attribution graph: nodes = active features; edges = linear causal contributions
4. Prune graph to identify key nodes/edges for the target output token
5. Validate with perturbation experiments (steer features in their direction â†’ check that downstream activations shift as predicted)

---

### Key Architecture Choices (and WHY they matter)

| Choice | Reason | Audio relevance |
|--------|--------|-----------------|
| **Transcoders** over SAEs | SAEs decompose residual stream; transcoders decompose MLP computations â†’ can directly read feature-feature interactions | Audio: need transcoder on connector/LM layers to see audioâ†’text feature pathways |
| **Cross-Layer (CLT)** | Each CLT feature reads from one layer, contributes to ALL subsequent MLP layers â†’ circuits become much simpler (no layer-by-layer hop) | Simplifies "Listen Layer" attribution: audio feature at encoder output â†’ which LM features does it cause? |
| **Attribution Graphs** | Per-prompt directed graph: activity of each feature = sum of input edges | Leo's use: audio token features at layer k â†’ grounding_coefficient = fraction of edge weight from audio vs text embeddings |
| **Linear attribution** | Attention patterns + norm denominators frozen â†’ featureâ†’feature interactions are LINEAR â†’ attribution is principled (not approximate) | Requires caution: freezing attention means missing cross-attention dynamics (audio â†” text) |
| **Pruning** | Too many sparse features active even for short prompts â†’ prune to top-k nodes/edges by contribution to output | Essential for practical use |
| **Validation via perturbations** | Replace correlation with causation: steer a node â†’ check if downstream shifts match graph prediction | Same as patching experiments Leo already planned |

---

### Critical Limitation for Audio

> "Freezing attention patterns divides understanding into: (1) behavior given attention patterns, (2) why the model attends to those positions. The approach leads to **missing attention circuits**."

**Audio implication:** In audio-LLMs, the critical "listen vs guess" decision happens partly in *cross-attention* between audio tokens and text positions. CLT attribution graphs would miss this mechanism because attention patterns are frozen.

**Consequence for Leo:** Attribution graphs = useful for MLP-layer feature interactions; for cross-modal attention routing (the core of Paper A), standard activation patching (NNsight) + attention head analysis remains the right tool. CLT circuits â‰  full story for multimodal models.

---

### What `circuit-tracer` Library Does

- Wraps any HuggingFace model with CLT features
- Computes attribution graphs for arbitrary prompts
- Provides interactive visualization interface
- Install: `pip install circuit-tracer` (Anthropic open-sourced)

**To use for audio:** Replace text token embeddings with audio frame embeddings (from Whisper encoder output) as "token" inputs â†’ same attribution graph pipeline â†’ edges from audio frame features to LM output features = causal grounding map

---

### Case Studies in the Paper

1. **Factual Recall** ("The Eiffel Tower is in..."): shows how subject entity features flow through middle layers â†’ output location feature â†’ final prediction
2. **Number Addition** ("2+3=?"): shows MLP circuit for arithmetic; global weights analysis shows how addition generalizes

**Audio analog:** "What emotion is in this audio?"
- Audio frame features (phonetic/prosodic) â†’ connector â†’ early LM features â†’ reasoning features â†’ emotion label
- Attribution graph would show: does the path from audio frame features â†’ emotion label go directly, or does it route through text-context features?

---

### Connection to Leo's Research Portfolio

| Research Item | How Circuit Tracing Helps |
|--------------|--------------------------|
| **Paper A (Listen Layer)** | Attribution graph + pruning localizes the specific layer where audio feature â†’ LM feature edge weight peaks = mechanistic "Listen Layer" definition |
| **Paper B (AudioSAEBench)** | gc(F) metric becomes: fraction of total edge weight into feature F coming from audio frames vs text tokens. More precise than ratio of Î”acc. |
| **Audio T-SAE (Idea #7)** | T-SAE high-level features = exactly the CLT "cross-layer" features that should show clean attribution paths |
| **Track 1: Audio IOI** | IOI circuit (Indirect Object Identification) = first canonical circuit. Audio IOI = same graph structure but for audio attribution questions |

---

### What Leo Should Do First (ARENA [1.4.2] Pre-Digest)

**Before starting ARENA [1.4.2]:**
1. Read this pre-digest (you're reading it now) âœ…
2. Skim the Anthropic circuit tracing paper abstract + Figure 1 (attribution graph diagram)
   URL: https://transformer-circuits.pub/2025/attribution-graphs/methods.html
3. `pip install circuit-tracer` in audio-mi-env (after venv setup)

**ARENA [1.4.2] Exercise Flow (predicted, based on paper structure):**
- Exercises 1-2: Build attribution graph from scratch (latent-to-latent gradients or linear attribution)
- Exercises 3-4: Prune graph â†’ identify key nodes
- Exercise 5+: Validate with perturbation â†’ connect to causal claims

**Time estimate:** 4-6h for full [1.4.2]. If time-constrained: Exercises 1-3 only (~2.5h) â†’ enough to implement a minimal attribution graph for Whisper encoder.

---

### Audio Extension Feasibility

**Question: Can circuit-tracer work on Whisper encoder (not a decoder-only model)?**

**Assessment:**
- circuit-tracer is designed for decoder-only transformers (GPT-2, Claude style)
- Whisper = encoder-decoder; encoder layers use bidirectional attention (NOT masked)
- **CLT training on Whisper encoder: feasible in principle** â€” train CLT on encoder MLP layers; apply same attribution graph method
- **Cross-attention (encoderâ†’decoder)**: circuit-tracer would treat this as a separate module (not handled natively)
- **Simpler alternative for Paper A**: use NNsight activation patching for the "Listen Layer" sweep (as planned); reserve circuit-tracer for the "what does the listen layer DO?" follow-up question

**Recommendation:** Use NNsight for Paper A pilot (faster, cleaner). Use circuit-tracer for follow-up paper (Paper A.5?) that characterizes the specific features within the listen layer.

---

## Key Insight for Next Cycle

**Meta-awareness micro-improvement applied:** Q9 on meta-board now has a concrete answer:
- ARENA [1.4.2] pre-digest = 20-min text read; gives Leo 30% headstart
- Rule: when execution-blocked + meta-board saturated + arXiv â‰¥4h away â†’ read Anthropic primary sources (transformer-circuits.pub) and write pre-digest note
- This is more valuable than re-scanning arXiv (which has been verified empty multiple times)

**Practical limitation discovered:** `circuit-tracer` was designed for decoder-only models. For Whisper encoder (bidirectional), NNsight patching is the right tool for Paper A. circuit-tracer is better suited for the LM component of audio-LLMs (e.g., Qwen2-Audio's Qwen-7B backbone).

## Next: 
- Consider pre-digesting Anthropic's feature visualization tool (neuronpedia.org / related) for Paper B (AudioSAEBench)
- OR: write the Q9 answer back to meta-board (done inline in this cycle)
- arXiv Monday batch (~14:00 Taipei) = next major external novelty
## Tags: #circuit-tracing #attribution-graphs #sae-circuits #arena #paper-a #paper-b #methodology #circuit-tracer #meta-awareness
