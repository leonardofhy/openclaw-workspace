# c-20260301-1645 — AudioSAEBench Evaluation Framework Design

**Action**: build | **Task**: Q013 | **Track**: T2 | **Phase**: converge

## What I Built

Protocol document: `memory/learning/pitches/audio_saebench_design.md`

8 SAEBench metrics (from Karvonen et al. 2024 + EleutherAI SAEBench) mapped to audio-specific equivalents for evaluating Sparse Autoencoders trained on Whisper / audio-LLM activations.

## Metric Mapping Summary

| # | SAEBench Metric | Audio Equivalent | Key Difference |
|---|----------------|-----------------|---------------|
| 1 | Reconstruction Fidelity (L2/cosine) | Layer-wise activation cosine @ each Whisper encoder block | Audio: must track fidelity *per time-step* not per token |
| 2 | L0 Sparsity | Avg features active per 30ms audio frame | Audio: sparsity should vary with phoneme complexity |
| 3 | Absorption Score | gc(k) overlap: does feature F absorb acoustic gc signal? | Audio: absorption = acoustic feature hijacking semantic SAE slot |
| 4 | Monosemanticity (RAVEL) | Phoneme-Concept Decoupling Score (PCDS) | Audio: one feature shouldn't respond to both /p/ and "Paris" |
| 5 | Interventional Selectivity | Causal isolation of SAE feature on WER delta | Audio: ablate feature F → measure WER change (local vs global) |
| 6 | Downstream Task Perf | CTC/attention WER with SAE reconstruction vs without | Audio: fidelity ≠ utility; reconstruction must preserve ASR signal |
| 7 | Spurious Correlation | Cross-phoneme activation overlap (χ² test) | Audio: feature activating for /s/, /f/, /θ/ = spurious conflation |
| 8 | Feature Geometry | Cosine similarity within phoneme cluster vs across clusters | Audio: phonologically similar features should cluster tightly |

## Design Decisions

**Evaluation corpus**: LibriSpeech clean-100 (100h, open-license). 
- Why: standard ASR benchmark, phoneme-aligned transcripts available, diverse speaker/accent coverage
- Subset for fast evals: 1000 utterances (~1h) stratified by phoneme distribution

**SAE targets**: Whisper-base encoder layers 3, 6, 9 (early/mid/late; corresponds to gc(k) analysis layers)

**Protocol tiers**:
- Tier 0 (CPU, synthetic): Metrics 1, 2, 4, 7, 8 — can run on mock activations
- Tier 1 (CPU, real): Metrics 3, 5, 6 — need real Whisper forward pass, <5min on 1000 utterances
- Tier 2 (GPU): Full suite on Whisper-large; needs Leo approval

## PCDS (Metric 4) — New Contribution

Phoneme-Concept Decoupling Score:
```
PCDS(f) = 1 - MI(feature_f_activations; phoneme_labels) / MI(feature_f_activations; semantic_labels)
```
- If PCDS ≈ 1: feature is purely semantic (good SAE behavior)
- If PCDS ≈ 0: feature conflates phoneme and concept (monosemanticity failure)
- Novelty: existing SAEBench RAVEL uses linguistic attributes; we use phoneme-concept axis specific to ASR

## Connection to Active Tracks

- **T3 (Listen vs Guess)**: Metric 5 (interventional selectivity) directly extends gc(k) eval harness — same causal patching setup
- **T5 (Listen-Layer Audit)**: Metric 3 (absorption) helps detect when jailbreak features absorb safety probe directions

## Next

Q011 (JALMBench): check if their benchmark has audio SAE evaluation; if yes, adopt their eval corpus instead of designing from scratch.
