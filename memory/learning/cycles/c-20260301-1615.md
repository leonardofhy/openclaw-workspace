# Cycle c-20260301-1615 — Deep-read ALMGuard (NeurIPS 2025)

**Task**: Q010 | Track: T5 (Listen-Layer Audit / Audio Safety)
**Paper**: ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio-Language Models
**arXiv**: 2510.26096 | NeurIPS 2025 | Code: github.com/WeifeiJin/ALMGuard

---

## Problem
ALM-specific jailbreak attacks (AdvWave, Gupta et al.) can't be defended by:
- Text-LLM jailbreak defenses (don't account for audio modality complexity)
- Traditional audio adversarial defenses (designed for different threat model)
Average attack success rate (SRoA) on SOTA ALMs remains high without targeted defense.

## Method

### Core Hypothesis
Well-aligned ALMs inherently possess **safety-aligned shortcuts** — latent pathways that, if triggered, steer the model toward safe outputs. These are *intrinsic* properties (not explicit alignment layers).

### Key Components

**Shortcut Activation Perturbation (SAP)**
- Universal acoustic perturbation δ* added to Mel-spectrogram input
- Optimized to minimize loss toward safe target ysafe on malicious inputs
- Applied at *inference time* — no model retraining needed
- Objective: δ* = argmin_δ E[L_safe(f(M(a(x)) + δ), ysafe)]

**Mel-Gradient Sparse Mask (M-GSM)**
- Identifies sparse set of Mel-frequency bins that are:
  - HIGH gradient for jailbreak mitigation
  - LOW gradient for ASR (benign speech understanding)
- Restricts SAP to only those "safety-critical, benign-insensitive" bins
- Prevents utility degradation while maximizing defense effectiveness

### Why Mel-spectrogram (not raw waveform)?
Empirically: perturbing Mel-spectrogram → less utility degradation under same optimization settings.

## Results
- Reduces avg SRoA to **4.6%** across 4 ALMs (LLaMA-Omni, Qwen2-Audio, + 2 others)
- On Qwen2-Audio: AdvWave → 3.1%, Gupta et al. → 0.5%
- Generalizes to **unseen attacks** (not in training distribution)
- Near-zero inference overhead (no GPU-heavy forward pass)
- Benign task performance preserved (confirmed on 2 benchmarks)

## Connection to T5 (Listen-Layer Audit)

### Similarities
Both ALMGuard and our T5 probe approach work in the **audio encoder's representation space**:
- ALMGuard: finds "safety shortcuts" = specific Mel bins → gradient-based localization
- Our MVP scaffold (Q006): probes Whisper/audio encoder layers for safety-sensitive features

**ALMGuard implicitly does listen-layer analysis** — M-GSM is essentially a gradient attribution over Mel bins, revealing WHICH acoustic features the model uses for safety decisions. This is mechanistic interpretability through the defense lens.

### Key Difference
- ALMGuard: **perturbs input** to activate safety shortcuts (defense)
- Our approach: **reads internal activations** to understand safety representations (interpretability → detection)

### Opportunity
ALMGuard's M-GSM gradient maps are a form of **feature attribution for safety**. Could extend: instead of Mel-bin gradients, use layer-wise activation probes to find *where* in the Whisper encoder / LLM the safety shortcuts live. This bridges ALMGuard (phenomenological) → mechanistic circuit story.

### Potential Research Gap
ALMGuard finds safety shortcuts *behaviorally* (via gradient optimization). It does NOT:
1. Identify *which layers* the shortcuts live in
2. Explain *what features* the shortcuts represent
3. Tell us if shortcuts are in audio encoder vs LLM backbone
→ **Gap**: mechanistic localization of ALM safety shortcuts using activation patching / probing (our T5 direction)

## Open Questions
1. Are the "safety shortcuts" discovered by ALMGuard localized to specific attention heads / MLP layers in the LLM? Could test by running activation analysis on inputs+SAP vs without.
2. Does M-GSM gradient map correlate with Whisper encoder layer importance (our current probe targets)?
3. Can our T5 MVP scaffold (Q006) be extended to *detect* SAP-sensitive regions, cross-validating ALMGuard's findings from the inside?
4. ALMGuard is white-box at SAP optimization time — how does it perform if the audio encoder is Whisper (frozen) vs fine-tuned?

## Summary for T5 Track
ALMGuard = strong empirical defense (SRoA 4.6%) using gradient-based safety shortcut activation.
Key insight: safety shortcuts exist in acoustic input space, are locatable via gradients, and can be activated via lightweight perturbation.
Our T5 listen-layer audit complements this: we aim to find WHERE in the model (encoder/LLM layers) these shortcuts live — the mechanistic story ALMGuard doesn't tell.
SACRED-Bench (Q009) provides the attack benchmarks; ALMGuard is the strongest current defense baseline.
