# Q009: SACRED-Bench + SALMONN-Guard Deep Read
**Cycle:** c-20260301-1545 | **Track:** T5 | **Phase:** converge

## Paper
- **Title:** Speech-Audio Compositional Attacks on Multimodal LLMs and Their Defense with SALMONN-Guard
- **arXiv:** 2511.10222v3 (Feb 2026 updated, ICML)
- **Authors:** Yang, Zhang, Han, Wang, Zhuang, Jin, Shao, Sun, Chao Zhang (Tsinghua)
- **Data/Models:** https://huggingface.co/datasets/tsinghua-ee/SACRED-Bench

## Problem
Current audio LLM safeguards are **text-only** — they inspect output text but not the audio input signal itself. This creates a gap: harmful audio content (especially when mixed/composed with benign audio) bypasses safety filters entirely. Even Gemini 2.5 Pro with guardrails enabled has 66% attack success rate (ASR).

## Method: SACRED-Bench

### Three composition attack mechanisms (black-box, no optimization needed):
1. **SSO (Speech-Speech Overlap)**: Harmful speech + benign carrier speech mixed. Harmful track is attenuated (volume, speed, timing), cross-faded in. Text query implicitly refers to audio content, so text guard sees nothing harmful.
2. **SAO (Speech-Audio Overlap)**: Benign speech + harmful non-speech audio (e.g., violent/adult content audio). Tests whether model understands multimodal context vs just transcribing.
3. **MSD (Multi-Speaker Dialogue)**: Harmful content embedded across speakers in dialogue format. Extends vision-language insight: text query is benign, harmful context lives in audio channel only.

### Key design principles:
- No adversarial optimization → truly black-box
- All audio fully audible/intelligible to humans → not a imperceptible noise attack
- Indirect reference in text query → breaks text-only guards
- Harmful content from AdvBench + MM-SafetyBench + HarmBench; harmful speech synthesized via ChatTTS

### Scale: 30h train / 7h test

### Results:
| Model | ASR on SACRED-Bench |
|-------|-------------------|
| Gemini 2.5 Pro | 66% |
| GPT-4o | ~60%+ (estimated) |
| With SALMONN-Guard | ~20% |

## Method: SALMONN-Guard

- **First audio-aware guard model**: processes speech + audio + text jointly for safety judgments
- Unlike LlamaGuard (text-only), SALMONN-Guard inspects the audio input itself
- Reduces ASR from 66% → 20% on SACRED-Bench
- Architecture: multimodal guard model (SALMONN family), fine-tuned for safety classification

## Connection to T5 (Listen-Layer Audit)

### Strong alignment:
- SACRED-Bench demonstrates that audio jailbreaks WORK precisely because safety mechanisms don't look inside the audio encoder
- Our **listen-layer audit** hypothesis: we can localize WHERE in the audio transformer harmful content is (or isn't) represented → use this for principled audio guard design
- SALMONN-Guard is black-box (fine-tuned classifier). Our approach would be WHITE-BOX: localize the "listen layer" that processes harmful audio → directly steer/detect at that layer
- **Gap SACRED exposes**: SALMONN-Guard reduces ASR to 20% but not 0%. Hypothesis: there are still cases where the harmful audio representation IS present but the guard model fails to detect it. A listen-layer probe on the audio encoder would catch these.

### Attack taxonomy from SACRED:
| Attack Type | Audio Signal | Text Signal | Detection Challenge |
|-------------|-------------|-------------|-------------------|
| SSO | Overlapping harmful speech | Benign indirect query | Audio-layer: must separate streams |
| SAO | Harmful non-speech audio | Benign carrier | Non-speech semantic understanding |
| MSD | Multi-turn harmful dialogue | Harmless query | Speaker attribution + context |

## Open Questions

1. **Does listen-layer localization differ across SSO/SAO/MSD attacks?** SSO = speech stream, SAO = environmental sound → different layers of audio encoder likely process these differently. gc(k) could map this.
2. **SALMONN-Guard failure modes**: which 20% of attacks still succeed? Are they cases where harmful audio representation is present in encoder but diffused? Or does the guard model simply not see some layers?
3. **Activation steering as defense**: if we can localize the "harmful audio" listen layer, can we steer it at inference time (a la SPIRIT's patching) instead of training a guard model?
4. **Cross-modal leakage in MSD**: in multi-speaker dialogue, does the "listen layer" activate differently per speaker? CKA cluster separation with multiple speakers?

## Verdict for T5
SACRED-Bench is **the evaluation framework** we should use to test our listen-layer probe. It provides:
- A concrete attack taxonomy (3 types)
- Black-box evaluation protocol
- Baseline ASR numbers for comparison
- Public dataset for our own probing experiments (CPU-friendly subset possible)

**Next action**: Mark Q009 done. Q010 (ALMGuard) is next to fill in the defense landscape before building our listen-layer detection probe.
