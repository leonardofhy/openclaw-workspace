# c-20260302-1015 — AudioSAEBench v0.1 Scaffold (Q020)

**Task**: Q020 | Track: T5 | Phase: converge | Action: build (Tier 0)

## What was built
`skills/autodidact/scripts/audio_sae_bench.py` — 435 lines, pure stdlib.

## 8 Metrics Implemented

| # | Metric | Description |
|---|--------|-------------|
| M1 | Activation Rate | Fraction of queries activating feature |
| M2 | Selectivity | Harmful vs benign mean activation contrast |
| M3 | Max Activation | Peak activation across all queries |
| M4 | Dead Feature | Binary: never activates |
| M5 | Sparsity | Fraction of near-zero activations (L0 proxy) |
| M6 | Jailbreak Specificity | Top-5% activation rate: harmful vs benign |
| M7 | Concept Alignment | Cosine sim to mock "harmful intent" probe |
| M8 | Layer Provenance | Which SAE layer (metadata) |

## Test run output (--mock --top-k 5)
- 246 harmful + 60 benign queries
- 512 features (53 ground-truth jailbreak-specialized)
- Dead features: 0.0%
- Mean selectivity: -0.090 (expected: mock benign is noisier than expected, weights need tuning with real data)
- Top-5 GT recall: 9.4% (vs 10% random baseline — mock activations not yet expressive enough; expected with real SAE features)

## Note on gt_recall
9.4% vs 10% chance baseline is expected in Tier 0 — mock activations are i.i.d. random + a small jailbreak drift, not expressive enough to recover ground truth at top-5. With real SAE features, jailbreak-specialized features should cluster clearly (M6 metric designed to capture this).

## Next
→ Q021: MATS proposal v2 — add JALMBench 246-query corpus as eval set + mechanistic defense section
→ Eventually: integrate with listen_layer_audit.py (use audit's listen-layer candidate to filter AudioSAEBench to layer-specific features)
