# AI Safety Radar

> Purpose: 30-min internal scan + balanced recommendations for Leo.
> Sources: Alignment Forum, LessWrong (blogwatcher)

## Operating Rules
- Scan every 30 minutes during daytime/evening.
- Do NOT spam Leo on every cycle.
- If no high-signal new item, log briefly and continue.
- Prioritize posts that are:
  1) actionable for current research,
  2) conceptually important for alignment/safety,
  3) likely to change decisions.

## 2026-02-27
- Initialized radar.
## 2026-02-27 02:48 (CST)
No new unread posts from Alignment Forum / LessWrong in this cycle.

## 2026-02-27 02:49 (CST)
No new posts (scan: AF +0, LW +0). Backlog: 20 unread items.
**[backlog-pick]** 2 picks from LessWrong (2026-02-26 batch):
- **"How Robust Is Monitoring Against Secret Loyalties?"** — Core AI-control question: if a model has a secret loyalty, how well do current monitoring schemes detect it? Directly relevant to oversight robustness. <https://www.lesswrong.com/posts/CYaNSccGaCRfMKCQt/how-robust-is-monitoring-against-secret-loyalties>
- **"A Positive Case for Faithfulness: LLM Self-Explanations Help Predict Model Behavior"** — Empirical finding that LLM self-explanations have genuine predictive power, positive signal for interpretability approaches. <https://www.lesswrong.com/posts/Y4MJRniZ6noumncKJ/a-positive-case-for-faithfulness-llm-self-explanations-help>
