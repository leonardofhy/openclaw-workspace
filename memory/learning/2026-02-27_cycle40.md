# ðŸ§  Cycle #40 â€” 2026-02-27 14:01
## Action: learn
## Context: Feb 27 arXiv batch now live (~14:00 Taipei). Queue was depleted at cycle #39 (13:01). Prime time to scan fresh papers. arXiv cs.SD + cs.CL batch retrieved.

## Content: Feb 27 arXiv Scan â€” 4 Papers, 3 Directly Hit Track 3

### ðŸ”¥ PAPER 1: "Modality Collapse as Mismatched Decoding" (Billa et al., 2602.23136)
**Submitted:** 2026-02-26 | cs.CL/AI/LG | 22 pages, code: github.com/jb1999/modality_collapse_paper

**Core finding:** Speaker identity, emotion, and visual attributes survive through EVERY LLM layer (3-55Ã— above chance in linear probes) â€” but removing 64-71% of modality-specific variance **IMPROVES decoder loss**. The decoder has no learned use for these directions; their presence is noise.

**Formal framework:** "Mismatched decoder" problem. Accessible information bounded by Generalized Mutual Information (GMI). Degradation scales with distributional distance + decoder sensitivity. The bound is a property of the decoder's scoring rule â€” not architecture. Validated across 5 models (speech + vision).

**Controlled experiment:** Two Prismatic VLMs differing only in encoder text-alignment â€” bottleneck is decoder's scoring rule, not encoder or projection.

**LoRA fix:** Training with emotion objective improves emotion accessibility (+7.5%) without affecting other attributes.

**Connection to Leo's Track 3 (Listen vs Guess):**
- This paper gives *theoretical grounding* for why grounding_coefficient might be low even when audio embeddings are rich â€” the decoder simply can't access non-text-aligned directions
- The GMI bound is a theoretical ceiling on what causal patching can recover
- New framing: "Is the decoder the bottleneck, or is it the encoder/connector?" â€” Leo's IIT experiment can test this directly
- Gap: They use linear probes (correlation), not causal patching (necessity) â€” still no mechanistic localization

**New Gap #14:** How does the GMI bottleneck map to specific layers? Their theory doesn't specify *where* in the LLM the mismatch occurs. Leo's layer-wise causal patching (IIT experiment) directly answers this.

---

### ðŸ”¥ PAPER 2: "Cascade Equivalence Hypothesis" (Billa et al., 2602.17598)
**Submitted:** 2026-02-19 | cs.CL/AI + eess.AS | 10 pages, 6 figures

**Core finding:** Current speech LLMs largely perform **implicit ASR** â€” on tasks solvable from a transcript, they are behaviorally AND mechanistically equivalent to simple Whisperâ†’LLM cascades.

**Methods used:**
- Matched-backbone testing across 4 speech LLMs Ã— 6 tasks
- **Logit lens** reveals literal text emerging in hidden states (confirms AudioLens pattern)
- **LEACE concept erasure** confirms text representations are causally necessary â†’ ablating them collapses accuracy to near-zero (strong causal claim!)
- Ultravox Îº=0.93 (indistinguishable from cascade); Qwen2-Audio genuinely diverges

**Key nuance:** Cascade equivalence is **architecture-dependent, not universal**. Qwen2-Audio breaks the pattern â€” this is exactly the kind of architecture Leo should study: *when* does a speech LLM actually "listen"?

**Under noise:** Clean-condition advantages reverse by up to 7.6% at 0 dB â€” cascades are worse in noise.

**Connection to Leo's research:**
- This is the most directly related paper to Track 3 yet published (outside AudioLens)
- Their LEACE erasure = causal claim; Leo's patching = stronger causal claim with layer-level granularity
- **Gap: They don't localize WHERE text representations become dominant** â€” single-pass LEACE doesn't identify the specific layer or circuit. Leo's denoising patching sweep does.
- Their Qwen2-Audio finding motivates Track 3: it's the architecture that *doesn't* behave like a cascade â€” studying WHY is mechanistically interesting

**New Gap #15:** No paper has mapped the *layer-wise causal transition* from audio-processing to cascade-equivalent behavior in speech LLMs. LEACE is global; patching is local. Leo owns this gap.

---

### ðŸ”¥ PAPER 3: "When Audio-LLMs Don't Listen" / ALME Benchmark (Billa et al., 2602.11488)
**Submitted:** 2026-02-12 (v2: 2026-02-19) | cs.CL/SD + eess.AS | 25 pages, 57,602 conflict stimuli, 8 languages

**Core finding:** When audio and text conflict, speech-enabled LMs follow text 10Ã— more often than in text-text conflict. Gemini 2.0 Flash: 16.6% text dominance under A-T conflict vs 1.6% in T-T conflict. Audio-only accuracy (97.2%) exceeds cascade (93.9%) â€” audio embeddings preserve MORE info than text transcripts.

**Key experiments:**
- Forcing transcription before answering INCREASES text dominance (19â†’33%)
- Framing text as "deliberately corrupted" REDUCES text dominance by 80% (prompt can override!)
- LoRA on LLM halves text dominance (-23.9%); training only audio projection INCREASES it (+26.5%) â†’ **localizes text dominance to LLM reasoning, not audio encoder**

**Connection to Leo's Track 3:**
- This paper is a *behavioral benchmark* for "Listen vs Guess" â€” 57K stimuli, 8 languages, 4 models
- Their key finding (text dominance in LLM, not encoder) is a *behavioral localization* â€” Leo's causal patching is the mechanistic verification
- **Gap: ALME is behavioral (conflict stimuli) â€” doesn't explain WHERE in the LLM this happens causally. Leo's layer-wise patching on ALME stimuli = direct extension**
- LoRA ablation gives direction: if re-training the LLM is what fixes text dominance, the LLM attention layers are the locus â†’ Leo should patch there specifically

**New Gap #16:** No paper applies layer-wise causal patching to ALME-style conflict stimuli. Doing this would localize *which LLM layers* are responsible for text dominance. This is a clean, well-scoped experiment.

---

### PAPER 4: "Phonological Vector Arithmetic" (Choi et al., 2602.18899)
**Submitted:** 2026-02-21 | eess.AS/CL/LG/SD | submitted to ACL

**Core finding:** S3Ms encode speech using linear directions corresponding to phonological features. [d]-[t] = voicing vector; adding it to [p] produces [b]. Scale correlates with degree of acoustic realization (continuous voicing).

**Relevance:** LOW-MEDIUM for Leo's thesis. Confirms Linear Representation Hypothesis holds for phonological space in S3Ms. Useful background for Track 2 (AudioSAE) â€” SAE features should recover these linear directions. No connection to Track 3 directly.

---

## Summary of New Gaps Discovered This Cycle

| Gap # | Paper | Gap |
|--------|-------|-----|
| #14 | Modality Collapse (2602.23136) | GMI bottleneck not mapped to specific LLM layers â€” Leo's IIT patching answers this |
| #15 | Cascade Equivalence (2602.17598) | No layer-wise causal transition map from audio-processing to cascade-equiv â€” LEACE is global, patching is local |
| #16 | ALME (2602.11488) | No layer-wise causal patching on conflict stimuli â€” behavioral localization lacks mechanistic verification |

## Convergence Update: "Listen vs Guess" Literature Map (as of 14:00 Feb 27)
Now 5 papers directly characterize the audio-vs-text modality question:
1. **AudioLens** (æ™ºå‡±å“¥, 2025) â€” logit lens, critical layer; no patching
2. **MiSTER-E** (2602.23300) â€” MoE gating weights as behavioral proxy; not mechanistic
3. **Cascade Equivalence** (2602.17598) â€” LEACE erasure (causal but global); behavioral cascade test
4. **ALME** (2602.11488) â€” conflict stimuli benchmark; behavioral; LoRA ablation localizes to LLM
5. **Modality Collapse** (2602.23136) â€” GMI theory; probe evidence; not causal

**Leo's Track 3 contribution = the only layer-wise causal patching on audio-text conflict.** The gap is now stated from 5 different angles. Competition is very active (~2 papers/week this month).

## New Synthesis: "Listen Layer" Hypothesis
From ALME + Cascade Equivalence + Modality Collapse, a new hypothesis:
- Most speech LLMs have a **"listen layer"** â€” a small number of LLM attention heads at specific layers where audio representations are actually causally consulted (not just linearly encoded)
- Below the listen layer: audio info exists but is not consulted (consistent with ALME's text dominance)
- Above the listen layer: text representations dominate (consistent with logit lens showing text tokens emerge)
- **Leo can localize this "listen layer" with a single denoising patching sweep across LLM layers**
- This is the Track 3 paper's core contribution, now better named: "Localizing the Listen Layer in Speech LLMs"

## Next: This is a genuine insight worth flagging to Leo (report action in a future cycle). Queue is now replenished with 3 new deep reads.

## Tags: #arxiv-scan #track3 #listen-vs-guess #modality-collapse #cascade-equivalence #ALME #listen-layer-hypothesis #grounding-coefficient #GMI #LEACE
