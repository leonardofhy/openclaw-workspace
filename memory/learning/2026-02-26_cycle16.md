# ðŸ§  Cycle #16 â€” 2026-02-26 22:31
## Action: learn
## Context: Must-read list still has 5 papers unread. Whisper LoRA mech interp paper ("Behind the Scenes", ICASSP 2026) was scanned in Cycle #3 but never deep-read. Directly relevant to Track 4 (Mechanistic Interp of Adaptation). Good closing read for day 1 to solidify LoRA adaptation mechanisms.

## Content: Deep Read â€” "Behind the Scenes: Mechanistic Interpretability of LoRA-adapted Whisper for Speech Emotion Recognition"
**Paper:** Ma et al. 2025 â†’ accepted ICASSP 2026  
**arXiv:** 2509.08454  
**Code:** https://github.com/harryporry77/Behind-the-Scenes  
**Tool:** NNsight library (not pyvene â€” note this alternative!)

### Problem
How does LoRA actually reshape Whisper encoder internals for speech emotion recognition (SER)?  
Most PEFT papers report accuracy without explaining the mechanism. This is the **first systematic MI study of LoRA in speech**.

### Setup
- Whisper-large-v2 encoder as backbone (WhisperForAudioClassification)
- Task: 4-class SER (anger, happiness, neutrality, sadness) on IEMOCAP
- Analytical toolkit: layer contribution probing + logit-lens inspection + CKA + SVD
- Baseline: frozen encoder (only classification head trained) vs LoRA (r=32 on attention projections)

### Key Results

**Performance:** LoRA UAR=0.774 / WAR=0.768 vs frozen (irregular/worse). Scales with model size.

**Finding 1: Delayed Specialization**
- Frozen encoder: volatile KL in early layers, minimum at layer ~27, then rebounds â†’ emotion latent but unstable
- LoRA: flat/high KL across early+mid layers (stable but unspecialized) â†’ **pronounced late-stage drop converging at top layers**
- Interpretation: LoRA **delays commitment** â€” preserves general representations early, consolidates task-specific info only in deep layers
- Analogy: AudioLens "critical layer" behavior (info drops at peak for failures vs monotonic rise for successes) is now mechanistically explained by whether LoRA successfully redirects information flow

**Finding 2: Forward Alignment, Backward Differentiation**
- Layer contribution probing: LoRA introduces **negligible changes in early layers**, substantial changes toward top
- In deep layers: attention sublayer contribution grows more than MLP â†’ LoRA primarily sharpens **temporal focusing and long-range integration**
- Increasingly **negative cosine similarity** at higher layers: LoRA's updates are counter-directional to residual stream
- Mechanism: LoRA injects "subtractive/corrective" signals that suppress irrelevant ASR-trained features and emphasize emotional ones
- **Forward alignment (A matrix)**: A learns to align with input acoustic feature space
- **Backward differentiation (B matrix)**: B differentiates by task-specific emotional targets

**Finding 3: CKA Analysis**
- CKA reveals LoRA reshapes inter-layer representation similarity pattern
- Creates **distinct representational clusters** that didn't exist in frozen encoder
- Aligns with whisper_hook_demo.py findings: layers 0-2 cluster (acoustic), layers 3-5 cluster (semantic)

### Connections to Leo's Research

**New link â†’ Triple Convergence Hypothesis:**
- Cycle #12 found layer 3 (base)/6-7 (large) = semantic transition zone
- "Behind the Scenes" now explains *why*: that's where LoRA's delayed specialization "commits" â€” late-layer consolidation
- The transition zone is also where LoRA's corrective signals are strongest â†’ triple convergence zone = where both acousticâ†’semantic AND old-taskâ†’new-task transitions happen simultaneously

**Direct relevance to Track 4 (Mechanistic Interp of Adaptation):**
- "Behind the Scenes" = proof-of-concept that MI methods (logit-lens + CKA + probing) work on Whisper for non-ASR tasks
- Gap they leave: no patching / causal interventions (same gap as AudioLens)
- Leo's opportunity: apply causal patching to LoRA layers â†’ which specific LoRA components are causally necessary vs sufficient?

**New method: NNsight library**
- "Behind the Scenes" uses NNsight (not pyvene) for Whisper encoder access
- NNsight = alternative to pyvene for encoder MI; more recent, designed for large models
- Worth checking: may have simpler API for our use case
- Add to cheat sheet

### Open Questions Generated
1. Does "delayed specialization" apply to ALL tasks, or specific to classification (SER)? Would ASR LoRA show the same late-stage pattern?
2. Can patching into LoRA's B matrix (the "differentiating" matrix) reconstruct emotional classification from a different sample â†’ causal evidence?
3. Does the transition zone (layer 3 in base, layer 6-7 in large) correspond exactly to where LoRA's "delayed commitment" triggers in different model sizes?
4. If AudioLens failure mode (mid-layer peak then drop) corresponds to LoRA-like "incomplete commitment," can we detect this without training LoRA at all?
5. NNsight vs pyvene: which is better for causal interventions on encoder?

### Research Gap Identified
- "Behind the Scenes" + AudioLens both lack causal patching â†’ same gap, two papers, two tasks (SER vs auditory attributes)
- Leo can write ONE paper covering both: "Causal AudioLens with LoRA Mechanistic Analysis" = Track 3 + Track 4 combined
- Shared methodology: logit-lens (done), CKA (done via whisper_hook_demo), causal patching (next step)
- **Combined paper scope**: grounding_coefficient (Track 3) + LoRA component causal necessity (Track 4) = broader contribution than either alone

## Next: 
- Day 1 complete! 7 deep reads (+ 1 scanned â†’ deep), 2 scripts, 1 cheat sheet.
- Next session priority: Run whisper_hook_demo.py on real speech (not synthetic) to validate Triple Convergence.
- Read NNsight docs â€” may replace pyvene in our toolchain (simpler API).
- Consider: write minimal-pair audio corruption protocol based on Heimersheim & Nanda best practices.

## Tags: #whisper #LoRA #PEFT #SER #delayed-specialization #logit-lens #CKA #mech-interp #track4 #nnsight
