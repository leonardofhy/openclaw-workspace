# üß† Cycle #15 ‚Äî 2026-02-26 22:00
## Action: learn
## Context: Must-read #4 in goals.md. All prior build cycles (patching code, logit-lens) need solid foundations before writing actual causal patching experiments. "Activation patching best practices" is the key paper to prevent wasted experiments.

## Content

### Paper: Heimersheim & Nanda (2024) ‚Äî "How to Use and Interpret Activation Patching"
**arXiv:2404.15255** | 13 pages, tutorial

---

### Core Framework

**Activation Patching** = replace internal activations of a neural net from a "source" run into a "destination" run. Also called: Causal Tracing, Resample Ablation, Interchange Intervention, Causal Mediation Analysis.

Key insight vs ablation: Ablation zeros out everything. Patching lets you control *which* information changes while holding everything else fixed.

---

### 1. Two Main Patching Directions (CRITICAL DISTINCTION)

**Denoising (clean ‚Üí corrupt):**
- Save activations from clean prompt, inject into corrupted run
- Tests: "Is this activation SUFFICIENT to restore behavior?"
- Works well for OR circuits (either A or B is sufficient)
- Miss: AND circuits ‚Äî patching A alone won't restore if B is also needed

**Noising (corrupt ‚Üí clean):**
- Save activations from corrupted prompt, inject into clean run
- Tests: "Is this activation NECESSARY to maintain behavior?"
- Works well for AND circuits (finds all dependent components)
- Miss: OR circuits ‚Äî patching A alone won't break if B is backup

**‚ö†Ô∏è KEY WARNING**: These are NOT symmetric! The same patching experiment tells you DIFFERENT things depending on direction. Must choose deliberately based on expected circuit structure.

---

### 2. What to Patch (Granularity)

- **Coarse**: residual stream at layer+position (start here for exploratory)
- **Medium**: MLP output, attention head output at [layer, position]
- **Fine**: individual neurons, SAE features
- **Surgical**: path patching (isolate A‚ÜíB connections, blocking downstream effects)

Rule of thumb: Start with residual stream sweep ‚Üí increase granularity ‚Üí use path patching for circuit verification.

---

### 3. Choosing Corrupted Prompts (CRITICAL FOR AUDIO)

- The prompt difference defines WHAT you're tracing
- Audio version: "dog barking" vs "silence" traces acoustic content
- Audio version: "same speaker, different content" traces linguistic identity
- Audio version: "same content, different speaker" traces speaker identity
- **‚ö†Ô∏è Pitfall**: Your corruption must change only the variable of interest ‚Äî everything else held constant

For "Listen vs Guess" (Track 3):
- Audio patch: same text prompt, DIFFERENT audio content ‚Üí measures how much model relies on audio
- Text context patch: same audio, DIFFERENT text context ‚Üí measures how much model uses context to "guess"
- **Grounding coefficient = Œîacc(audio patch) / Œîacc(text patch)** ‚Äî now clearly operationalizable!

---

### 4. Metrics and Pitfalls

**Logit difference** (RECOMMENDED): Linear in residual stream, allows specific comparison (correct vs incorrect). 
- Use: correct answer logit - incorrect answer logit
- For audio: token prediction accuracy or attribute classification logit
- Pitfall: driven by either getting better at correct OR worse at incorrect

**Logprob**: Good intuition but saturates when model is already confident (bad for exploratory)

**Probability**: Non-linear (exponential relationship to logit) ‚Äî AVOID for exploratory patching, distorts partial effects

**Accuracy**: Too discrete ‚Äî misses partial effects; only use for confirmatory patching

**Key advice**: Implement multiple metrics. If they agree = strong evidence. Where they disagree, trust logit difference most.

---

### 5. AND vs OR Gates in Practice

| Structure | Denoising finds | Noising finds |
|-----------|----------------|---------------|
| AND (serial) | Only final component | All components |
| OR (parallel) | All components | Only final component |

Real-world implication for audio circuits: If audio pathway = AND gate with text pathway ‚Üí denoising only finds where they merge. Use noising to find the full circuit.

---

### 6. Other Pitfalls

**Backup behavior (Hydra effect)**: Ablating key component activates backup ‚Äî makes component look LESS important than it is. Backup is lossy (compensates ~70%), so original component is still usually visible.

**Negative components**: Some heads consistently reduce performance. Using logit diff, "fully recovered" circuit may be hiding negative components. KL divergence is better here.

**No minimality**: Finding a sufficient circuit ‚â† finding the smallest circuit. Always state: "sufficient but possibly not minimal."

**Distribution specificity**: Patching experiments only make claims for the specific prompt distribution tested.

---

### Application to "Causal AudioLens" Paper Design

The Causal AudioLens experiment should:

1. **Corrupted prompt design**: 
   - Corrupt audio: replace with silence/noise/different attribute (e.g., replace "happy" speech with "neutral")
   - Corrupt text: replace question/context with irrelevant/misleading text
   
2. **Denoising first** (it's what AudioLens does observationally anyway ‚Äî shows clean = correct, corrupted = wrong):
   - Clean audio ‚Üí corrupted audio: test "which layers are SUFFICIENT for audio attribute processing?"
   
3. **Noising second** (find the full circuit):
   - Corrupted audio ‚Üí clean audio: test "which layers are NECESSARY?"

4. **Metric**: Layer-wise logit difference (not accuracy). This extends AudioLens's LIS (Layer Information Score) naturally ‚Äî LIS ‚âà cosine similarity, but logit diff is more mechanistically principled.

5. **Grounding coefficient operationalization**:
   - Run denoising with audio corrupt at each layer ‚Üí get Œîacc(audio)
   - Run denoising with text corrupt at each layer ‚Üí get Œîacc(text)
   - grounding_coefficient = Œîacc(audio patch) / (Œîacc(audio patch) + Œîacc(text patch))
   - gc ‚Üí 1.0 = pure listener, gc ‚Üí 0.0 = pure guesser
   - This is now a CLEAN, CAUSAL metric (vs AudioLens's purely correlational LIS)

6. **Path patching for confirmation**: Once exploratory sweep identifies critical layers, use path patching to confirm direct audio‚Üíoutput connections vs mediated-through-text paths.

---

### Open Questions (for audio patching specifically)

1. In encoder models (Whisper), what's the equivalent of "position" for audio frames?
2. For speech, the "token" granularity is frames (20ms) ‚Äî how do we handle temporal alignment when patching?
3. The "correct answer" logit for ASR (sequence) vs attribute (class) needs different metrics ‚Äî WER-derived vs classification logit diff

---

### Research Synthesis: Triple Convergence + Patching Best Practices

The paper confirms that "white-noise patching" used in Beyond Transcription is essentially Gaussian noise Causal Tracing ‚Äî which Heimersheim & Nanda note is sensitive to noise level and can be ineffective. This means Beyond Transcription's patching results should be reproduced with properly chosen minimal pair audio samples (same speaker/duration, different attribute), not just white noise.

**New gap identified**: Most audio patching to date uses suboptimal corruptions (white noise). Minimal pair audio corruptions (same content, different attribute) = cleaner causal evidence. This is an explicit methodological improvement Leo can claim.

## Next: 
- Cycle #16: Start writing the minimal-pairs audio patching protocol based on today's best practices reading. This is the methodological foundation for Causal AudioLens.
- OR: Read Whisper LoRA mech interp paper (scanned in Cycle #3, not deep-read yet)

## Tags: #activation-patching #causal #best-practices #methodology #grounding-coefficient #causal-audiolens
