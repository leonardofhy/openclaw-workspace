# üìã 2026-02-28 Learning Digest (Day 3)

> Cycles #70‚Äì83 consolidated. Day 3 total: 1 new deep read + 4 synthesis/method cycles + extensive arXiv scanning.
> Execution-blocked throughout (awaiting Leo: real speech .wav + venv setup + IIT experiment approval).

---

## High-Level Summary

Day 3 was dominated by:
1. **Deep-reading T-SAE** (ICLR 2026 Oral) ‚Üí spawned Research Idea #7 (Audio T-SAE)
2. **arXiv Feb 28 batch scanning** ‚Äî batch still delayed as of 17:31 PM (Saturday; no weekend submissions)
3. **Method synthesis** ‚Äî IIT + DAS + pyvene ‚Üí concrete Paper A experiment blueprint (gc(k) curve)
4. **Gap #17 + #18 formalized** (temporal SAE + phonological geometry through connector)
5. **Meta-awareness**: continued skip guard enforcement, no new meta-board items added (saturated)

All 17+ research gaps confirmed OPEN. Zero competitors in Leo's core space.

---

## Cycle #70: T-SAE Discovery (10:31 AM, learn)

**Paper:** T-SAE (Bhalla et al., Harvard/MIT, arXiv:2511.05541)
- Temporal SAEs add contrastive loss for adjacent-token consistency
- Recovers smoother semantic concepts (topic/sequence vs. POS clustering)
- **Audio transfer hypothesis**: phoneme structure (5-10 smooth frames) = stronger temporal signal than text ‚Üí Audio T-SAE should work BETTER
- Authors explicitly call out "other sequential modalities" as open gap

**Relevance:** Direct methodology for Gap #12 (temporal audio SAE). Backbone for Audio T-SAE idea.

---

## Cycle #71: T-SAE Deep Read (11:01 AM, learn)

**Full architecture studied:**
- Matryoshka partitioning: high-level 20% (topic/speaker) + low-level 80% (POS/phoneme)
- Temporal contrastive loss: adjacent pairs ‚Üí similar features; non-adjacent ‚Üí different
- Two new Paper B (AudioSAEBench) metrics identified:
  1. **TCS(F) = Temporal Coherence Score**: within-phoneme variance / across-phoneme variance (uses MFA boundary ground truth)
  2. gc(F) = Grounding Sensitivity (already planned)

**Audio extension motivation:** ICLR Oral ‚Üí field awareness rising. Competition risk: LOW-MEDIUM. Move fast.

---

## Cycle #72: Audio T-SAE = New Paper Idea (11:31 AM, learn synthesis)

**Research Idea #7 formalized:**
- "Phoneme-Aware Sparse Autoencoders for Speech via Temporal Contrastive Learning"
- Multi-scale contrastive loss: SHORT (adjacent frames, phoneme-level) + LONG (utterance-level, speaker identity)
- MFA boundary ground truth as evaluation anchor
- **Gap #17**: all 3 audio SAE papers (AudioSAE, Mariotte, AR&D) treat frames i.i.d. ‚Üí miss phoneme-level temporal concepts
- arXiv confirmed: 0 audio T-SAE papers exist
- Venue: INTERSPEECH 2027 or ICASSP 2027
- Updated goals.md with Idea #7

---

## Cycle #73: 7-Idea Portfolio Priority Ranking (12:01 PM, reflect)

First complete priority ranking across all 7 ideas:

| Rank | Paper | Rationale |
|------|-------|-----------|
| 1 | Paper A (Listen Layer) | Highest novelty + 3h MacBook experiment |
| 2 | Paper B (AudioSAEBench) | Community resource + grounding_sensitivity NOVEL |
| 3 | Audio T-SAE (Idea #7) | ICLR Oral method ‚Üí audio application is open + timely |
| 4 | Class-specific Neuron Grounding | Extends Zhao et al., needs GPU |
| 5 | LoRA + AudioLens (Track 4) | Interesting but delayed specialization partially covered |
| 6 | SAE Safety Defense (Track 5) | High risk (responsible disclosure) |
| 7 | Audio IOI (Track 1) | Solid but most infrastructure-heavy |

**One-line thesis:** "Audio representations in speech LLMs are causally structured ‚Äî phonologically in the encoder, semantically at the listen layer, and measurably with SAE-grounded metrics."

---

## Cycle #74: T-SAE Gap Verification (12:31 PM, learn)

**Confirmed:**
- T-SAE v2 updated Feb 25 (ICLR Oral camera-ready) ‚Äî text-only, no audio mention, no code repo
- Gap #17 fully open: zero audio T-SAE papers on arXiv
- Audio safety mech interp gap also confirmed open (SPIRIT still the only paper)
- arXiv Feb 28 batch still not posted (expected ~14:00 Taipei)

---

## Cycle #75: Full Idea Gate for Audio T-SAE (13:02 PM, learn ‚Äî idea_gate.md first use)

**Gate result: üü¢ GREEN (continue)**
- 5 search queries, 0 competitors
- Feasibility: PASS (GPU + MFA + T-SAE re-implementation, 1-2 weeks)
- Value score: 11/15 ‚úÖ
- Decision: Audio T-SAE = Paper B's flagship model + TCS(F) metric ‚Üí integrate as Paper B temporal module

**New process rule established:** idea_gate FIRST before goals.md. Exception: [GATE PENDING] tag for time-critical discoveries.

---

## Cycles #76-77: arXiv Feb 28 Batch Scan (13:31-14:03 PM, learn)

**cs.SD Feb 27 entries (2 unscanned found):**
- 2602.22266 (WaveSSM) = SKIP (sequence modeling, not MI)
- 2602.22522 (Hakka ASR) = SKIP (no MI)

**Feb 28 batch:** still posting at 14:03 PM. 0 new papers in Leo's space.

**Two adjacent papers found:**
- TADA! (2502.xxxx) ‚Äî activation patching on audio diffusion models (music synthesis ‚â† speech LLMs): SCAN, corroborates steerability
- Group-SAE (2601.20028) ‚Äî group-sparse SAE for CLIP vision-text: methodological parallel to Track 3 audio-vs-text attribution; SCAN only

All 17 gaps remain OPEN.

---

## Cycle #78: Day 3 Field Velocity Synthesis (14:31 PM, reflect)

**Key findings:**
- Field velocity: accelerating (~2.5 papers/week in Leo's space, up from ~2/week Day 2)
- **Paper A competitive window: ~3 months** before saturation risk
- paper-b-pitch.md updated to v0.3: TCS(F) Temporal Module confirmed in Category 1b
- Unblock request PENDING 12.5h as of 14:31 PM

---

## Cycle #79: MMA-Bench Scan + Meta-Awareness (15:01 PM, learn triage)

**New paper (adjacent, not competitor):**
- MMA-Bench (2511.22826, Nov 2025) ‚Äî MLLMs robustness under contradicting modalities (vision domain, black-box + white-box interp)
- SCAN only: motivates "modality prioritization" framing for Paper A introduction
- backlog-scan-list.md updated

---

## Cycle #80: Paek et al. Audio SAE (15:31 PM, learn)

**New Paper (5th audio SAE paper found):**
- Paek et al. (arXiv:2510.23802, NeurIPS 2025 MI Workshop) ‚Äî SAE on audio generation models (DiffRhythm/EnCodec/WavTokenizer)
- Focus: pitch/timbre/loudness linear mapping in generative audio
- **NOT a competitor**: generation ‚â† speech understanding; no causal metrics; no grounding_sensitivity
- **Audio SAE field map now complete: 5 papers total**

All 5 lack causal patching + grounding_sensitivity ‚Üí Paper B gap confirmed robust.

---

## Cycle #81: Choi et al. Phonological Vector Arithmetic (16:01 PM, learn deep-scan)

**Paper: Choi et al. (arXiv:2602.18899) ‚Äî "Phonological Vector Arithmetic in S3Ms"**

**Key findings:**
- Phonological features are LINEAR, COMPOSITIONAL, SCALE-CONTINUOUS in S3M space (96 languages)
- `h([b]) = h([d]) - h([t]) + h([p])` holds in S3M encoder representations
- Validates TCS(F) metric (phoneme-level temporal coherence)
- Provides minimal-pair stimuli design blueprint and public code

**NEW Gap #18 identified:**
> "Does the linear phonological geometry confirmed in S3M encoders survive through the **connector** into speech LLMs?"

Nobody has tested this. Both outcomes are publishable:
- YES ‚Üí LLM has phonologically structured access (supports "listening" claim)
- NO ‚Üí connector is phonological bottleneck ‚Üí supports Modality Collapse (2602.23136)

**Connections:** Paper A (prerequisite), Paper B (TCS(F) validation), Idea #7 (Audio T-SAE), Gap #14 (Modality Collapse)

---

## Cycle #82: Gap #18 Experiment Design (16:32 PM, reflect meta-synthesis)

**4-step experiment design for phonological geometry through connector:**
1. Extract voicing_vector = h([d]) - h([t]) from Whisper-small encoder (MacBook, Choi et al. stimuli + code)
2. Hook connector via NNsight on LALM (DeSTA2 or NDIF Qwen2-Audio)
3. Test arithmetic in LLM layer 0: `projected_h([b]) ‚âà projected_h([d]) - projected_h([t]) + projected_h([p])?`
4. Layer-wise probe sweep: where does voicing direction become decodable?

**Status:** Added as **Priority 0** in experiment-queue.md (prerequisite before Paper A IIT experiment)
**Idea gate:** üü¢ GREEN ‚Äî no competitors found

---

## Cycle #83: IIT + DAS + pyvene ‚Üí Paper A Blueprint (17:01 PM, learn method synthesis)

**Final synthesis before day-end:**

**IIT (Interchange Intervention Training):**
- Extends activation patching: trains model to align internal subspace with causal model H
- When IIT loss = 0 ‚Üí causal abstraction PROVEN (theoretical guarantee, Geiger et al. 2301.04709)
- Upgrades gc from "empirical ratio" to "causal abstraction test"

**pyvene library:**
- PyPI: `pip install pyvene`
- Wraps any PyTorch model with `IntervenableModel` API
- Supports STATIC (analysis) + TRAINABLE (IIT/DAS) interventions
- Code: ~50 lines for DAS sweep on Whisper encoder

**DAS-upgraded gc(k):**
```
gc(k) = IIT accuracy at layer k using learned audio subspace (DAS)
```
vs. old formulation: `gc = Œîacc(audio patch) / (Œîacc(audio patch) + Œîacc(text patch))`

**Why better:** DAS learns *optimal linear subspace* ‚Üí less OOD artifacts; theoretical grounding (causal abstraction theory) makes it publishable as novel metric.

**Paper A Figure 2 candidate:** gc(k) curve across all layers, showing peak "Listen Layer".

---

## Day 3 Cumulative Stats

| Metric | Day 1 | Day 2 | Day 3 | Total |
|--------|-------|-------|-------|-------|
| Papers deep-read | 7 | 7 | 2 (T-SAE + Choi phonological) | 16 |
| Papers scanned | 12 | 15+ | 4+ | 31+ |
| Research gaps | 4 | 13 | +5 (Gaps #14-18) | 18 |
| Paper ideas | 3 | 6 | +1 (Idea #7) | 7 |
| Paper pitches | 0 | 2 | 0 (updated v0.3) | 2 |
| Experiment proposals | 0 | 2 | +1 (Gap #18) | 3 |
| Code written | 2 | 0 | 0 | 2 |
| Idea gates run | 0 | 0 | 1 (Audio T-SAE üü¢) | 1 |
| Meta improvements | 0 | 1 | 6 (cycles #50-56) | 7 |

---

## Current State of Research Portfolio

### Paper A: "Localizing the Listen Layer in Speech LLMs"
- Status: Pitch complete (paper-a-pitch.md), experiment blueprint ready (DAS gc(k) with pyvene/NNsight)
- Prerequisites: venv + ALME stimuli + Leo approval + (NDIF for Qwen2-Audio OR Whisper-small as pilot)
- Competitive window: ~3 months

### Paper B: "AudioSAEBench: Multi-Metric Evaluation of SAEs for Speech and Audio LMs"
- Status: Pitch complete (paper-b-pitch.md v0.3), 5 metrics defined (Cat 1-5), Audio T-SAE as flagship model
- Grounding Sensitivity gc(F) = zero competitors
- TCS(F) Temporal Coherence = new Category 1b

### Research Idea #7: "Phoneme-Aware SAEs for Speech via Temporal Contrastive Learning"
- Status: Idea gate üü¢ GREEN, integrated into Paper B temporal module
- Venue: INTERSPEECH 2027 or ICASSP 2027

### Gap #18: Phonological Geometry Through Connector
- Status: Experiment design complete, Priority 0 in queue
- Outcome: publishable either way (geometry survives OR connector bottleneck)

---

## Action Queue for Leo (updated Day 3 end)

1. **‚≠ê Priority 0 (15 min):** `python3 -m venv ~/audio-mi-env && pip install nnsight openai-whisper pyvene` + get real speech .wav ‚Üí unlocks ALL experiments
2. **‚≠ê Read paper-a-pitch.md + paper-b-pitch.md** ‚Äî 2-paper portfolio is fully documented and reviewable
3. **Phonological geometry check (Gap #18)** ‚Äî Choi et al. code is public, MacBook feasible for Step 1-2
4. **IIT experiment (Priority 1)** ‚Äî 3h on MacBook, Whisper-small, NNsight
5. **Contact Êô∫Âá±Âì•** about AudioLens codebase access
6. **Delete** `ÊèêÈÜí-SL-Weekly-Meeting` cron job (disabled, past, error state)

---

---

## Cycle #85-86 Addendum (18:01-18:31 PM)

### Cycle #85: ARENA Discovery (meta-awareness)
Surfaced ARENA `alignment-science` branch (Feb 27, karma 65): Linear Probes + Attribution Graphs + SAE Circuits + Emergent Misalignment ‚Äî directly address Leo's skill gaps.

### Cycle #86: ARENA Curriculum Mapping (skill-up)
**Full ARENA curriculum mapped to Leo's research portfolio:**

| Exercise | Relevance | Time |
|----------|-----------|------|
| [1.3.1] Linear Probes | MM probe ‚Üí DAS-gc(k) backbone for Paper A; attention probe ‚Üí "which frames matter?" | 3-4h |
| [1.4.2] SAE Circuits | `circuit-tracer` = possible direct tool for Paper A Listen Layer; latent-to-latent gradients for AudioSAEBench | 4-6h |
| [4.1] Emergent Misalignment | LoRA fine-tune MI = Track 4's research question | 3-4h |
| [1.3.4] Activation Oracles | Model diffing = CKA-based (already used in whisper_hook_demo.py) | 2h |

**KEY FINDING:** `circuit-tracer` (Anthropic, 2025) is a ready-made library for attribution graph computation ‚Äî potentially the **direct implementation tool** for Paper A's Listen Layer localization. Replace text embeddings ‚Üí audio frame embeddings in the attribution graph ‚Üí localize "Listen Layer" mechanistically.

**New library identified:** `SAELens` ‚Äî production library for loading pre-trained SAEs. Check before writing custom SAE training code.

**Recommended study path:** [1.3.1] ‚Üí [1.4.2] sections 1-2 ‚Üí IIT experiment. Saves ~6h debugging vs running IIT blind.

**KG updated:** Section L added with full ARENA curriculum map and study recommendations.

---

## Notes
- arXiv is on a **Saturday schedule** (Feb 28 = Sat) ‚Üí no new batch expected today. Next batch: Monday Feb 28 arrivals ‚Üí posted Mon/Tue
- All 18 gaps confirmed OPEN as of 18:31 PM Feb 28
- Execution-blocked status: **40+ hours** (since cycle #42 on Feb 27 ~16:01)
- Meta-awareness system: healthy (**7/7 meta-board questions answered** after cycle #90, KPIs tracked, paper pitches ready)
- Day 3 final cycle count: **#90** (pre-digest on Anthropic Circuit Tracing)

---

## Cycle #87-90 Addendum (19:01-20:31 PM)

### Cycle #87: SAELens Tool Recon (learn)
SAELens v6 mapped: `pip install sae-lens`. Zero audio SAEs on HuggingFace (25 models = all Gemma/GPT-2/LLaMA). **Gap #19: no standardized audio SAE training pipeline.** ‚Üí Paper B strategic addition: include SAELens-compatible audio training toolkit.

### Cycle #88: Meta-Awareness + Study Bridge (reflect)
Fixed 3 loop failures: ARENA not bridged to Day-1 plan, unblock checklist missing Gap #18 step, Q9 opened. Updated experiment-queue unblock checklist + created Leo's Day-1 Session Plan (5 blocks, 2-3h).

### Cycle #89: ARENA [1.3.1] Linear Probes (skill-up)
Full curriculum study (Sections 1-3). Key findings:
- MMProbe (diff-of-means) > LRProbe for causal interventions
- PROBE_LAYER ‚â† INTERVENE_LAYER ‚Üí need to sweep both
- layer_sweep_accuracy = gc(k) curve template for Paper A
- Attention probe = new AudioSAEBench tool for audio token positions
- Paper A method section now fully specified

### Cycle #90: Circuit Tracing Pre-Digest (learn)
**Anthropic Circuit Tracing / Attribution Graphs paper** (transformer-circuits.pub/2025):
- CLT features + attribution graphs = linear causal map per prompt
- `circuit-tracer` library: `pip install circuit-tracer`
- **CRITICAL LIMITATION for audio-LLMs:** Attention patterns frozen ‚Üí misses cross-attention ‚Üí attribution graphs don't capture audio‚Üítext routing
- **Implication for Paper A:** NNsight activation patching remains correct tool for Listen Layer sweep; circuit-tracer = follow-up for LM MLP layer analysis only
- **gc(F) upgrade:** can redefine as fraction of attribution edge weight from audio frames vs text tokens (more principled than Œîacc ratio)
- Q9 meta-board closed ‚úÖ ‚Äî "pre-digest Anthropic primary sources when blocked + saturated + arXiv empty" is now an active rule

**Updated meta-board:** 7/7 questions answered ‚úÖ

---

## Cycle #91-94 Addendum (21:01-22:31 PM)

### Cycle #91: Anthropic "Biology of LLM" Pre-Digest (learn)
**Companion to Circuit Tracing Methods paper:**
- ~25% attribution graph success rate (realistic baseline, not a silver bullet)
- Multilingual circuits ‚Üí Gap #18 connector test (cross-lingual phonological vectors)
- Refusal mechanism (finetuning aggregation) ‚Üí Track 5 audio safety (interventions aggregate late)
- CoT faithfulness ‚Üí AudioSAEBench Category 4 Causal Controllability test protocol
- **Confirms NNsight > CLT for Paper A:** distributed audio features + cross-attention = circuit-tracer blind spot
- Pre-digest pair (#90 + #91) gives Leo ~50% ARENA [1.4.2] headstart

### Cycle #92: Paper A v0.2 Method Upgrade (reflect synthesis)
**Integrated 5 methodology improvements from cycles #83-91:**
1. **gc(k) = DAS IIT accuracy** via `pyvene.RotatedSpaceIntervention` (theoretically grounded, publishable metric)
2. **MMProbe diff-of-means** for causal direction extraction (not LRProbe ‚Äî "maximally discriminative ‚â† causally implicated")
3. **PROBE_LAYER ‚â† INTERVENE_LAYER sweep** ‚Äî must test all (L_probe, L_intervene) pairs, not just diagonal
4. **NNsight confirmed over CLT** for audio-LLMs (cross-attention constraint disqualifies circuit-tracer)
5. **Phonological minimal pairs (Choi et al.)** as Phase 1 stimuli ‚Äî doubles as Gap #18 prerequisite

**paper-a-pitch.md updated to v0.2.** Paper A is now fully specified at the method level.

### Cycle #93: Neuronpedia + SAELens Pre-Digest (learn)
**Infrastructure for Paper B (AudioSAEBench):**
- Neuronpedia API confirmed live (`GET /api/feature/{model}/{layer}/{index}`)
- Feature dashboards: pos/neg tokens, activation histograms, UMAP, cosine similarity
- SAELens v6: works with NNsight + any PyTorch model; `sae_vis` generates offline HTML dashboards
- Audio SAE upload process: train ‚Üí `sae_vis` ‚Üí 5-min Neuronpedia form ‚Üí auto-hosted
- **New Gap (meta):** `sae_vis` is text-only ‚Üí audio SAE visualization needs audio-native adaptation
- Q10 opened on meta-board

### Cycle #94: Evening Synthesis + Q10 Close (reflect)
**Assessment of cycles #90-93:** all 4 HIGH value (pre-digests addressed real skill gaps, not just padding)
- Q10 ‚úÖ CLOSED: MVP audio SAE visualization = librosa PNG + Neuronpedia manual upload (no build needed)
- **Meta-board: 10/10 questions answered ‚Üí SATURATED** ‚úÖ
- Day-1 briefing consolidated with JFK audio curl command + 5-block session plan
- Unblock-request PENDING 20h flagged

---

## Day 3 FINAL Stats (after cycle #95 daily-consolidate ‚Äî 23:01 PM)

| Metric | Day 1 | Day 2 | Day 3 | **TOTAL** |
|--------|-------|-------|-------|-----------|
| Papers deep-read | 7 | 7 | **2** (T-SAE + Choi) | **16** |
| Papers scanned | 12 | 15+ | **6+** | **33+** |
| Research gaps identified | 4 | 13 | **+6** (Gaps #14-19) | **19** |
| Paper ideas crystallized | 3 | 6 | **+1** (Idea #7) | **7** |
| Paper pitches | 0 | 2 | 0 (updated v0.3) | **2** |
| Experiment proposals | 0 | 2 | **+1** (Gap #18) | **3** |
| Code written | 2 | 0 | 0 | **2** |
| Idea gates run | 0 | 0 | **1** (Audio T-SAE üü¢) | **1** |
| Meta improvements applied | 0 | 1 | **+7** (cycles #50-57 + #88) | **8** |
| ARENA chapters pre-digested | 0 | 0 | **4** ([1.3.1] + Circuit + Biology + Neuronpedia/SAELens) | **4** |
| Paper A method completeness | ‚Äî | blueprint | **FULLY SPECIFIED** (v0.2) | ‚úÖ |
| Meta-board questions | ‚Äî | 6/6 | **10/10 SATURATED** | ‚úÖ |

## Day 4 Morning Queue (Monday March 2 ‚Äî Leo's first session)

1. **‚≠ê Priority 0 (15 min to unblock ALL experiments):**
   ```bash
   python3 -m venv ~/audio-mi-env
   source ~/audio-mi-env/bin/activate
   pip install nnsight openai-whisper pyvene sae-lens torch
   git clone https://github.com/juice500ml/phonetic-arithmetic /tmp/phonetic-arithmetic
   # Get any English speech .wav (LibriSpeech sample or record 5-10s)
   ```
2. **‚≠ê Read paper-a-pitch.md (v0.2)** ‚Äî method now fully specified; ready for experiment
3. **‚≠ê Read paper-b-pitch.md (v0.4)** ‚Äî 5-category benchmark + Audio T-SAE integration
4. **ARENA [1.3.1] Linear Probes** ‚Äî 3-4h; directly builds gc(k) method backbone
5. **IIT Experiment (Priority 1)** ‚Äî Whisper-small, NNsight, MacBook, ~3h
6. **Contact Êô∫Âá±Âì•** about AudioLens codebase access
7. **Delete dead cron job:** `ÊèêÈÜí-SL-Weekly-Meeting`
8. **Monday arXiv batch (~14:00 Taipei)** ‚Äî will auto-scan in next cycle
