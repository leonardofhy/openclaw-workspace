# üìã 2026-02-28 Learning Digest (Day 3)

> Cycles #70‚Äì83 consolidated. Day 3 total: 1 new deep read + 4 synthesis/method cycles + extensive arXiv scanning.
> Execution-blocked throughout (awaiting Leo: real speech .wav + venv setup + IIT experiment approval).

---

## High-Level Summary

Day 3 was dominated by:
1. **Deep-reading T-SAE** (ICLR 2026 Oral) ‚Üí spawned Research Idea #7 (Audio T-SAE)
2. **arXiv Feb 28 batch scanning** ‚Äî batch still delayed as of 17:31 PM (Saturday; no weekend submissions)
3. **Method synthesis** ‚Äî IIT + DAS + pyvene ‚Üí concrete Paper A experiment blueprint (gc(k) curve)
4. **Gap #17 + #18 formalized** (temporal SAE + phonological geometry through connector)
5. **Meta-awareness**: continued skip guard enforcement, no new meta-board items added (saturated)

All 17+ research gaps confirmed OPEN. Zero competitors in Leo's core space.

---

## Cycle #70: T-SAE Discovery (10:31 AM, learn)

**Paper:** T-SAE (Bhalla et al., Harvard/MIT, arXiv:2511.05541)
- Temporal SAEs add contrastive loss for adjacent-token consistency
- Recovers smoother semantic concepts (topic/sequence vs. POS clustering)
- **Audio transfer hypothesis**: phoneme structure (5-10 smooth frames) = stronger temporal signal than text ‚Üí Audio T-SAE should work BETTER
- Authors explicitly call out "other sequential modalities" as open gap

**Relevance:** Direct methodology for Gap #12 (temporal audio SAE). Backbone for Audio T-SAE idea.

---

## Cycle #71: T-SAE Deep Read (11:01 AM, learn)

**Full architecture studied:**
- Matryoshka partitioning: high-level 20% (topic/speaker) + low-level 80% (POS/phoneme)
- Temporal contrastive loss: adjacent pairs ‚Üí similar features; non-adjacent ‚Üí different
- Two new Paper B (AudioSAEBench) metrics identified:
  1. **TCS(F) = Temporal Coherence Score**: within-phoneme variance / across-phoneme variance (uses MFA boundary ground truth)
  2. gc(F) = Grounding Sensitivity (already planned)

**Audio extension motivation:** ICLR Oral ‚Üí field awareness rising. Competition risk: LOW-MEDIUM. Move fast.

---

## Cycle #72: Audio T-SAE = New Paper Idea (11:31 AM, learn synthesis)

**Research Idea #7 formalized:**
- "Phoneme-Aware Sparse Autoencoders for Speech via Temporal Contrastive Learning"
- Multi-scale contrastive loss: SHORT (adjacent frames, phoneme-level) + LONG (utterance-level, speaker identity)
- MFA boundary ground truth as evaluation anchor
- **Gap #17**: all 3 audio SAE papers (AudioSAE, Mariotte, AR&D) treat frames i.i.d. ‚Üí miss phoneme-level temporal concepts
- arXiv confirmed: 0 audio T-SAE papers exist
- Venue: INTERSPEECH 2027 or ICASSP 2027
- Updated goals.md with Idea #7

---

## Cycle #73: 7-Idea Portfolio Priority Ranking (12:01 PM, reflect)

First complete priority ranking across all 7 ideas:

| Rank | Paper | Rationale |
|------|-------|-----------|
| 1 | Paper A (Listen Layer) | Highest novelty + 3h MacBook experiment |
| 2 | Paper B (AudioSAEBench) | Community resource + grounding_sensitivity NOVEL |
| 3 | Audio T-SAE (Idea #7) | ICLR Oral method ‚Üí audio application is open + timely |
| 4 | Class-specific Neuron Grounding | Extends Zhao et al., needs GPU |
| 5 | LoRA + AudioLens (Track 4) | Interesting but delayed specialization partially covered |
| 6 | SAE Safety Defense (Track 5) | High risk (responsible disclosure) |
| 7 | Audio IOI (Track 1) | Solid but most infrastructure-heavy |

**One-line thesis:** "Audio representations in speech LLMs are causally structured ‚Äî phonologically in the encoder, semantically at the listen layer, and measurably with SAE-grounded metrics."

---

## Cycle #74: T-SAE Gap Verification (12:31 PM, learn)

**Confirmed:**
- T-SAE v2 updated Feb 25 (ICLR Oral camera-ready) ‚Äî text-only, no audio mention, no code repo
- Gap #17 fully open: zero audio T-SAE papers on arXiv
- Audio safety mech interp gap also confirmed open (SPIRIT still the only paper)
- arXiv Feb 28 batch still not posted (expected ~14:00 Taipei)

---

## Cycle #75: Full Idea Gate for Audio T-SAE (13:02 PM, learn ‚Äî idea_gate.md first use)

**Gate result: üü¢ GREEN (continue)**
- 5 search queries, 0 competitors
- Feasibility: PASS (GPU + MFA + T-SAE re-implementation, 1-2 weeks)
- Value score: 11/15 ‚úÖ
- Decision: Audio T-SAE = Paper B's flagship model + TCS(F) metric ‚Üí integrate as Paper B temporal module

**New process rule established:** idea_gate FIRST before goals.md. Exception: [GATE PENDING] tag for time-critical discoveries.

---

## Cycles #76-77: arXiv Feb 28 Batch Scan (13:31-14:03 PM, learn)

**cs.SD Feb 27 entries (2 unscanned found):**
- 2602.22266 (WaveSSM) = SKIP (sequence modeling, not MI)
- 2602.22522 (Hakka ASR) = SKIP (no MI)

**Feb 28 batch:** still posting at 14:03 PM. 0 new papers in Leo's space.

**Two adjacent papers found:**
- TADA! (2502.xxxx) ‚Äî activation patching on audio diffusion models (music synthesis ‚â† speech LLMs): SCAN, corroborates steerability
- Group-SAE (2601.20028) ‚Äî group-sparse SAE for CLIP vision-text: methodological parallel to Track 3 audio-vs-text attribution; SCAN only

All 17 gaps remain OPEN.

---

## Cycle #78: Day 3 Field Velocity Synthesis (14:31 PM, reflect)

**Key findings:**
- Field velocity: accelerating (~2.5 papers/week in Leo's space, up from ~2/week Day 2)
- **Paper A competitive window: ~3 months** before saturation risk
- paper-b-pitch.md updated to v0.3: TCS(F) Temporal Module confirmed in Category 1b
- Unblock request PENDING 12.5h as of 14:31 PM

---

## Cycle #79: MMA-Bench Scan + Meta-Awareness (15:01 PM, learn triage)

**New paper (adjacent, not competitor):**
- MMA-Bench (2511.22826, Nov 2025) ‚Äî MLLMs robustness under contradicting modalities (vision domain, black-box + white-box interp)
- SCAN only: motivates "modality prioritization" framing for Paper A introduction
- backlog-scan-list.md updated

---

## Cycle #80: Paek et al. Audio SAE (15:31 PM, learn)

**New Paper (5th audio SAE paper found):**
- Paek et al. (arXiv:2510.23802, NeurIPS 2025 MI Workshop) ‚Äî SAE on audio generation models (DiffRhythm/EnCodec/WavTokenizer)
- Focus: pitch/timbre/loudness linear mapping in generative audio
- **NOT a competitor**: generation ‚â† speech understanding; no causal metrics; no grounding_sensitivity
- **Audio SAE field map now complete: 5 papers total**

All 5 lack causal patching + grounding_sensitivity ‚Üí Paper B gap confirmed robust.

---

## Cycle #81: Choi et al. Phonological Vector Arithmetic (16:01 PM, learn deep-scan)

**Paper: Choi et al. (arXiv:2602.18899) ‚Äî "Phonological Vector Arithmetic in S3Ms"**

**Key findings:**
- Phonological features are LINEAR, COMPOSITIONAL, SCALE-CONTINUOUS in S3M space (96 languages)
- `h([b]) = h([d]) - h([t]) + h([p])` holds in S3M encoder representations
- Validates TCS(F) metric (phoneme-level temporal coherence)
- Provides minimal-pair stimuli design blueprint and public code

**NEW Gap #18 identified:**
> "Does the linear phonological geometry confirmed in S3M encoders survive through the **connector** into speech LLMs?"

Nobody has tested this. Both outcomes are publishable:
- YES ‚Üí LLM has phonologically structured access (supports "listening" claim)
- NO ‚Üí connector is phonological bottleneck ‚Üí supports Modality Collapse (2602.23136)

**Connections:** Paper A (prerequisite), Paper B (TCS(F) validation), Idea #7 (Audio T-SAE), Gap #14 (Modality Collapse)

---

## Cycle #82: Gap #18 Experiment Design (16:32 PM, reflect meta-synthesis)

**4-step experiment design for phonological geometry through connector:**
1. Extract voicing_vector = h([d]) - h([t]) from Whisper-small encoder (MacBook, Choi et al. stimuli + code)
2. Hook connector via NNsight on LALM (DeSTA2 or NDIF Qwen2-Audio)
3. Test arithmetic in LLM layer 0: `projected_h([b]) ‚âà projected_h([d]) - projected_h([t]) + projected_h([p])?`
4. Layer-wise probe sweep: where does voicing direction become decodable?

**Status:** Added as **Priority 0** in experiment-queue.md (prerequisite before Paper A IIT experiment)
**Idea gate:** üü¢ GREEN ‚Äî no competitors found

---

## Cycle #83: IIT + DAS + pyvene ‚Üí Paper A Blueprint (17:01 PM, learn method synthesis)

**Final synthesis before day-end:**

**IIT (Interchange Intervention Training):**
- Extends activation patching: trains model to align internal subspace with causal model H
- When IIT loss = 0 ‚Üí causal abstraction PROVEN (theoretical guarantee, Geiger et al. 2301.04709)
- Upgrades gc from "empirical ratio" to "causal abstraction test"

**pyvene library:**
- PyPI: `pip install pyvene`
- Wraps any PyTorch model with `IntervenableModel` API
- Supports STATIC (analysis) + TRAINABLE (IIT/DAS) interventions
- Code: ~50 lines for DAS sweep on Whisper encoder

**DAS-upgraded gc(k):**
```
gc(k) = IIT accuracy at layer k using learned audio subspace (DAS)
```
vs. old formulation: `gc = Œîacc(audio patch) / (Œîacc(audio patch) + Œîacc(text patch))`

**Why better:** DAS learns *optimal linear subspace* ‚Üí less OOD artifacts; theoretical grounding (causal abstraction theory) makes it publishable as novel metric.

**Paper A Figure 2 candidate:** gc(k) curve across all layers, showing peak "Listen Layer".

---

## Day 3 Cumulative Stats

| Metric | Day 1 | Day 2 | Day 3 | Total |
|--------|-------|-------|-------|-------|
| Papers deep-read | 7 | 7 | 2 (T-SAE + Choi phonological) | 16 |
| Papers scanned | 12 | 15+ | 4+ | 31+ |
| Research gaps | 4 | 13 | +5 (Gaps #14-18) | 18 |
| Paper ideas | 3 | 6 | +1 (Idea #7) | 7 |
| Paper pitches | 0 | 2 | 0 (updated v0.3) | 2 |
| Experiment proposals | 0 | 2 | +1 (Gap #18) | 3 |
| Code written | 2 | 0 | 0 | 2 |
| Idea gates run | 0 | 0 | 1 (Audio T-SAE üü¢) | 1 |
| Meta improvements | 0 | 1 | 6 (cycles #50-56) | 7 |

---

## Current State of Research Portfolio

### Paper A: "Localizing the Listen Layer in Speech LLMs"
- Status: Pitch complete (paper-a-pitch.md), experiment blueprint ready (DAS gc(k) with pyvene/NNsight)
- Prerequisites: venv + ALME stimuli + Leo approval + (NDIF for Qwen2-Audio OR Whisper-small as pilot)
- Competitive window: ~3 months

### Paper B: "AudioSAEBench: Multi-Metric Evaluation of SAEs for Speech and Audio LMs"
- Status: Pitch complete (paper-b-pitch.md v0.3), 5 metrics defined (Cat 1-5), Audio T-SAE as flagship model
- Grounding Sensitivity gc(F) = zero competitors
- TCS(F) Temporal Coherence = new Category 1b

### Research Idea #7: "Phoneme-Aware SAEs for Speech via Temporal Contrastive Learning"
- Status: Idea gate üü¢ GREEN, integrated into Paper B temporal module
- Venue: INTERSPEECH 2027 or ICASSP 2027

### Gap #18: Phonological Geometry Through Connector
- Status: Experiment design complete, Priority 0 in queue
- Outcome: publishable either way (geometry survives OR connector bottleneck)

---

## Action Queue for Leo (updated Day 3 end)

1. **‚≠ê Priority 0 (15 min):** `python3 -m venv ~/audio-mi-env && pip install nnsight openai-whisper pyvene` + get real speech .wav ‚Üí unlocks ALL experiments
2. **‚≠ê Read paper-a-pitch.md + paper-b-pitch.md** ‚Äî 2-paper portfolio is fully documented and reviewable
3. **Phonological geometry check (Gap #18)** ‚Äî Choi et al. code is public, MacBook feasible for Step 1-2
4. **IIT experiment (Priority 1)** ‚Äî 3h on MacBook, Whisper-small, NNsight
5. **Contact Êô∫Âá±Âì•** about AudioLens codebase access
6. **Delete** `ÊèêÈÜí-SL-Weekly-Meeting` cron job (disabled, past, error state)

---

---

## Cycle #85-86 Addendum (18:01-18:31 PM)

### Cycle #85: ARENA Discovery (meta-awareness)
Surfaced ARENA `alignment-science` branch (Feb 27, karma 65): Linear Probes + Attribution Graphs + SAE Circuits + Emergent Misalignment ‚Äî directly address Leo's skill gaps.

### Cycle #86: ARENA Curriculum Mapping (skill-up)
**Full ARENA curriculum mapped to Leo's research portfolio:**

| Exercise | Relevance | Time |
|----------|-----------|------|
| [1.3.1] Linear Probes | MM probe ‚Üí DAS-gc(k) backbone for Paper A; attention probe ‚Üí "which frames matter?" | 3-4h |
| [1.4.2] SAE Circuits | `circuit-tracer` = possible direct tool for Paper A Listen Layer; latent-to-latent gradients for AudioSAEBench | 4-6h |
| [4.1] Emergent Misalignment | LoRA fine-tune MI = Track 4's research question | 3-4h |
| [1.3.4] Activation Oracles | Model diffing = CKA-based (already used in whisper_hook_demo.py) | 2h |

**KEY FINDING:** `circuit-tracer` (Anthropic, 2025) is a ready-made library for attribution graph computation ‚Äî potentially the **direct implementation tool** for Paper A's Listen Layer localization. Replace text embeddings ‚Üí audio frame embeddings in the attribution graph ‚Üí localize "Listen Layer" mechanistically.

**New library identified:** `SAELens` ‚Äî production library for loading pre-trained SAEs. Check before writing custom SAE training code.

**Recommended study path:** [1.3.1] ‚Üí [1.4.2] sections 1-2 ‚Üí IIT experiment. Saves ~6h debugging vs running IIT blind.

**KG updated:** Section L added with full ARENA curriculum map and study recommendations.

---

## Notes
- arXiv is on a **Saturday schedule** (Feb 28 = Sat) ‚Üí no new batch expected today. Next batch: Monday Feb 28 arrivals ‚Üí posted Mon/Tue
- All 18 gaps confirmed OPEN as of 18:31 PM Feb 28
- Execution-blocked status: **40+ hours** (since cycle #42 on Feb 27 ~16:01)
- Meta-awareness system: healthy (6/6 meta-board questions answered, KPIs tracked, paper pitches ready)
- Day 3 final cycle count: **#86** (skill-up on ARENA methodology)
