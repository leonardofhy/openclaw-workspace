# ðŸ§  Cycle #6 â€” 2026-02-26 16:30
## Action: learn
## Context: Deep-read "Beyond Transcription" (Glazer et al., arXiv:2508.15882) â€” must-read #2, foundational ASR mech interp methodology. AudioMatters deadline in 2.5h so Leo is occupied; cycle runs autonomously.

---

## Content: Beyond Transcription â€” Full Notes

### Paper Info
- **Title:** Beyond Transcription: Mechanistic Interpretability in ASR
- **Authors:** Neta Glazer et al. (aiOla Research + Bar-Ilan Univ)
- **Date:** Aug 2025 [arXiv:2508.15882]
- **Models studied:** Whisper-large-v3 (1.5B, 32+32 layers), Qwen2-Audio-7B-Instruct (8.2B)

---

### Problem
Standard ASR interpretability research studies representations but **not causally**. LLM mech interp tools (logit lens, probing, patching) haven't been systematically applied to ASR. Key unknown: what internal mechanisms produce hallucinations, repetitions, and contextual errors?

---

### Methods (all adapted from LLM interp for ASR)

**1. Logit Lens** (decoder side)
- Apply unembedding matrix E to residual stream r_t^l at each decoder layer
- Compute "saturation layer" l*_t = earliest layer where top-1 prediction stabilizes
- Tells you: when does the model commit to a decision?

**2. Linear Probing** (encoder + decoder)
- Frozen activations â†’ simple linear classifier WÂ·h + b
- Encoder: average-pool across time frames first
- Decoder: probe final token position (<eos>)
- High accuracy = attribute is **linearly decodable** (but not necessarily causal!)

**3. Component Patching** (causal)
- Run on target + reference input (white noise as disruptive ref)
- Patched activation: Ã£_C = (1-Î±)a_C^orig + Î±Â·a_C^ref
- Vary Î± to measure sensitivity; apply to encoder/decoder layers, attention heads, FFN blocks
- Causal: if patching breaks behavior, component is **necessary**

**4. Ablation**
- Zero out a component: Ã£_C = 0
- Tests necessity

**5. Encoder Lens (novel contribution)**
- Extract encoder representation h^l_e at each layer
- Apply final encoder layer norm, pass directly to decoder
- Generates textual output for each encoder layer
- Reveals: what linguistic content is available in encoder at each depth?
- Key detail: **must apply final layer norm** or decoder output is incoherent

---

### Key Findings

#### A. Probing Results (encoder)
| Attribute | Best Layer | Accuracy |
|-----------|-----------|----------|
| Speaker gender | Layer 25 | 94.6% |
| Clean vs. noisy | Layer 27 | 90.0% |
| Accent (4-class) | Layer 22 | 97.0% |

- **Pattern:** Richer acoustic features concentrate in **upper encoder layers** (22-27 of 32)
- Interesting: Qwen2-Audio asked to *output* gender = 87.8%; probe on internals = 94.6% â†’ **model knows more than it says** (same finding as LLMs)

#### B. Hallucination Detection from Decoder Residual Stream
- Binary task: zero-WER vs. high-WER
- Whisper decoder layer 22: **93.4% accuracy** (LibriSpeech), **88.1%** (CommonVoice)
- Qwen2-Audio layer 22: **70.2%** (LibriSpeech), **83.6%** (CommonVoice)
- Implication: **hallucination signals are encoded at the <eos> position** â†’ real-time quality monitor with zero overhead

#### C. Speech vs. Non-Speech Detection
- Perfect 100% accuracy at decoder layers 10-28
- Model generates confident transcriptions for noise, but internally "knows" it's non-speech
- Practical use: add probe as real-time hallucination flag at inference

#### D. Acoustic vs. Contextual Mechanism (most relevant to "Listen vs Guess")
- Dataset: 700 sentences with phonetically ambiguous words (e.g., "white lice" said when "white rice" expected)
- Qwen2-Audio made contextual errors on 251/700 examples
- Whisper errors on 153/700 (less context-biased)
- **Activation patching revealed:** encoder components (not just decoder) are responsible for contextual errors â€” patching encoder with white noise **improves acoustic accuracy**
- **Takeaway: encoder encodes semantic/contextual information, not just acoustics!** Violates common assumption of encoder-decoder role separation.
- This directly connects to Leo's "Listen vs Guess" Track 3

#### E. Repetition Loops
- Specific cross-attention heads in decoder identified as responsible for repetitions
- Ablating these heads reduces repetitions
- Encoder Lens shows semantics emerge mid-encoder, stabilize in upper layers

---

### Methodological Insights (for Leo's research)

1. **White noise as reference** for activation patching = good baseline for "removing acoustic information"
2. **Saturation layer** concept = when model commits; useful for "listen vs guess" analysis
3. **Encoder Lens** = novel tool, not in TransformerLens or pyvene yet â†’ opportunity to implement for audio
4. **Probing limitation:** high probe accuracy â‰  causal role (must combine with patching)
5. **Time-averaging encoder activations** for probe = necessary since audio frames â‰  fixed positions

---

### Connection to Leo's Research Goals

| Paper Finding | Leo's Track | How It Connects |
|---------------|-------------|-----------------|
| Encoder encodes context (not just acoustics) | Track 3 (Listen vs Guess) | Direct overlap â€” this is evidence that encoder grounding can be measured |
| Saturation layer = commitment point | Track 3 | Grounding coefficient can use saturation layer as proxy |
| Hallucination from decoder residual | Track 2 (AudioSAE) | SAE features might also predict hallucinations |
| Encoder Lens tool | Track 1 (Causal Protocol) | Should be part of the audio mech interp toolkit |
| White noise as reference input | Track 1 | Standard reference choice for audio patching |

### Open Questions Raised
1. AudioLens (æ™ºå‡±å“¥) does logit-lens for LALMs â€” how does it compare to this encoder lens approach?
2. The "saturation layer" metric: does it generalize from encoder-decoder (Whisper) to decoder-only (Qwen2-Audio)?
3. Can patching sensitivity ratio (acoustic vs. context) directly operationalize Leo's "grounding coefficient"?
4. They use white noise as reference â€” would silence work better? Corrupted speech? How to choose?
5. Their repetition mechanism finding: specific attention heads control loops â€” can SAE features identify these heads more cleanly?

---

### Paper's Gaps / Critique
- No code released yet (as of paper date)
- Only Whisper-large-v3 + Qwen2-Audio-7B; smaller/larger models not tested
- Contextual bias dataset is synthetic (TTS) â€” real speech may differ
- No circuit-level analysis (attention head composition, QK/OV circuits) â€” stays at layer-level
- Encoder Lens requires decoder for output â†’ can't analyze encoder-only models directly

---

## Next: 
Cycle #7 should read **Whisper LoRA Mech Interp** (arXiv:2509.xxxxx) to understand fine-tuning's mechanistic effect. Or pivot to **AudioLens** (arXiv:2506.05140) since it's the most directly relevant to Leo's lab collaboration.
Recommendation: **AudioLens first** (lab connection > methodological diversity)

## Tags: #mech-interp #ASR #whisper #activation-patching #probing #logit-lens #hallucination #listen-vs-guess #encoder-lens
