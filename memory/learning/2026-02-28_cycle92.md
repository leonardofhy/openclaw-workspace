# ðŸ§  Cycle #92 â€” 2026-02-28 21:31
## Action: reflect (synthesis)
## Context: Last 3 cycles = skill-up/learn/learn â†’ reflect triggered by rule. Q7 synthesis threshold also active (16+ deep reads, 0 experiments). Accumulated methodology upgrades from cycles #83-91 not yet integrated into paper-a-pitch.md v0.1. Goal: synthesize into Paper A v0.2.

## Content: Paper A Method Upgrade (v0.1 â†’ v0.2)

### What changed since paper-a-pitch.md v0.1 (created cycle #57, 04:01 AM):

**Cycles #83-91 produced 5 methodology upgrades:**

1. **DAS-grounded gc(k)** (cycle #83 â€” IIT + DAS + pyvene):
   - OLD: gc(k) = Î”acc(audio patch) / [Î”acc(audio) + Î”acc(text)] â€” ratio, not theoretically pure
   - NEW: gc(k) = IIT accuracy using DAS (Distributed Alignment Search) at layer k
   - DAS trains a linear rotation to align neural subspace to causal model â†’ IIT accuracy = provably grounded (Geiger et al. 2301.04709 + Wu et al. pyvene)
   - Paper framing upgrade: "We PROVE audio causal abstraction at L*" (not just "we observe gc peak")
   - cite: Wu et al. arXiv:2403.07809 (pyvene library)

2. **MMProbe for causal direction** (cycle #89 â€” ARENA [1.3.1]):
   - LR probe = maximally discriminative direction â‰  causally implicated direction (can be orthogonal!)
   - MMProbe = difference-of-means = points in direction of causal effect â†’ correct for intervention
   - Rule: PROBE_LAYER â‰  INTERVENE_LAYER â€” must sweep both independently
   - Implementation note: MMProbe direction extracted at probe_layer â†’ DAS rotation around that direction at intervene_layer
   - Paper A method section update: use MMProbe for direction extraction, then DAS at intervention layer

3. **layer_sweep_accuracy template** (cycle #89):
   - ARENA [1.3.1] has exact code pattern for sweeping probes across layers â†’ direct template for gc(k) plot
   - gc(k) curve = layer_sweep_accuracy applied to the AUDIO PATCH intervention (not just direction)
   - Produces Figure 2 of Paper A: L* = argmax gc(k) over layers

4. **NNsight > CLT for audio-LLMs** (cycle #90):
   - Attribution graphs (circuit-tracer) = CLT features + edge weights; decoder-only only + attention patterns FROZEN
   - Audio-LLMs require cross-attention patching (audio tokens â†’ LLM) â†’ circuit-tracer cannot do this
   - NNsight patching = correct tool; circuit-tracer = valid follow-up for LM backbone (text-only analysis)
   - NEW: gc(F) can ALSO be expressed as edge-weight fraction from audio frames in attribution graph â€” but only as secondary analysis (circuit-tracer can't be primary for audio)

5. **Realistic attribution success rate** (cycle #91 â€” Biology of LLMs):
   - Anthropic reports ~25% success rate for mechanistic attribution of model behavior in LLMs
   - Audio-LLMs likely harder (larger, multimodal cross-attention) â†’ expect 15-25% interpretable circuits
   - Impact on Paper A: frame as "Listen Layer localization" (coarser-grained, higher success rate) NOT full circuit enumeration
   - Paper A scope = LAYER-LEVEL causal claim (achievable) + optional head attribution (lower success rate â†’ extension)

### Synthesis: Paper A v0.2 Method Summary

```
Phase 1 (MacBook, ~3h):
  - Whisper-small, LibriSpeech minimal pairs (or phonological arithmetic pairs from Choi et al.)
  - NNsight denoising patching, per-encoder-layer
  - MMProbe (diff-of-means) extracts audio direction at probe_layer
  - DAS rotation at intervene_layer â†’ gc(k) = IIT accuracy
  - Plot gc(k) vs layer â†’ find L*
  - Expected: L* â‰ˆ layer 3 (= 50% depth, Triple Convergence zone)

Phase 2 (NDIF/GPU, ~1 day):
  - Qwen2-Audio-7B, ALME 57K conflict stimuli
  - Two-sweep: audio-stream patch + text-stream patch per LLM layer
  - gc(L) = IIT accuracy (DAS) at each layer
  - Find LALM L* â†’ "Listen Layer in the LLM"
  - Expected: peak in layers 16-22 (based on Zhao et al. ESN clustering)

Phase 3 (optional extensions):
  - LoRA shift (Behind the Scenes): does L* shift after SER fine-tuning?
  - Attention head attribution: top-5 "listen heads" at L*
  - Attribution graph (NNsight): gc(F) as edge fraction â€” secondary analysis only
```

### Phonological Vector Geometry as Stimuli Source (Gap #18 â€” cycle #81+82)

NEW addition to Paper A: Use Choi et al. (2602.18899) phonological minimal pairs as Phase 1 stimuli
- voicing contrasts ([b]/[d]/[p]/[t]) = principled "clean/corrupt" = better than random LibriSpeech
- If phonological vector geometry SURVIVES connector (Gap #18 = Priority 0 experiment) â†’
  confirms audio representations have the right structure for IIT
- If NOT â†’ connector = bottleneck â†’ framing upgrades to "Listen Layer blocked by connector"
- Either way = publishable finding; Phase 1 now doubles as Gap #18 experiment

### Cross-paper grounding (all confirmed post-cycle-91):

| Concept | Theory source | Method source | Stimuli source |
|---------|-------------|--------------|----------------|
| IIT accuracy = gc | Geiger et al. 2301.04709 | pyvene (Wu et al. 2403.07809) | ALME (2602.11488) |
| DAS = linear subspace | Geiger et al. + ARENA [1.3.1] | pyvene DAS module | â€” |
| MMProbe direction | ARENA [1.3.1] diff-of-means | sklearn/numpy | Choi et al. minimal pairs |
| NNsight hooks | Behind the Scenes (Ma et al.) | `pip install nnsight` | â€” |
| Triple Convergence = L* | AudioSAE + BeyondTranscription + AudioLens + whisper_hook_demo | â€” | â€” |
| Phonological minimal pairs | Choi et al. 2602.18899 | phonetic-arithmetic repo | LibriSpeech + MFA |

## Next: 
- paper-a-pitch.md updated to v0.2 (this cycle)
- Daily-consolidate (Day 3 digest) = already done (cycle #84); progress.md update = this cycle
- Next high-value action: awaiting arXiv Monday batch (~14:00) or Leo unblock
- Recommended cycle #93: skip OR pre-digest neuronpedia.org for Paper B SAE feature visualization

## Tags: #reflect #synthesis #paper-a #das #mmprobe #arena #nnsight #rit #methodology
