# ðŸ§  Cycle #34 â€” 2026-02-27 07:00
## Action: plan
## Context: Queue fully depleted (cycles #28-33). Must-read list exhausted. arXiv Feb 27 submissions not yet posted (appear ~14:00 Taipei). Leo waking up ~7-9 AM. Highest-value action = crystallize the accumulated reading into a concrete experiment proposal that Leo can approve/reject this morning.
## Content:

### ðŸ§ª Experiment Proposal: Triple Convergence IIT Test
*Ready for Leo's review*

**Hypothesis:** The "semantic crystallization layer" (~50% depth) independently found by AudioSAE (layer 6-7 in large), Beyond Transcription (saturation layer), and AudioLens (critical layer) all refer to the same architectural transition point â€” validatable via causal abstraction (IIT).

**Design (MacBook-feasible, Whisper-tiny/small, ~3h coding):**

Step 1: Norm + CKA sweep (DONE â€” whisper_hook_demo.py confirms layer 3 = 4.2x jump for Whisper-base)

Step 2: Logit-lens saturation curve (DONE â€” whisper_logit_lens.py, needs real speech for sharp signal)

Step 3: Add causal IIT test (NEW â€” needs coding approval):
- Take 10 pairs of speech clips: (correct_transcription_clip, incorrect_transcription_clip)
- For each layer L: patch internal state from clean clip â†’ corrupt clip
- Measure: does accuracy recover? (denoising = sufficiency test)
- Expected: recovery peaks at Triple Convergence layer â†’ this IS the causally necessary layer
- Metric: logit diff (not accuracy, per Heimersheim & Nanda best practices)

**Claim if confirmed:** "We demonstrate via causal interchange interventions (IIT) that the semantic crystallization layer corresponds to the transition where audio representations become causally sufficient for transcription accuracy â€” providing the first causal confirmation of a phenomenon previously observed via three independent observational methods."

**Costs:**
- Speech files: need 10 LibriSpeech clips (free, downloadable)
- Compute: Whisper-base/small on MacBook, ~10 min per clip
- Code: ~100 lines extending whisper_hook_demo.py
- Dependencies: already have transformers + pyvene cheat sheet; or use NNsight (recommended)

**Risks:**
- Whisper-base only 6 layers â†’ "mid-layer" = layer 3 (already confirmed)
- Whisper-small = 12 layers â†’ better resolution, still MacBook-feasible
- IIT requires minimal pairs (not white noise) â€” needs real speech contrasts

---

### ðŸ§ª Experiment Proposal: Class-specific Neuron Grounding (grounding_coefficient at neuron level)
*From cycles #24-26, Zhao et al. + Kawamura et al.*

**Hypothesis:** Emotion-sensitive neurons (ESNs) in LALMs (Qwen2.5-Omni) respond to audio emotional content, not just linguistic/text context â€” i.e., they are causally grounded in audio.

**Design:**
1. Apply AAPE/MAD selector to find top-K emotion-specific neurons in Qwen2.5-Omni
2. For each neuron cluster: run 2 denoising sweeps:
   a. Patch audio tokens from emotional clip â†’ neutral clip (audio patching)
   b. Patch text tokens from emotional label â†’ neutral label (text patching)
3. Compute: gc_neuron = Î”activation(audio patch) / (Î”activation(audio) + Î”activation(text))
4. Expected: early-layer ESNs have gc > 0.8 (audio-driven); late-layer ESNs may have lower gc (more text-context-dependent)

**Requires:** GPU (Qwen2.5-Omni-7B). Options: NNsight NDIF remote execution OR lab server (SSH tunnel to DESKTOP-Q1L6LLN).

**Why now:** Zhao et al. just published (Jan 2026, JHU). Field is moving. This extends their work with one clean new contribution (audio-vs-text grounding). ~2-3 weeks to implement + run.

---

### ðŸ“Œ Morning Queue for Leo (priority order)
1. **[APPROVAL REQUEST]** Triple Convergence IIT test â€” needs: (a) 10 LibriSpeech clips download OK?, (b) coding using NNsight OK?, (c) estimate: 1 session
2. **[DEMO]** Run whisper_hook_demo.py on a real .wav file â€” Leo provides or I download one LibriSpeech clip
3. **[INFO]** Class-specific Neuron Grounding experiment (needs GPU, lower priority vs Triple Convergence)
4. **[WAIT]** arXiv Feb 27 submissions appear ~14:00 Taipei â€” schedule cs.SD + cs.CL scan for afternoon cycle

## Next: When Leo wakes up â†’ present this proposal. Await approval. If approved â†’ proceed to build (librispeech download + extend whisper_hook_demo.py with IIT patching).
## Tags: #plan #experiment-design #triple-convergence #IIT #causal-abstraction #leo-approval-needed
