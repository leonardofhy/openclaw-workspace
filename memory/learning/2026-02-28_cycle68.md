# ðŸ§  Cycle #68 â€” 2026-02-28 09:31
## Action: learn (citation scan â€” competition mapping)
## Context: arXiv Feb 28 batch not yet posted (~14:00 Taipei). Meta-board saturated. Chose targeted Semantic Scholar search to check for competing papers on Paper A's core claim ("Listen Layer" / audio causal grounding). Execution-blocked on experiments, but novelty possible via competition mapping.

## Content

### Query: "listen layer speech LLM mechanistic interpretability" + "audio grounding speech LLM causal patching layer"

### Key Finding: FCCT (Li et al., Nov 2025, AAAI 2026 Oral)
**"Causal Tracing of Object Representations in Large Vision-Language Models"** [arXiv:2511.05923]
- Full causal tracing framework for VISION-LLMs (LVLMs)
- METHOD: FCCT â€” causal effects across visual+textual tokens, MHSA+FFN+hidden states, ALL layers
- KEY FINDING: MHSAs of LAST TOKEN in MIDDLE LAYERS = critical for cross-modal info aggregation; FFNs show 3-stage hierarchical progression
- APPLICATION: IRI (training-free inference-time injection at specific layers) â†’ SOTA hallucination mitigation on 5 benchmarks
- Venue: **AAAI 2026 Oral** (highly respected)

### Significance for Paper A ("Localizing the Listen Layer in Speech LLMs")
- FCCT is the VISION equivalent of exactly what Paper A proposes for AUDIO
- This is **the closest competitor** â€” but it's vision-only, not speech/audio
- Frames Paper A even better: "we do for speech LLMs what FCCT did for vision LLMs, with audio-specific adaptations (temporal alignment, acoustic vs linguistic, IIT grounding coefficient)"
- IRI analogy: Leo's "Listen Layer" paper could produce a training-free inference-time fix too (patch the listen layer = IRI for audio)
- FCCT finding (middle layers = cross-modal aggregation) is consistent with Triple Convergence Hypothesis (layers 6-7 in Whisper-base)
- **No competition** in audio/speech space â€” FCCT confirms the pattern but leaves audio completely open

### Other results: 5 papers returned, all irrelevant to speech:
- LayerNorm removal for MI (text LLMs)
- Domain-specific SAEs for medical text
- Sparse Tracing for long context
- LLM relevance understanding for IR
- FCCT (vision) â€” RELEVANT âœ…

### Rate limit on second query (429) â€” session-limited, acceptable

## RECORD: Competition Map Update

| Paper | Domain | Method | Gap for Leo |
|-------|--------|--------|-------------|
| AudioLens (2025) | Audio-LLM | Logit Lens | No causal patching |
| Beyond Transcription (2025) | Whisper | Encoder Lens + patching | No audio-vs-text pathway |
| Zhao et al. 2601.03115 (2026) | Audio-LLM | ESN deactivation | No audio-vs-text pathway |
| ALME (2602.11488, 2026) | Audio-LLM | Behavioral (no patching) | No causal layer attribution |
| **FCCT (2511.05923, 2025)** | **Vision-LLM** | **Full causal tracing âœ…** | **Audio version = LEO's gap** |

Leo = first to do full causal tracing in SPEECH LLMs (FCCT is the closest work â€” in vision only).

## Next: paper-a-pitch.md should add FCCT as "most similar work" in related work map.

## Tags: #competition #paper-a #listen-layer #causal-tracing #vision-llm-analogy
