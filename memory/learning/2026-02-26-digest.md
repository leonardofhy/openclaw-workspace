# üìã 2026-02-26 Learning Digest

> 10 cycles completed (updated 18:30). First day of Autodidact system.

## Cycle #1: UniWhisper (learn)
- Unified audio representation via instruction-answer format, 20-task eval
- MLP probe NWA: 0.64‚Üí0.81 over vanilla Whisper
- Relevance: benchmark design methodology for AudioMatters

## Cycle #2: AudioMatters Competitive Landscape (learn)
- Scanned 8 papers: AudioRAG, EmoOmni, GSRM, Qwen3-ASR, PhoStream, PodBench, etc.
- Key finding: all existing benchmarks are narrow-scoped (retrieval-only, emotion-only, streaming-only)
- AudioMatters fills vacuum as cross-scenario, cross-capability, LLM-era unified evaluation

## Cycle #3: Mech Interp √ó Speech Field Scan (learn)
- **Only 4 papers exist** on "mechanistic interpretability" + "speech" in arXiv
- Papers: Beyond Transcription (2025-08), Whisper LoRA interp (2025-09), Brain-to-speech (2026-02), Audio SSL neuron dissection (2026-02)
- Multimodal mech interp survey (2025-02) barely covers speech = GAP
- Vision has Prisma toolkit, speech has nothing = GAP
- Generated 3 paper ideas (see goals.md)

## Cycle #4: Day 1 Reflect (reflect)
- 0 code written (weakness to fix next week)
- Must-read list: 6 papers flagged, 0 deeply read ‚Üí first priority post-AudioMatters
- Post-AudioMatters cycle order: Beyond Transcription ‚Üí Whisper LoRA interp ‚Üí TransformerLens skill-up ‚Üí probing experiment
- MacBook-feasible experiments identified (no GPU needed for first 4 cycles)

## Cycle #6: Beyond Transcription Deep Read (learn)
- Encoder Lens applied to Whisper-large-v3 + Qwen2-Audio
- Hallucination detectable from decoder residual stream (93.4% acc)
- Saturation layer = where encoder "commits" to a transcription
- Encoder encodes context, not just acoustics
- Connects to Track 3: patching sensitivity ‚Üí grounding coefficient

## Cycle #7: AudioLens Deep Read (learn)
- Êô∫Âá±Âì•'s lab paper (NTU, ASRU 2025)
- LALMs heavily rely on querying audio inputs directly (not text context) ‚Äî empirical evidence for "Listen vs Guess"
- Critical layer = weighted average resolution layer; earlier = better accuracy
- Failure mode = info peaks mid-layer then drops; success = monotonic rise
- KEY GAP: no causal patching ‚Üí Leo's opportunity = "Causal AudioLens" + grounding_coefficient
- Cross-paper synthesis: saturation layer (Beyond Transcription) ‚Üî critical layer (AudioLens) ‚Üí unified framework possible

## Cycle #8: AudioSAE Deep Read (learn)
- SAE trained on all 12 layers of Whisper/HuBERT; >50% feature stability across seeds
- Layer 6-7 = speech/acoustic transition: audio-level encoding peaks layer 6, drops at 7; frame-level (phonetic) rises at 7
- **Steering: 70% hallucination FPR reduction** via suppressing top-100 features (Œ±=1, WER cost only +0.4%)
- Speech concepts distributed (2000 features to erase accent vs ~tens in text) ‚Üí higher redundancy
- EEG correlation: SAE features align with brain response (0-500ms lags, Pz electrode)
- **Synthesis: Whisper layer 6-7 = "saturation layer" (BeyondTranscription) = "critical layer" (AudioLens)?** Three papers may have independently found same transition zone
- Code available: https://github.com/audiosae/audiosae_demo

## Cycle #9: TransformerLens + pyvene Skill-Up (18:00)
- Created full cheat sheet: `skills/autodidact/references/transformerlens-pyvene-cheatsheet.md`
- Key clarification: **TransformerLens = decoder-only only** ‚Üí pyvene is the right tool for Whisper/HuBERT encoder MI
- Documented 5 ordered MacBook-feasible experiments: text IOI ‚Üí Whisper cache ‚Üí probing ‚Üí manual patch ‚Üí pyvene systematic
- pyvene `IntervenableModel` wraps ANY PyTorch model ‚Üí enables "Causal AudioLens" experiments
- Timed correctly: Leo focused on AudioMatters 19:00 deadline; cheat sheet ready for post-deadline use

## Cycle #10: whisper_hook_demo.py (build, 18:30)
- Built `skills/autodidact/scripts/whisper_hook_demo.py` (230 lines, MacBook-feasible)
- Features: hook registration on all encoder layers, CKA heatmap, norm-per-layer, layer 6 deep-inspect
- Synthetic audio fallback (no files needed), headless-safe matplotlib output
- Syntax verified. Ready to run: `python skills/autodidact/scripts/whisper_hook_demo.py --no-plot`
- This is "Experiment 2" from the cheat sheet; can be extended to pyvene patching (Experiment 4-5)

## Cycle #11: Day 1 Reflect (19:00)
- Formalized **Triple Convergence Hypothesis** into knowledge-graph.md as Experiment 0
- Updated goals.md: AudioMatters deadline passed ‚Üí attention shifts to mech interp
- Updated goals.md with "Immediate next steps" section
- Micro-reflect: rated all 11 cycles; cycles #6-7-8 = the high-value spine of the day
- Day stats: 4 deep reads, 1 cheat sheet, 1 script, 4 gaps, 3 paper ideas, 0 experiments run
- **Weakness identified**: 0 experiments run today ‚Üí Day 2 must start with running whisper_hook_demo.py
- **Clearest paper opportunity**: Causal AudioLens = AudioLens + patching + grounding_coefficient

## Cycle #12: Toolchain Verified ‚Äî whisper_hook_demo.py (20:30)
- **First actual experiment run ‚úÖ** ‚Äî resolves Day 1 weakness of 0 experiments executed
- Whisper-base = 6 layers (not 12 like large-v3 in AudioSAE paper)
- Clear norm transition at **layer 3** (midpoint): 565 ‚Üí 2379 (4.2x jump)
- CKA heatmap: layers 0-2 cluster together (acoustic); layers 3-5 cluster together (semantic)
- **New insight**: transition zone = ~50% depth is model-size invariant. base=layer3, large=layer6-7, tiny‚âàlayer2. This strengthens the Triple Convergence Hypothesis.
- CKA plot saved: `/tmp/whisper_hook_demo.png`
- Open question: Does the transition persist with real speech vs synthetic sine wave?

## Cycle #13: SPIRIT Deep Read (21:00)
- **SPIRIT** = activation patching defense for audio jailbreaks (EMNLP 2025, MBZUAI)
- PGD attack on Qwen2-Audio + LLaMa-Omni achieves **100% ASR** in some categories
- Activation patching (inject clean activations at specific layers) reduces ASR to ~1% with negligible utility cost
- **Key gap**: patching is empirical/blind ‚Äî doesn't explain where adversarial signal lives or use SAE-guided feature suppression
- **New synthesis**: AudioSAE (interpretable feature steering) + SPIRIT (activation patching defense) = "SAE-guided inference-time safety patching" ‚Äî more interpretable, more surgical
- **Question**: Does SPIRIT's optimal defense layer = Triple Convergence transition zone in Whisper?
- Code available: https://github.com/mbzuai-nlp/spirit-breaking
- Cycle #14 recommended: build ‚Äî extend whisper_hook_demo.py with logit-lens projection

## Cycle #14: whisper_logit_lens.py (build, 21:30)
- **Built full logit-lens implementation for Whisper** (`whisper_logit_lens.py`, ~300 lines)
- Method: apply encoder's ln_post + project via decoder token embedding matrix ‚Üí vocab logits per layer
- Layer Information Score (LIS) = cosine similarity of softmax distributions vs. final layer
- Key finding: **synthetic sine wave audio gives compressed LIS (0.90-0.95 from layer 0)** ‚Äî threshold recalibration needed; entropy or KL-div would be sharper metric
- LIS still monotonically increases (0.9027 ‚Üí 0.9493), confirming progressive refinement
- Token decode: early layers ‚Üí "tek"/"mL" (noise); late layers ‚Üí language ID tokens (`<|hu|>`, `<|ar|>`) ‚Äî consistent with model seeing non-speech input
- **Real speech needed** to see clean acoustic‚Üísemantic transition (script design is correct, inputs are limited)
- Script verified ‚úÖ ‚Äî runs end-to-end in ~30 seconds on MacBook
- Next recommended: read Heimersheim & Nanda "activation patching best practices" before writing pyvene code

## Cycle #15: Heimersheim & Nanda "Activation Patching Best Practices" (22:00)
- **arXiv:2404.15255** ‚Äî 13-page tutorial, fully read
- **Critical distinction**: Denoising (clean‚Üícorrupt) tests SUFFICIENCY; Noising (corrupt‚Üíclean) tests NECESSITY ‚Äî these are NOT symmetric!
- AND gates (serial): use noising. OR gates (parallel): use denoising. Audio circuits likely have both ‚Üí must choose deliberately.
- **Metrics**: Logit diff > logprob > probability > accuracy for exploratory patching. Probability is non-linear ‚Üí distorts partial effects.
- **Gaussian noise patching is fragile**: highly sensitive to noise level; Beyond Transcription used this approach ‚Üí results should be treated with caution / reproduced with minimal pairs
- **NEW GAP identified**: All audio MI papers use suboptimal corruptions (white noise, random speech). Minimal pair audio corruptions (same speaker + duration + content structure, different attribute) = cleaner causal evidence ‚Üí methodological improvement Leo can claim
- **Grounding coefficient now operationalizable**: gc = Œîacc(audio patch) / (Œîacc(audio patch) + Œîacc(text patch)) ‚Äî clean causal metric, runs in two denoising sweeps
- **Backup behavior (Hydra effect)**: ablating key components activates backups ‚Äî component looks weaker than it is. Log this as a risk for future patching experiments.

## Cycle #16: "Behind the Scenes" Whisper LoRA MI Deep Read (22:31)
- **arXiv:2509.08454** ‚Äî "Behind the Scenes: Mechanistic Interpretability of LoRA-adapted Whisper for SER" (Ma et al., ICASSP 2026)
- First systematic MI study of LoRA in speech models
- **Finding 1 ‚Äî Delayed Specialization**: LoRA preserves early layers as general-purpose, commits to task in deep layers only (flat KL early ‚Üí sharp drop late). Frozen encoder = volatile/unstable.
- **Finding 2 ‚Äî Forward Alignment, Backward Differentiation**: A matrix aligns to input; B matrix differentiates for task. Deep layers show counter-directional (negative cosine similarity) corrective signals that suppress ASR-specific features.
- **New tool: NNsight library** ‚Äî alternative to pyvene; used in this paper for Whisper encoder access
- **KEY GAP**: No causal patching in this paper OR AudioLens ‚Üí same unaddressed gap ‚Üí Leo can close both with one unified approach
- **New synthesis**: Delayed specialization explains WHY the Triple Convergence transition zone exists ‚Äî it's where both acoustic‚Üísemantic AND old-task‚Üínew-task transitions trigger simultaneously
- **Combined paper idea**: Track 3 (Listen vs Guess) + Track 4 (Mechanistic Interp of Adaptation) may be one paper: "Causal AudioLens with LoRA" ‚Äî logit-lens already in both, add patching = unified contribution

## Key Decisions
- Leo shifted research interest to mech interp √ó speech (away from just AudioMatters)
- Set north star: DeepMind/Anthropic-level AI researcher
- Built Autodidact skill with 8 core values
- **AudioMatters deadline passed** ‚Üí full attention now available for mech interp (as of 19:00)

## Papers Encountered
| Paper | Year | Read Depth | Relevance |
|-------|------|-----------|-----------|
| UniWhisper | 2026-02 | Deep | AudioMatters |
| AudioRAG | 2026-02 | Scan | Competitive |
| EmoOmni | 2026-02 | Scan | Competitive |
| Beyond Transcription (mech interp ASR) | 2025-08 | **Deep** | **Core** |
| Whisper LoRA mech interp | 2025-09 | Scan | **Core** |
| Audio SSL neuron dissection | 2026-02 | Scan | **Core** |
| Prisma (vision interp toolkit) | 2025-04 | Scan | Method transfer |
| Mech Interp MMFM Survey | 2025-02 | Scan | Landscape |
| AudioLens (NTU ASRU 2025) | 2025-06 | **Deep** | **Core ‚Äî Track 3** |
| SPIRIT (Djanibekov et al., EMNLP 2025) | 2025-05 | **Deep** | **Core ‚Äî Track 5** |
| Behind the Scenes (Ma et al., ICASSP 2026) | 2025-09 | **Deep** | **Core ‚Äî Track 4** |

## Deep Research Integration (15:16)
Leo Êèê‰æõ GPT-5.2 Pro deep research reportÔºåÂ§ßÂπÖÊõ¥Êñ∞ landscapeÔºö
- ÂØ¶ÈöõÁõ∏ÈóúÂ∑•‰Ωú 15-20+Ôºà‰∏çÊòØ‰πãÂâç‰ª•ÁÇ∫ÁöÑ 4 ÁØáÔºâ
- **üî• AudioLens ÊòØ NTU ÊùéÂÆèÊØÖ lab ÁöÑÂ∑•‰Ωú** ‚Äî ‰∏ªÂ†¥ÂÑ™Âã¢
- AudioSAE (EACL 2026), SPIRIT (EMNLP 2025) ÊòØÊñ∞ÁöÑÈáçË¶Å anchor
- Paper ideas ÈáçÊéíÔºöListen vs Guess (#1) > InterpBench (#2) > Safety (#3)
- Êõ¥Êñ∞ must-read listÔºàAudioLens ÂçáÁÇ∫ #1Ôºâ
