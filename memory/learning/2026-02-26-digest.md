# ğŸ“‹ 2026-02-26 Learning Digest

> 10 cycles completed (updated 18:30). First day of Autodidact system.

## Cycle #1: UniWhisper (learn)
- Unified audio representation via instruction-answer format, 20-task eval
- MLP probe NWA: 0.64â†’0.81 over vanilla Whisper
- Relevance: benchmark design methodology for AudioMatters

## Cycle #2: AudioMatters Competitive Landscape (learn)
- Scanned 8 papers: AudioRAG, EmoOmni, GSRM, Qwen3-ASR, PhoStream, PodBench, etc.
- Key finding: all existing benchmarks are narrow-scoped (retrieval-only, emotion-only, streaming-only)
- AudioMatters fills vacuum as cross-scenario, cross-capability, LLM-era unified evaluation

## Cycle #3: Mech Interp Ã— Speech Field Scan (learn)
- **Only 4 papers exist** on "mechanistic interpretability" + "speech" in arXiv
- Papers: Beyond Transcription (2025-08), Whisper LoRA interp (2025-09), Brain-to-speech (2026-02), Audio SSL neuron dissection (2026-02)
- Multimodal mech interp survey (2025-02) barely covers speech = GAP
- Vision has Prisma toolkit, speech has nothing = GAP
- Generated 3 paper ideas (see goals.md)

## Cycle #4: Day 1 Reflect (reflect)
- 0 code written (weakness to fix next week)
- Must-read list: 6 papers flagged, 0 deeply read â†’ first priority post-AudioMatters
- Post-AudioMatters cycle order: Beyond Transcription â†’ Whisper LoRA interp â†’ TransformerLens skill-up â†’ probing experiment
- MacBook-feasible experiments identified (no GPU needed for first 4 cycles)

## Cycle #6: Beyond Transcription Deep Read (learn)
- Encoder Lens applied to Whisper-large-v3 + Qwen2-Audio
- Hallucination detectable from decoder residual stream (93.4% acc)
- Saturation layer = where encoder "commits" to a transcription
- Encoder encodes context, not just acoustics
- Connects to Track 3: patching sensitivity â†’ grounding coefficient

## Cycle #7: AudioLens Deep Read (learn)
- æ™ºå‡±å“¥'s lab paper (NTU, ASRU 2025)
- LALMs heavily rely on querying audio inputs directly (not text context) â€” empirical evidence for "Listen vs Guess"
- Critical layer = weighted average resolution layer; earlier = better accuracy
- Failure mode = info peaks mid-layer then drops; success = monotonic rise
- KEY GAP: no causal patching â†’ Leo's opportunity = "Causal AudioLens" + grounding_coefficient
- Cross-paper synthesis: saturation layer (Beyond Transcription) â†” critical layer (AudioLens) â†’ unified framework possible

## Cycle #8: AudioSAE Deep Read (learn)
- SAE trained on all 12 layers of Whisper/HuBERT; >50% feature stability across seeds
- Layer 6-7 = speech/acoustic transition: audio-level encoding peaks layer 6, drops at 7; frame-level (phonetic) rises at 7
- **Steering: 70% hallucination FPR reduction** via suppressing top-100 features (Î±=1, WER cost only +0.4%)
- Speech concepts distributed (2000 features to erase accent vs ~tens in text) â†’ higher redundancy
- EEG correlation: SAE features align with brain response (0-500ms lags, Pz electrode)
- **Synthesis: Whisper layer 6-7 = "saturation layer" (BeyondTranscription) = "critical layer" (AudioLens)?** Three papers may have independently found same transition zone
- Code available: https://github.com/audiosae/audiosae_demo

## Cycle #9: TransformerLens + pyvene Skill-Up (18:00)
- Created full cheat sheet: `skills/autodidact/references/transformerlens-pyvene-cheatsheet.md`
- Key clarification: **TransformerLens = decoder-only only** â†’ pyvene is the right tool for Whisper/HuBERT encoder MI
- Documented 5 ordered MacBook-feasible experiments: text IOI â†’ Whisper cache â†’ probing â†’ manual patch â†’ pyvene systematic
- pyvene `IntervenableModel` wraps ANY PyTorch model â†’ enables "Causal AudioLens" experiments
- Timed correctly: Leo focused on AudioMatters 19:00 deadline; cheat sheet ready for post-deadline use

## Cycle #10: whisper_hook_demo.py (build, 18:30)
- Built `skills/autodidact/scripts/whisper_hook_demo.py` (230 lines, MacBook-feasible)
- Features: hook registration on all encoder layers, CKA heatmap, norm-per-layer, layer 6 deep-inspect
- Synthetic audio fallback (no files needed), headless-safe matplotlib output
- Syntax verified. Ready to run: `python skills/autodidact/scripts/whisper_hook_demo.py --no-plot`
- This is "Experiment 2" from the cheat sheet; can be extended to pyvene patching (Experiment 4-5)

## Cycle #11: Day 1 Reflect (19:00)
- Formalized **Triple Convergence Hypothesis** into knowledge-graph.md as Experiment 0
- Updated goals.md: AudioMatters deadline passed â†’ attention shifts to mech interp
- Updated goals.md with "Immediate next steps" section
- Micro-reflect: rated all 11 cycles; cycles #6-7-8 = the high-value spine of the day
- Day stats: 4 deep reads, 1 cheat sheet, 1 script, 4 gaps, 3 paper ideas, 0 experiments run
- **Weakness identified**: 0 experiments run today â†’ Day 2 must start with running whisper_hook_demo.py
- **Clearest paper opportunity**: Causal AudioLens = AudioLens + patching + grounding_coefficient

## Key Decisions
- Leo shifted research interest to mech interp Ã— speech (away from just AudioMatters)
- Set north star: DeepMind/Anthropic-level AI researcher
- Built Autodidact skill with 8 core values
- **AudioMatters deadline passed** â†’ full attention now available for mech interp (as of 19:00)

## Papers Encountered
| Paper | Year | Read Depth | Relevance |
|-------|------|-----------|-----------|
| UniWhisper | 2026-02 | Deep | AudioMatters |
| AudioRAG | 2026-02 | Scan | Competitive |
| EmoOmni | 2026-02 | Scan | Competitive |
| Beyond Transcription (mech interp ASR) | 2025-08 | **Deep** | **Core** |
| Whisper LoRA mech interp | 2025-09 | Scan | **Core** |
| Audio SSL neuron dissection | 2026-02 | Scan | **Core** |
| Prisma (vision interp toolkit) | 2025-04 | Scan | Method transfer |
| Mech Interp MMFM Survey | 2025-02 | Scan | Landscape |
| AudioLens (NTU ASRU 2025) | 2025-06 | **Deep** | **Core â€” Track 3** |

## Deep Research Integration (15:16)
Leo æä¾› GPT-5.2 Pro deep research reportï¼Œå¤§å¹…æ›´æ–° landscapeï¼š
- å¯¦éš›ç›¸é—œå·¥ä½œ 15-20+ï¼ˆä¸æ˜¯ä¹‹å‰ä»¥ç‚ºçš„ 4 ç¯‡ï¼‰
- **ğŸ”¥ AudioLens æ˜¯ NTU æå®æ¯… lab çš„å·¥ä½œ** â€” ä¸»å ´å„ªå‹¢
- AudioSAE (EACL 2026), SPIRIT (EMNLP 2025) æ˜¯æ–°çš„é‡è¦ anchor
- Paper ideas é‡æ’ï¼šListen vs Guess (#1) > InterpBench (#2) > Safety (#3)
- æ›´æ–° must-read listï¼ˆAudioLens å‡ç‚º #1ï¼‰
