# ðŸ§  Cycle #25 â€” 2026-02-27 02:30
## Action: learn
## Context: Must-read list exhausted. Zhao et al. 2601.03115 flagged by cycle #24 arXiv scan as top priority (emotion-sensitive neurons in LALMs â€” direct Track 3 gap). 2:30 AM but Leo's instructions say no auto-skip at night if high-value action exists. This qualifies.

## Content

### Paper: "Discovering and Causally Validating Emotion-Sensitive Neurons in Large Audio-Language Models"
**Authors**: Xiutian Zhao, BjÃ¶rn Schuller, Berrak Sisman (Johns Hopkins / Imperial College London)
**arXiv**: 2601.03115 (Jan 2026, cs.CL + eess.AS) â€” 16 pages, 6 figures

### Problem
LALMs (Qwen2.5-Omni, Kimi-Audio, Audio Flamingo 3) are used for affective applications but no one has asked: *which internal components causally produce emotion decisions?* Prior audio interpretability work focuses on phonetics/speaker/acoustics â€” not affect.

### Method: Logâ€“Identifyâ€“Intervene
1. **Log**: Attach forward hooks to decoder MLP SwiGLU gates; record activations on *correctly solved* SER items (smart: avoids contamination from failure modes)
2. **Identify**: 4 neuron selectors compared:
   - LAP (activation probability â€” frequency-only)
   - LAPE (activation probability entropy â€” cross-emotion selectivity)
   - MAD (Mean Activation Difference â€” magnitude contrastive)
   - CAS (Contrastive Activation Selection â€” top-emotion margin)
3. **Intervene**: Two types:
   - Deactivation (zero selected neurons â†’ test necessity)
   - Steering (scale by 1+Î± â†’ test controllability)
   - + 3 agnostic injection strategies (label-free)

### Key Findings

**1. ESNs exist and are causally validated (not just correlated)**
- Ablating emotion-e neurons â†’ disproportionate degradation of emotion-e recognition (self > cross deactivation)
- Consistent across Qwen2.5-Omni, Kimi-Audio, Audio Flamingo 3 across 3 benchmarks (IEMOCAP, MELD, MSP-Podcast)

**2. Selector quality matters: MAD and CAS >> LAP/LAPE**
- Frequency/entropy alone insufficient; magnitude + contrastive criteria yield much stronger causal effects
- Practical: if Leo does ESN-style analysis, use MAD or CAS selector

**3. Layer-wise locality: non-uniform, 3 clusters**
- ESNs cluster in: **early (layer 0)**, **early-mid (layers 6-8)**, **later (layers 19-22)**
- NOT evenly distributed â†’ consistent with Triple Convergence Hypothesis (early=acoustic, mid=transition, late=semantic/task)
- Cluster pattern matches "Behind the Scenes" LoRA delayed specialization (layers 6-8 also show LoRA commitment)

**4. Emotion steering works (not just ablation)**
- Amplifying ESNs (gain Î±) biases predictions toward target emotion systematically
- Targeted steering > agnostic injection (non-additivity: ESNs interact non-linearly under joint amplification)

**5. Partial cross-dataset transfer**
- ESNs show asymmetric but non-trivial transfer across IEMOCAP/MELD/MSP-Podcast
- Some emotion categories transfer better than others (anger > disgust pattern in text LLMs too)

### THE KEY GAP â€” Track 3 Connection
**This paper asks "do ESNs exist?" but NOT "where does the causal signal come from â€” audio or text?"**

- The entire paper instruments **decoder MLPs** only
- They never test: do these emotion-sensitive decoder neurons fire because of audio input or text context?
- **Grounding coefficient** = exactly what's missing: gc = Î”acc(audio patch) / (Î”acc(audio patch) + Î”acc(text patch))
- If the ESN responds to audio (gc â‰ˆ 1) = truly listening to emotion in speech
- If ESN responds to text (gc â‰ˆ 0) = guessing from linguistic context ("angry words" not "angry voice")
- **This is Track 3's most direct intervention point yet**

### New Synthesis: ESN + Grounding Coefficient = Combined Contribution
```
Zhao et al. (2601.03115) + AudioLens (2506.05140) + patching best practices
â†’ "Do LALMs Listen or Guess at the Neuron Level?"
â†’ Find ESNs (Zhao method) â†’ for each cluster (layers 0, 6-8, 19-22):
   â†’ patch audio activations from matched-content different-emotion utterance
   â†’ patch text activations from same-emotion different-content prompt
   â†’ compare: does ESN activity follow audio patch or text patch?
   â†’ gc per layer cluster = novel contribution
```

### Technical Notes for Implementation
- Instruments: forward hooks on SwiGLU gate activations (g = SiLU(u))
- Benchmarks: IEMOCAP (4 emotions), MELD (7 emotions), MSP-Podcast (4 emotions)
- Models: Qwen2.5-Omni-7B is most accessible + our toolchain supports Whisper â†’ consider extending
- Code not released, but method is fully described â†’ reproducible
- NNsight (discovered cycle #16) would work well here (used in "Behind the Scenes" for decoder access)

### New Insight: Emotion â‰  Simple Linear Feature
"ESNs interact non-additively under joint amplification" â†’ emotion circuits are not disentangled, unlike text SAE features
â†’ This motivates SAE-style decomposition even for emotions â†’ AudioSAE emotion features would be cleaner substrate
â†’ Track 2 (AudioSAEBench) + Track 3 (Listen vs Guess) may need each other: find ESNs â†’ apply SAE â†’ test grounding

## Research Ideas Generated
1. **"Do LALMs Listen or Guess at the Neuron Level?"** â€” apply Zhao's ESN method + audio/text patching = grounding coefficient per ESN cluster
2. **ESN via SAE features** â€” instead of individual neurons (polysemantic), find ESNs in SAE feature space (monosemantic) â†’ cleaner causal unit
3. **Cross-emotion transfer as a probe** â€” partial transfer (Zhao finding) suggests some emotions share acoustic circuits while others share linguistic circuits; patching could reveal which

## Connection to 10 Core Questions
- Q6 (When does model listen vs guess?): **Most direct answer yet** â€” ESN layer-wise patching with audio/text interventions
- Q5 (Audio SAE evaluation criteria): emotion features may need different coverage metrics than phonetic features (non-additive interaction)
- Q3 (WER alignment to local mechanism): SER = cleaner than WER for ESN localization â†’ better starting point for circuit work

## Next: Recommended Cycle #26
- This cycle completes the "Deep-read Zhao et al." action. All must-reads now truly done.
- **Recommended**: Brief reflect/synthesis on what Cycles #22-#25 add together (Causal Abstraction + Multimodal MI Survey + Zhao et al.) â†’ update knowledge graph
- Then **sleep** â€” 2:30 AM, daylight work (real speech test, NNsight) awaits Leo's participation

## Tags: #emotion #ESN #LALMs #track3 #grounding #patching #Qwen2Audio #neurons #layer-clustering
