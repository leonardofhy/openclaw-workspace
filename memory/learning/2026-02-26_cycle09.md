# ðŸ§  Cycle #9 â€” 2026-02-26 18:00
## Action: skill-up
## Context: 4 deep reads done (#6â€“#8), 3 consecutive learn cycles â†’ skill-up required. AudioMatters deadline 19:00 today â€” Leo is occupied. Chose low-interruption skill-up (read + write cheat sheet, no Leo input needed). Output ready for post-deadline use.

## Content: TransformerLens + pyvene Cheat Sheet

Created: `skills/autodidact/references/transformerlens-pyvene-cheatsheet.md`

### Key outputs:
- **TransformerLens**: decoder-only LMs (GPT-2, Llama), `HookedTransformer`, `run_with_cache`, hook-based patching API â€” clear code patterns documented
- **pyvene**: model-agnostic (works on Whisper/HuBERT!), `IntervenableModel`, VanillaIntervention, DAS (RotatedSpaceIntervention) â€” the right tool for audio patching experiments
- **Manual HuggingFace hooks**: `register_forward_hook` pattern for Whisper encoder layers â€” immediate usability without pyvene
- **MacBook-feasible models**: Whisper-tiny/base/small (39Mâ€“244M), GPT-2 (117M), HuBERT-base (94M)
- **5 ordered first experiments**: text IOI â†’ Whisper cache â†’ probe â†’ manual patch â†’ pyvene systematic

### Critical insight confirmed:
TransformerLens = decoder-only ONLY â†’ **pyvene is the right tool for Whisper/HuBERT experiments**
This is the practical interface between our theory (patching, circuits, grounding coefficient) and implementation.

## Next: 
- Cycle #10: Either build (whisper_hook_demo.py â€” starter script, MacBook-runnable) OR daily-consolidate (if this is the last cycle before 23:50 digest). Given the 19:00 deadline and evening schedule, recommend waiting for post-deadline focus.
- Post-AudioMatters: pyvene installation + Whisper-base first hook run (experiment #2 from cheat sheet)

## Tags: #skill-up #transformerlens #pyvene #whisper #tooling #mech-interp
