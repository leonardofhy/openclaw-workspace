# üß† Cycle #7 ‚Äî 2026-02-26 17:00
## Action: learn
## Context: Cycle #6 recommended AudioLens as the next deep-read. This is Êô∫Âá±Âì•'s lab paper (NTU ÊùéÂÆèÊØÖ lab), directly relevant to Track 3 (Listen vs Guess), and Leo has a collaboration advantage. AudioMatters CMT deadline is at 19:00 ‚Äî still 2 hours away, no active work needed here. High-value action confirmed.

## Content: Deep-read "AudioLens" (Yang et al., 2025, ASRU)
**Paper:** A Closer Look at Auditory Attribute Perception of Large Audio-Language Models  
**arXiv:** 2506.05140v2 | Accepted ASRU 2025  
**Authors:** Neo Ho, Yi-Jyun Lee, Hung-yi Lee (NTU) ‚Äî **Êô∫Âá±Âì• = Chih-Kai Yang (ckyang1124)**

### Problem
- LALMs analyzed at task-level (benchmarks), but internal mechanism = unknown
- Existing work only covers high-level behaviors (biases, hallucinations) without probing representations
- First study of *how LALMs internally process auditory attributes*

### Method
**Core tool:** Logit Lens (training-free vocabulary projection)
- Project intermediate hidden states h_i^‚Ñì through unembedding matrix W_U ‚Üí probability dist over vocab
- "Layer-wise Information Score" I_i^‚Ñì = accuracy of predicting attribute label from layer ‚Ñì, position i
- "Critical Layer" ‚Ñì_i* = weighted average of layer indices (weighted by contribution above chance+threshold)
  - Œ±=0.2, threshold = (1+Œ±)/|Y|; filters near-chance layers

**Models tested:** DeSTA2, Qwen-Audio, Qwen2-Audio  
**Attributes:** speaker gender, spoken language, speaker emotion, animal  
**Dataset:** SAKURA benchmark (500 samples/attribute; 2, 8, 5, 9 labels respectively)  
**Prompts:** 3 formats ‚Äî Direct (P1), QA (P2), MC (P3)  
**Token focus:** Last token position ("is" in "The speaker's gender is ___")

### Key Findings

**RQ1: Attribute evolution across layers**
- Information scores increase with depth but NOT monotonically
- Sharp drops then recoveries are common (some recoveries fail)
- Gender has a characteristic concentrated pattern in mid-to-late layers in Qwen/Qwen2
- Failed recognition = information peaks mid-layer then decreases; success = rises with depth

**RQ2: Success vs. failure dynamics**
- Two opposing dynamics: success ‚Üí info rises monotonically; failure ‚Üí info peaks midway then drops
- This is a clean diagnostic signature: mid-layer peak + drop = failure mode

**RQ3: Critical layer vs. accuracy**
- Negative correlation: earlier resolution layer ‚Üí higher accuracy
- Why: earlier resolution leaves more subsequent layers to *refine* the representation
- Implication: if a model resolves "gender" at layer 10/32, it has 22 layers to polish ‚Üí better performance

**RQ4: Token position information flow**
- LALMs **heavily rely on querying auditory inputs directly** rather than aggregating info at text positions
- Even when attributes are "mentioned" earlier in the prompt, those text positions are insufficient
- This explains why LALMs struggle with complex multi-step reasoning tasks
- Key metaphor: models "look back" at audio tokens for prediction, not "remember" them in text context

**RQ5: Improvement method**
- Enrich deeper-layer representations with earlier attribute-rich representations
- No training required
- **+16.3% relative improvement** in prediction accuracy

### Connections to Leo's Research Goals

**Direct link to Track 3 (Listen vs Guess):**
- "LALMs rely on querying audio inputs directly" = empirical evidence for the "Listen vs Guess" hypothesis
- AudioLens uses Logit Lens; Leo's work could extend this with *causal* patching to *quantify* the grounding coefficient
- AudioLens finding ‚Üí operationalize: patching sensitivity to audio tokens vs. text tokens = grounding coefficient

**Critical layer concept:**
- "Saturation layer" in Beyond Transcription (cycle #6) ‚âà "Critical Layer" in AudioLens ‚Äî both identify where attributes resolve
- Cross-paper connection: Glazer's encoder saturation layer ‚Üî Yang's critical layer in LALM decoder
- Combined: can we build a unified "resolution layer" analysis across encoder (Whisper) and decoder (LALM)?

**Gap AudioLens doesn't fill:**
1. Only uses Logit Lens (no causal patching) ‚Äî cannot distinguish "the model encodes it" from "the model actually USES it"
2. No intervention experiments ‚Äî we don't know if the grounding relationship is causal
3. No cross-model comparison of grounding coefficients
4. Dataset: SAKURA (simple attributes) ‚Äî doesn't test complex reasoning or phonetic/prosodic tasks

**These gaps = Leo's research opportunity:**
- AudioLens + patching ‚Üí "We extend AudioLens by introducing causal interventions to verify grounding causally"
- Define: grounding_coefficient = (Œî accuracy when audio patched) / (Œî accuracy when text patched)
- üöÄ This is a concrete, testable extension of AudioLens that adds causal validity

### Open Questions
1. Does the "critical layer" shift when the model is hallucinating vs. correctly recognizing?
2. Is the "querying audio directly" pattern consistent for *phonetic* attributes (phoneme identity, accent) vs. semantic (language ID)?
3. Can we reproduce AudioLens results on a local model (DeSTA2 is smaller) as a starter experiment?
4. Does the +16.3% improvement method transfer to ASR (WER reduction)?

## Next: 
Cycle #8 options:
1. **AudioSAE** (arXiv:2602.05027) ‚Äî fills the SAE dimension gap
2. **Activation Patching Best Practices** (Heimersheim & Nanda) ‚Äî needed before patching experiments
3. **SPIRIT** (audio safety) ‚Äî fill safety track knowledge
‚Üí **Recommend:** AudioSAE (#8) since SAE is foundational for Track 2 and evaluating grounding mechanistically

## Tags: #audio-language-model #logit-lens #interpretability #listen-vs-guess #track3 #audiolens #NTU #critical-layer #grounding
