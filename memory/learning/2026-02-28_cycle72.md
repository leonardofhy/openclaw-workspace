# ðŸ§  Cycle #72 â€” 2026-02-28 11:31
## Action: learn (synthesis â€” T-SAE â†’ Audio T-SAE as standalone paper idea)
## Context: Continuing from cycle #71 (T-SAE deep read). arXiv Feb 28 batch not yet posted (~14:00 Taipei). Must-read list complete. Execution-blocked on experiments. Ran arXiv search to confirm no "audio temporal SAE" paper exists. High-value action = develop T-SAE â†’ audio transfer hypothesis into a concrete paper idea.

## Content

### Confirmation: No Audio T-SAE paper exists
- arXiv query "temporal SAE audio speech features" â†’ 0 results
- arXiv query "sparse autoencoder whisper hubert speech features" â†’ only AudioSAE (2602.05027)
- Mariotte (2509.24793), AR&D (2602.22253), AudioSAE (2602.05027) = only 3 audio SAE papers
- T-SAE authors explicitly flag "other sequential modalities" as open direction â€” audio not yet done
- **Gap #17 confirmed: No temporally-structured SAE exists for speech/audio.**

### Deep Synthesis: Why Audio T-SAE â‰  Just AudioSAEBench Category 1

Cycle #71 identified T-SAE as a **metric source** for AudioSAEBench (TCS(F) metric). But after deeper thinking, there's a stronger contribution:

**Audio T-SAE = standalone paper, not just a benchmark metric.**

Here's the argument:

#### T-SAE's Core Architecture (recap from cycle #71)
- **Matryoshka partition**: 20% high-level features (global, semantic) + 80% low-level features (local, syntactic)
- **Temporal contrastive loss**: high-level features encouraged to be consistent across adjacent tokens
- **Result**: Self-supervised recovery of semantic structure without semantic labels

#### Why Audio is HARDER for Standard SAEs
AudioSAE (Aparin et al.) requires ~2000 features to erase a speech concept vs ~tens for text. This is partly because:
1. Speech has CONTINUOUS temporal structure â€” phonemes span 5-10 frames at 20ms â†’ adjacent frames share acoustic identity
2. Standard SAE treats each frame independently (i.i.d. assumption) â†’ features capture frame-level artifacts, not phoneme-level concepts
3. T-SAE's insight DIRECTLY APPLIES: if high-level features = phoneme identity / speaker identity / emotion (long-range) and low-level = frame-specific articulation â†’ the Matryoshka partition maps cleanly to audio hierarchies

#### Audio T-SAE Architecture Sketch
```
Whisper/HuBERT encoder â†’ hidden states [B Ã— T Ã— 768]
Split features: 
  - high-level (20% = 154 dims): expected to encode phoneme / syllable / word / speaker
  - low-level (80% = 614 dims): expected to encode frame-level pitch variation, VOT, formant
Contrastive pairs:
  - SHORT RANGE: (frame_t, frame_{t-1}) within same utterance = phoneme-level
  - LONG RANGE: (frame_i, frame_j) where |i-j| < phoneme_boundary = same phoneme
  - NEGATIVE: same position across different utterances
```

**Prediction**: High-level features should segment speech at phoneme boundaries â€” measured by TCS(F) = within-phoneme variance / across-phoneme variance. Audio T-SAE should dramatically outperform standard SAE on TCS.

#### Why This Is a Paper (Not Just a Baseline)

1. **First application of T-SAE to audio** â€” T-SAE = ICLR 2026 Oral, landmark paper. First extension = guaranteed audience.
2. **Stronger prior than text**: Audio has known temporal structure (phoneme boundaries from forced alignment, e.g., Montreal Forced Aligner) â†’ can EVALUATE the temporal structure recovery with ground truth, not just qualitative inspection.
3. **New methodology**: long-range contrastive variant (speaker identity consistent across entire utterance = contrastive at much longer range than adjacent tokens). Text T-SAE uses only adjacent; audio can use multi-scale (phoneme 5-10 frames, word ~50 frames, utterance-level).
4. **Connects to Triple Convergence**: if T-SAE's transition from low-level â†’ high-level features aligns with layer 3 (Whisper-base) / layer 6-7 (Whisper-large) = triple convergence IIT validation from a new angle.
5. **Unique metric (TCS)**: temporal coherence is audio-native â€” impossible to define for text without forced alignment.

### New Gap #17 (formalized)
**Audio Temporal SAE: Multi-scale Temporal Structure Exists in Speech but No SAE Exploits It**
- Standard audio SAEs are i.i.d. across frames â†’ recover frame-level artifacts, not phoneme-level concepts
- T-SAE (Bhalla et al., ICLR 2026 Oral) provides exact architecture to fix this
- Transfer to audio = first application, with stronger priors (phoneme boundary ground truth) than text
- Metric: TCS(F) = within-phoneme variance / across-phoneme variance â€” measurable, publishable
- Venue: INTERSPEECH 2026 (Speech track = natural fit) or ICASSP 2027

### Relationship to Existing Tracks
- This is a **new Track 2.5** (between AudioSAEBench and pure methods work):
  - If framed as "methodology paper": standalone "Audio T-SAE" paper â†’ then AudioSAEBench can benchmark it
  - If framed as "new metric for AudioSAEBench": TCS(F) in Category 1 = weaker but faster
  - If collaboration with T-SAE authors (Bhalla, Oesterling at Harvard/MIT): joint paper = higher visibility
- Connection to Paper A: Audio T-SAE's phoneme-level features could be used as the SAE basis for grounding_coefficient computation â†’ gc(F) on temporally coherent features = more meaningful metric

### Research Idea #7: Audio T-SAE
**Title candidate**: "Phoneme-Aware Sparse Autoencoders for Speech Models via Temporal Contrastive Learning"
**One-line**: Apply T-SAE's Matryoshka + multi-scale temporal contrastive loss to Whisper/HuBERT; use phoneme boundary ground truth (MFA) to evaluate; introduce TCS(F) metric.
**Key claim**: Audio's richer temporal structure makes T-SAE's approach MORE effective for speech than for text, enabling unsupervised phoneme-level feature discovery.
**Experiments**: Whisper-small + LibriSpeech + MFA â†’ train Audio T-SAE â†’ compare TCS(F) vs standard SAE â†’ compare phoneme probe accuracy â†’ compare on downstream: hallucination steering, grounding_coefficient
**Venue**: INTERSPEECH 2026 (March 5 = impossible; 2027) or ICASSP 2027
**Risk**: T-SAE authors could do this themselves; move fast.

### Meta-Awareness Note
This cycle illustrates the pattern: arXiv scan (cycle #70) â†’ T-SAE deep read (cycle #71) â†’ synthesis into new paper idea (cycle #72). Three cycles, progressive depth. The gap-to-idea pipeline is working well. 
However: idea count is now 7 (goals.md has 6) â€” added Paper Idea #7 here. Need to update goals.md.

## Next
- Update goals.md with Research Idea #7 (Audio T-SAE)
- arXiv Feb 28 batch at ~14:00 Taipei â†’ cycle #73 = learn
- Leo unblock still needed for IIT experiment + real speech test + venv setup

## Tags: #T-SAE #audio-SAE #temporal #phoneme #Gap17 #paper-idea-7 #synthesis
