# ðŸ§  Cycle #14 â€” 2026-02-26 21:30
## Action: build
## Context: goals.md + progress.md both recommended this; toolchain verified in #12; logit-lens is the direct empirical test of Triple Convergence Hypothesis; ~50 line extension of whisper_hook_demo.py

## Content

### Built: `skills/autodidact/scripts/whisper_logit_lens.py`
Full logit-lens implementation for Whisper encoder.

**Method:**
1. Register forward hooks on all N encoder layers
2. For each layer hidden state [T, d_model], apply ln_post normalization
3. Project through decoder token embedding matrix (transposed) â†’ [T, vocab_size]
4. Compute Layer Information Score (LIS) = cosine similarity of softmax distributions vs. final layer
5. Decode top-k tokens at each layer to observe semantic coherence

**Run results (Whisper-base, synthetic sine wave):**
```
Layer  0:  LIS=0.9027  Top-1='tek'
Layer  1:  LIS=0.8990  Top-1='tek'
Layer  2:  LIS=0.9036  Top-1='tek'
Layer  3:  LIS=0.9183  Top-1='<|ar|>'
Layer  4:  LIS=0.9388  Top-1='Operations'
Layer  5:  LIS=0.9493  Top-1='<|hu|>'
```

**Key findings:**

1. **LIS uniformly high** (0.90-0.95 from layer 0) â€” cosine similarity of vocabulary distributions is high even at shallow layers. Threshold 0.8 = too loose for this metric.

2. **LIS monotonically increases** (0.9027 â†’ 0.9493) â€” confirms progressive refinement, but the metric is compressed. Need recalibration for synthetic audio: try top-k KL divergence or use entropy instead.

3. **Token incoherence at early layers**: "tek", "mL", "factors" â€” random-ish tokens. Later layers: language ID tokens (`<|hu|>`, `<|ar|>`) dominate. This is expected for a sine wave (not speech).

4. **Saturation at layer 0 is spurious** â€” triggered by too-low threshold, not genuine semantic crystallization.

5. **Methodological insight**: The logit-lens metric depends heavily on the signal. **Real speech data needed** to observe the clean acousticâ†’semantic transition (confirmed by CKA showing layer 3 transition in Cycle #12).

### Script verified: âœ… syntax, import, forward pass, output all correct

## Next

**Cycle #15 recommendation: learn OR build**

Two paths:
- **learn**: Read "Activation patching best practices" (Heimersheim & Nanda) to inform patching design â€” needed before writing the pyvene patching code
- **build**: Download a real speech sample and re-run both scripts; add a better LIS metric (entropy, KL-div) that captures the transition more sharply

**Better path = learn** (Heimersheim & Nanda). Patching pitfalls are non-trivial; one paper prevents wasted experiments. Then patching code with good foundations.

## Tags: #build #logit-lens #triple-convergence #whisper #experiment #toolchain
