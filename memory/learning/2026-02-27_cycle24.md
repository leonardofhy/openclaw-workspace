# ðŸ§  Cycle #24 â€” 2026-02-27 02:00
## Action: learn (arXiv radar scan)
## Context: Must-read list exhausted after cycle #22. Expanding the frontier with fresh arXiv discovery. Night cycle â€” no high-value deep-read available from existing list, but discovery scan = background compounding. 3 new papers surfaced, all directly relevant to core research tracks.

## Content: 3 New Papers Discovered

### Paper A: "Discovering and Causally Validating Emotion-Sensitive Neurons in Large Audio-Language Models"
**arXiv:2601.03115** | Zhao, Schuller, Sisman | Jan 2026 | cs.CL + eess.AS | 16 pages

**The first causal neuron-level interpretability study of LALMs for emotion.**

- **Models**: Qwen2.5-Omni, Kimi-Audio, Audio Flamingo 3 (3 major open-source LALMs)
- **Method**: Compare 4 neuron selectors (frequency-, entropy-, magnitude-, contrast-based) â†’ identify emotion-sensitive neurons (ESNs) â†’ causal validation via inference-time interventions
- **Key Findings**:
  - **ESNs causally matter**: ablating emotion-class neurons disproportionately degrades recognition of *that specific emotion* while preserving other classes
  - **Gain-based amplification steers predictions** toward target emotion
  - **Non-uniform layer-wise clustering**: ESNs don't distribute uniformly â€” concentrated in specific layers
  - Partial cross-dataset transfer of ESNs (general emotion-specific units, not just task-overfitting)
  - Effects scale with intervention strength (clean causal dose-response)
- **Gap**: Neuron-level only (not SAE or circuit-level). No causal patching between audio/text. Models tested are 2025-era (after AudioLens); no grounding analysis.

**Connection to Leo's research:**
- **Track 3 (Listen vs Guess)**: ESNs exist in audio pathway, but are they from the audio stream or inferred from text context? Zhao et al. don't ask this question. A patching experiment (audio vs text causal contribution to ESN activation) would directly test the "listen vs guess" hypothesis at neuron level.
- **Track 5 (Safety)**: Neuron-level emotion steering = controlled affective behavior handle â†’ relevant to safety interventions
- **Causal AudioLens synergy**: They causally validate ESNs; AudioLens operationalizes grounding coefficient. Combining ESN discovery + patching = "what causes these neurons to fire?"
- **KEY GAP**: No comparison of audio-pathway vs text-pathway contribution to ESN activation â€” the "audio pathway" that feeds into ESNs is untested causally. This is exactly the patching experiment in Track 3.

---

### Paper B: "Sparse Autoencoders Make Audio Foundation Models more Explainable"
**arXiv:2509.24793** | Mariotte et al. | Sep 2025 (v2 Dec 2025) | ICASSP 2026 | cs.SD

**Second SAE paper on audio (after AudioSAE), focused on singing technique classification.**

- **Model**: General-purpose audio SSL model (not Whisper â€” different encoder)
- **Task**: Singing technique classification (case study, not ASR)
- **Key Findings**:
  - SAEs retain both task-relevant information AND class labels in their features
  - SAEs enhance **disentanglement of vocal attributes** â€” reveal underlying factors in representations
  - Confirms SAE as general tool for audio SSL explainability (not just ASR models)
- **Comparison to AudioSAE**: AudioSAE = full-layer, all-model, causal steering. Mariotte = single task (singing), focused on disentanglement
- **Gap**: No causal interventions (steering, ablation, patching). No cross-model comparison.

**Connection to Leo's research:**
- Confirms SAE as valid tool for audio SSL models beyond Whisper/HuBERT (AudioSAE already covers those)
- Disentanglement finding: SAE naturally separates pitch/timbre/technique â†’ directly relevant to Audio Benchmark (Track 1) if "clean/corrupt" = attribute-level disentanglement
- **2 audio SAE papers now exist** â€” Leo's Track 2 (AudioSAEBench) fills the missing evaluation/comparison layer between them

---

### Paper C: "What Do Neurons Listen To? A Neuron-level Dissection of a General-purpose Audio Model"
**arXiv:2602.15307** | Kawamura, Niizumi, Ono | Feb 16, 2026 | EUSIPCO 2026 | eess.AS + cs.SD

**First systematic neuron-level analysis of a general-purpose audio SSL model.**

- **Model**: General-purpose audio SSL model (BEATS or M2D â€” likely Niizumi's work)
- **Method**: Identify class-specific neurons via conditional activation patterns across diverse tasks
- **Key Findings**:
  - SSL models develop **class-specific neurons** with broad task coverage
  - Neurons exhibit **shared responses** across different semantic categories AND acoustic similarities (speech attributes + musical pitch)
  - These neurons have functional impact on classification (causal confirmation)
  - First systematic neuron-level analysis of a general-purpose audio SSL
- **Gap**: Neuron-level (not SAE). Speech vs music neurons not cleanly separated. No intervention at pathway level (audio stream vs context).

**Connection to Leo's research:**
- Complements AudioSAE (SAE features) + Mariotte (SAE for singing) with neuron-level view of general SSL
- "Shared responses across semantic + acoustic" = the polysemanticity problem in audio â†’ SAE would disentangle these
- Track 2 (AudioSAEBench): this paper + AudioSAE + Mariotte = 3 papers all probing same models from different angles (neuron/SAE/task) â†’ evaluation methodology needs to unify them

## Field Update: New Synthesis

**3 papers now do causal/mechanistic work on LALMs at neuron level:**
1. Zhao et al. 2601.03115 â€” emotion neurons in LALMs (causal ablation/amplification)
2. Kawamura et al. 2602.15307 â€” class-specific neurons in audio SSL (functional impact confirmed)
3. AudioSAE 2602.05027 â€” SAE-level features in Whisper encoder (steering, hallucination)

**None of them ask: "Do these neurons fire because of audio input or text context?"**

This is Track 3's precise contribution: **causal attribution** of neuron activation to audio vs text pathway. The mechanism (neuron, SAE feature, or circuit) is secondary â€” the *causal question* is new.

## Next: 
- Leo should deep-read Zhao et al. 2601.03115 when active (16 pages, high relevance to Track 3)
- Consider updating knowledge-graph.md with these 3 new papers
- Radar note: audio mech interp field is clearly accelerating (3 papers in 6 weeks = Feb 2026 surge)

## Tags: #arXiv-radar #causal-validation #emotion-neurons #SAE #audio-SSL #field-update #Track3 #Track2
