# ğŸ—ºï¸ Knowledge Graph

> æ¦‚å¿µã€è«–æ–‡ã€é€£çµã€‚Paper ideas è¦‹ goals.mdï¼ˆsingle source of truthï¼‰ã€‚
> Last updated: 2026-02-26 17:00 (cycle #7: AudioLens deep read)

## Mech Interp Ã— Speech/Audio â€” Field Map (2026)

### A) ASR / Whisper MI
- Ellena Reid (2023, LessWrong) â€” æ—©æœŸ Whisper MIï¼Œphoneme-like features, localized attention
- **Glazer et al. "Beyond Transcription" (2025, aiOla)** â€” ğŸŸ¢ DEEP READ â€” logit lens + probing + activation patching for Whisper-large-v3 + Qwen2-Audio [arXiv:2508.15882]
  - KEY METHODS: Encoder Lens (novel), saturation layer, white-noise reference patching
  - KEY FINDINGS: encoder encodes context (not just acoustics!), hallucination detectable from decoder residual stream (93.4% acc at layer 22), repetition = specific cross-attn heads
  - Speaker gender probing peaks layer 25 (94.6%), accent peaks layer 22 (97%), noise peaks layer 27 (90%)
  - DIRECT LINK to Track 3 (Listen vs Guess): saturation layer + patching sensitivity â†’ operationalize grounding coefficient
- Mozilla Builders (2024) â€” Whisper SAE (L1, TopK), phonetic/positional features
- Open tools: whisper-interp (GitHub), whisper_logit_lens (GitHub)

### B) Speech Encoder SAEs
- **AudioSAE (Aparin et al., 2026, EACL)** â€” ğŸŸ¢ DEEP READ â€” SAE on all 12 layers of Whisper/HuBERT [arXiv:2602.05027]
  - KEY SETUP: TopK/BatchTopK SAE, 8x expansion (768â†’6144 features), all-layer coverage
  - KEY FINDINGS: >50% feature stability across seeds; phoneme acc 0.92/0.89; **70% hallucination FPR reduction via top-100 feature steering** (Î±=1, WER cost only +0.4%)
  - LAYER INSIGHT: Whisper layer 6-7 = transition from audio-level â†’ frame-level speech encoding
  - SPEECH â‰  TEXT: erasing speech concepts needs ~2000 features; text SAE needs only ~tens â†’ phonetic info is distributed
  - EEG correlation: SAE features align with brain activity during speech perception (Pz electrode, 0-500ms lags)
  - KEY GAP: only encoder models; no LALMs; phonetic auto-interpretation failed (bad caption model)
  - CODE: https://github.com/audiosae/audiosae_demo
- Parra et al. (2025, EMNLP) â€” interpretable sparse features for SSL speech models
- SAE on speaker embeddings (Titanet) â€” monosemantic factors [arXiv:2502.00127]

### C) Audio-Language Modelsï¼ˆæœ€æ¥è¿‘ Leoï¼‰
- **ğŸ”¥ AudioLens (Neo Ho, Yi-Jyun Lee, Hung-yi Lee 2025, NTU â†’ ASRU 2025)** â€” ğŸŸ¢ DEEP READ â€” logit-lens on LALMs (DeSTA2, Qwen-Audio, Qwen2-Audio); auditory attribute perception [arXiv:2506.05140]
  - KEY METHODS: Layer-wise Information Score (= layer accuracy via vocab projection), Critical Layer (weighted avg layer index above threshold), 3 prompt formats
  - KEY FINDINGS:
    - Attribute info â‰  monotonic with depth; sharp drops+recoveries common
    - Success mode = info rises with depth; Failure mode = peaks mid-layer then drops
    - Earlier critical layer â†’ better accuracy (more layers to refine)
    - **LALMs query audio tokens directly >> aggregate at text positions** (= "listen not guess")
    - No-training improvement: enrich deep layers with early attribute-rich reps â†’ +16.3% acc
  - CRITICAL GAP: only Logit Lens (observational), NO causal patching â†’ cannot prove causal grounding
  - DIRECT LINK: operationalizes "Listen vs Guess" (Track 3); Leo can extend with causal interventions
  - NOTE: æ™ºå‡±å“¥ = Chih-Kai Yang (ckyang1124), GitHub: https://github.com/ckyang1124/AudioLens
  - CROSS-PAPER: critical layer â†” saturation layer (Beyond Transcription); potential unified framework
- Beyond Transcription ä¹Ÿæ¶µè“‹ Qwen2-Audio
- **ğŸŸ¢ SPIRIT (Djanibekov et al., EMNLP 2025, MBZUAI)** â€” ğŸŸ¢ DEEP READ â€” activation patching for audio jailbreak defense [arXiv:2505.13541]
  - KEY SETUP: PGD attack on Qwen2-Audio + LLaMa-Omni (both share Whisper encoder); AdvBench 246 samples
  - KEY FINDINGS: PGD achieves 100% ASR in some categories; activation patching (inject clean activations) reduces to ~1% with negligible utility cost; bias addition and neuron pruning also effective
  - BEST DEFENSE: patch at critical encoder-output/early-LM layers (found empirically, not mechanistically)
  - KEY GAP: no explanation of *where* adversarial signal lives; no SAE-guided patching
  - CODE: https://github.com/mbzuai-nlp/spirit-breaking
  - LEO'S OPPORTUNITY: AudioSAE features â†’ surgically suppress adversarial features vs SPIRIT's blind layer patching

### D) Generative Audio/Music MI
- SMITIN (2024), Facchiano (2025), TADA! (2026) â€” attention steering, SAE for music concepts
- TADA!: å°‘æ•¸ attention layers æ§åˆ¶ semantic concepts [arXiv:2602.11910]

### E) Brain-to-Speech
- Maghsoudi & Mishra (2026) â€” cross-mode patching, causal scrubbing [arXiv:2602.01247]

### F) Neural Audio Codecsï¼ˆæ–°è§’åº¦ï¼‰
- EnCodec â†’ discrete tokens â†’ è®“ audio MI è®Šæˆã€ŒLM-likeã€
- AudioLM, MusicGen/AudioGen éƒ½åŸºæ–¼ codec tokens
- MI æ„ç¾©ï¼štoken-level patching, SAE on residual stream ç›´æ¥å¯ç”¨
- ç›®å‰ MI ç ”ç©¶å¹¾ä¹ç©ºç™½

## æ ¸å¿ƒæ–¹æ³•å·¥å…·ç®±
â†’ è©³è¦‹ `skills/autodidact/references/toolbox.md`

## ğŸ”— Cross-Paper Connections (emerging picture)

| Concept A | Paper A | â†” | Concept B | Paper B | Insight |
|-----------|---------|---|-----------|---------|---------|
| Saturation layer (encoder) | Beyond Transcription | â†” | Critical layer (LALM) | AudioLens | Both = "where attribute resolves" â€” unify into shared framework? |
| Encoder encodes context | Beyond Transcription | â†” | LALMs query audio directly | AudioLens | Two views of same phenomenon: audio pathway carries semantic context |
| Patching shows causal grounding | Beyond Transcription | â†” | Logit Lens = only observational | AudioLens | **Gap = Leo's opportunity**: add causality to AudioLens framework |
| Hallucination in decoder residual | Beyond Transcription | â†” | Failure = mid-layer peak then drop | AudioLens | Same failure signature? Check if AudioLens failure cases = hallucinations |

### Research Opportunity Crystallized (2026-02-26)
> **"Causal AudioLens"**: Take AudioLens methodology (Logit Lens + critical layer) â†’ add patching experiments â†’ produce grounding_coefficient = ratio of (Î”acc when audio patched) / (Î”acc when text patched). This is the missing causal link in AudioLens, and it directly operationalizes Track 3 "Listen vs Guess" hypothesis.

### New Synthesis Insight â€” Three Papers, One Phenomenon (2026-02-26 Cycle #8)
> **Whisper layers 6-7 = semantic-acoustic transition zone**:
> - AudioSAE: audio-level speech peaks layer 6, then drops â†’ frame-level peaks layer 7 (phonetic encoding transition)
> - Beyond Transcription: "saturation layer" = where encoder commits to transcription
> - AudioLens: "critical layer" = where attribute resolves in LALM
> **Hypothesis**: All three independently found the same architectural transition point from different methodological angles. Testing this directly (SAE + saturation layer + critical layer on same model) = tractable experiment on MacBook.

### ğŸ§ª Experiment 0: Triple Convergence Test (Cycle #11 crystallized â€” 2026-02-26)

**Q:** Do AudioSAE layer 6-7 transition, Beyond Transcription saturation layer, and AudioLens critical layer point to the *same* architectural feature in Whisper?

**Setup (MacBook-feasible, Whisper-tiny or small):**
1. **Saturation layer**: Run Encoder Lens on Whisper encoder â€” find the layer where logit lens output stabilizes (= saturation layer from Beyond Transcription). Expected: ~layer 6-7 for small model.
2. **Norm/CKA jump**: Use `whisper_hook_demo.py` â€” look for the layer where CKA similarity to final layer jumps (= representation converges). Expected: ~layer 6-7.
3. **Feature stability**: If SAE trained: compare feature stability profile per layer (from AudioSAE paper, Fig. 3). Not immediately runnable without SAE training, but CKA can proxy it.
4. **Claim**: If all three methods point to the same transition zone â†’ strong evidence for a universal "semantic crystallization layer" in Whisper encoder.

**Minimal viable version (no SAE training needed):**
- `whisper_hook_demo.py` already captures layer norms + CKA
- Add: logit-lens decoder vocab projection at each layer (requires decoder embedding matrix)
- Result: saturation curve + CKA curve on same plot â†’ visual test of convergence hypothesis

**Impact if confirmed:**
- Novel empirical finding (all prior papers used different models/methods)
- Directly supports "Causal AudioLens" paper: "first experiment" section
- Conference-quality if extended to multiple models (Whisper variants + HuBERT)

**Next step:** Extend `whisper_hook_demo.py` to include logit-lens projection â†’ run â†’ see if CKA jump and saturation layer coincide. ~2-3 hours coding.

| Concept A | Paper A | â†” | Concept B | Paper B | New Connection |
|-----------|---------|---|-----------|---------|----------------|
| Layer 6-7 speech transition | AudioSAE | â†” | Saturation layer | Beyond Transcription | Same phenomenon? |
| Layer 6-7 frame-level encoding | AudioSAE | â†” | Critical layer | AudioLens | Three papers converge |
| Steering pipeline (suppress top-100) | AudioSAE | â†” | White-noise patching | Beyond Transcription | Causal intervention templates |
| Speech concepts = distributed (2000 feat) | AudioSAE | â†” | Encoder encodes context | Beyond Transcription | Distributed = context-sensitive |
| SAE feature steering (AudioSAE) | AudioSAE | â†” | Blind activation patching (SPIRIT) | SPIRIT | **Gap â†’ SAE-guided safety patching**: know WHICH features to suppress (not just which layers) |
| 70% hallucination FPR reduction | AudioSAE | â†” | 99% jailbreak defense | SPIRIT | Both use sparse activation intervention; sparse+interpretable (SAE) > dense (SPIRIT) |
| Triple Convergence layer 3 (Whisper-base) | whisper_hook_demo | â†” | Best defense = specific layer patching | SPIRIT | Does SPIRIT's optimal defense layer = Triple Convergence transition zone? |

### G) Activation Patching Methodology
- **Heimersheim & Nanda (2024)** â€” ğŸŸ¢ DEEP READ â€” "How to Use and Interpret Activation Patching" [arXiv:2404.15255]
  - KEY DISTINCTION: Denoising (cleanâ†’corrupt) tests SUFFICIENCY; Noising (corruptâ†’clean) tests NECESSITY â€” NOT symmetric!
  - AND circuits: use noising (finds all components); OR circuits: use denoising
  - METRICS hierarchy: logit diff > logprob > probability > accuracy (for exploratory patching)
  - âš ï¸ Gaussian noise patching (Causal Tracing) is fragile â€” sensitive to noise level, can be ineffective
  - âš ï¸ Backup behavior (Hydra effect): ablating key component activates backup â†’ component looks less important than it is
  - Path patching: isolates direct Aâ†’B connections, needed for confirmatory circuit verification
  - AUDIO IMPLICATION: Beyond Transcription's white-noise patching = suboptimal corruption; minimal pair audio = cleaner evidence
  - NEW GAP (Leo): all audio MI papers use suboptimal corruptions â€” minimal pairs would be methodologically cleaner and more publishable

---

## é—œéµç ”ç©¶è€…/åœ˜éšŠ
- **NTU æå®æ¯… lab** â€” AudioLens (æ™ºå‡±å“¥ï¼Leo ä¸»å ´)
- aiOla Research (Glazer) â€” ASR MI, hallucination causal analysis
- Huawei Noah's Ark (Aparin) â€” AudioSAE
- MBZUAI â€” SPIRIT (audio safety)
- Stanford (Atticus Geiger) â€” causal abstraction theory + pyvene
- Neel Nanda â€” activation patching best practices, TransformerLens
- Mozilla Builders â€” Whisper SAE tooling
- Ellena Reid â€” early Whisper MI (LessWrong)
- Yuan Gong (MIT) â€” AST/SSAST audio transformers
