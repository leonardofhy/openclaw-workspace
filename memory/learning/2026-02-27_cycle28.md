# ðŸ§  Cycle #28 â€” 2026-02-27 04:00
## Action: skill-up
## Context: Must-read list exhausted, real-speech test requires Leo. NNsight API check is purely autonomous and feeds directly into next build session. Also ran supplementary arXiv scan.

## Content: NNsight API Assessment vs pyvene

### What is NNsight?
- Library by NDIF (National Deep Inference Fabric) team
- Core design: **tracing context manager** â€” define interventions *declaratively*, execute as a batch
- Key class: `LanguageModel` (wraps any HuggingFace model)
- Syntax pattern: `with model.trace(inputs) as tracer: model.layer[3].output.save()`

### NNsight API â€” Key Patterns
```python
from nnsight import LanguageModel

model = LanguageModel("openai-community/gpt2", device_map="auto")

# --- GET activations ---
with model.trace("Hello world") as tracer:
    layer3_out = model.transformer.h[3].output[0].save()
# After context: layer3_out.value has tensor

# --- PATCH activations (cross-prompt intervention) ---
with model.trace(clean_input) as tracer:
    clean_act = model.transformer.h[3].output[0].save()

with model.trace(corrupted_input) as tracer:
    model.transformer.h[3].output[0][:] = clean_act.value  # patch!
    patched_logits = model.lm_head.output.save()
```

### NNsight for Whisper Encoder
- NNsight wraps ANY HuggingFace model (it's model-agnostic like pyvene)
- For Whisper: `LanguageModel("openai/whisper-base")`
- Access encoder layers via: `model.model.encoder.layers[3].output[0]`
- The tracing context handles the async execution graph

```python
from nnsight import LanguageModel
model = LanguageModel("openai/whisper-base")

# Get encoder layer activations
with model.trace({"input_features": audio_tensor}) as tracer:
    enc_layer3 = model.model.encoder.layers[3].output[0].save()
    enc_final = model.model.encoder.output.last_hidden_state.save()
```

### NNsight Features Relevant to Leo's Research
- âœ… Activation patching tutorial available (colab link in nnsight.net docs)
- âœ… Logit Lens tutorial available
- âœ… DAS (Distributed Alignment Search) tutorial available
- âœ… Dictionary Learning (SAE) tutorial available
- âœ… LoRA tutorial available
- âœ… Attribution patching (gradient-based) available
- âœ… **Remote execution via NDIF** â€” run large models on national cluster (no GPU needed!)
  - NDIF = National Deep Inference Fabric; free research access
  - This means Qwen2-Audio, Qwen2.5-Omni can be accessed without a GPU!!

### NNsight vs pyvene: Comparison

| Dimension | NNsight | pyvene | Winner |
|-----------|---------|--------|--------|
| Syntax style | Tracing context (declarative) | Wrapped model + config dict | NNsight (cleaner) |
| Audio encoder support | âœ… (any HF model) | âœ… (any PyTorch model) | Tie |
| Activation get | `.save()` in context | `run_with_hooks()` | NNsight (cleaner) |
| Activation patch | assign in context | `IntervenableModel` + config | NNsight (simpler) |
| Cross-prompt patching | Built-in `.save()` across traces | Requires manual management | NNsight |
| DAS support | âœ… built-in tutorial | âœ… built-in (original source) | pyvene (more mature) |
| SAE/dict learning | âœ… tutorial available | âŒ separate library needed | NNsight |
| Large model access | âœ… **NDIF remote** (Qwen2-Audio!!) | âŒ local only | **NNsight wins** |
| Behind-the-Scenes (Ma 2026) | âœ… Used for Whisper SER MI | â€” | NNsight |
| Documentation | Good, tutorial-heavy | Good, colab-heavy | Tie |
| Installation | `pip install nnsight` | `pip install pyvene` | Not installed locally yet |

### Verdict: NNsight > pyvene for Leo's use case

**Primary reason: NDIF remote access**
- NDIF allows running interventions on large models (Qwen2-Audio-7B, etc.) on a remote cluster
- This is critical for Track 3 (Listen vs Guess) which needs LALM experiments
- pyvene is local-only â†’ requires GPU for 7B models
- NNsight local: MacBook-feasible models (Whisper-base/small) same as pyvene

**Secondary reason: cleaner syntax for patching**
- NNsight's tracing context is more readable than pyvene's config dict
- "Behind the Scenes" (Ma et al.) already validated it works on Whisper encoder

### Action Required
- Install: `pip install nnsight` (in venv, since macOS externally-managed)
- Leo needs to: (1) create a venv for audio MI experiments, (2) install nnsight + pyvene both
- Priority: migrate existing whisper_hook_demo.py and whisper_logit_lens.py to NNsight API

### arXiv Supplementary Scan (4 AM check)
- Confirmed: no new speech MI papers since cycle #24 (yesterday)
- The 4 known papers from "speech mechanistic interpretability" query = already all read
- "audio language model interpretability causal" query: Zhao 2601.03115 = already read
- New mention: "Speech World Model" (Zhou et al., Dec 2025) â€” causal state-action planning for speech; different research angle (planning, not MI) â†’ low relevance, skip

## Next: 
- Day 2 morning (when Leo wakes up): 
  1. Real speech test (whisper_hook_demo.py with actual .wav â€” needs Leo's audio file or download one)
  2. Create venv for audio MI; install nnsight
  3. Update cheat sheet with NNsight API patterns (see above)
  4. Present "Class-specific Neuron Grounding" experiment sketch â†’ wait for Leo approval

## Tags: #nnsight #pyvene #toolchain #skill-up #ndif #remote-execution
