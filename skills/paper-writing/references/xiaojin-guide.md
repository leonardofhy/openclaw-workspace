# å­¸ç”Ÿè«–æ–‡å¯«ä½œæ•™å­¸æŒ‡å—

## ç”±å°é‡‘æ•´ç†ï¼Œä¾›å®æ¯…è€å¸«æŒ‡å°å­¸ç”Ÿç”¨

### å»ºç«‹æ—¥æœŸï¼š2026-02-15

---

## ğŸ¯ æ•™å­¸ç›®æ¨™

è®“å­¸ç”Ÿèƒ½å¯«å‡º Interspeech/ACL æ°´æº–çš„è«–æ–‡ï¼Œé‡é»åŸ¹é¤Šï¼š

1. çµæ§‹åŒ–æ€è€ƒèƒ½åŠ›ï¼ˆå…ˆæƒ³æ¸…æ¥šå†å¯«ï¼‰  
2. ç‚ºè®€è€…å¯«ä½œçš„æ„è­˜  
3. æ¯å€‹ section çš„å…·é«”å¯«ä½œæŠ€å·§

---

## ç¬¬ä¸€èª²ï¼šè«–æ–‡çš„æœ¬è³ª

### æ ¸å¿ƒè§€å¿µï¼ˆä¾†è‡ªå››å¤§å¯«ä½œå°å¸«ï¼‰

**è«–æ–‡æ˜¯ä½ çš„å¤§ä½¿** â€” ä½ ä¸åœ¨å ´æ™‚ï¼Œå®ƒæ›¿ä½ èªªè©±ã€‚ï¼ˆSPJï¼‰ **å¯«ä½œ \= æ€è€ƒ** â€” å¯«ä¸æ¸…æ¥šä»£è¡¨ä½ æ²’æƒ³æ¸…æ¥šã€‚ï¼ˆSPJ \+ Ernstï¼‰ **ç›®æ¨™æ˜¯æ”¹è®Šè®€è€…è¡Œç‚º** â€” è®“ä»–å€‘ç›¸ä¿¡ä½ çš„æ–¹æ³•å€¼å¾—å˜—è©¦ã€‚ï¼ˆErnstï¼‰

### Ernst çš„ä¸‰ä»¶äº‹æ¡†æ¶ï¼ˆæœ€ç°¡æ½”çš„è‡ªæˆ‘æª¢æŸ¥ï¼‰

è®€è€…å¿…é ˆç›¸ä¿¡ï¼š

1. âœ… **å•é¡Œå¾ˆé‡è¦**ï¼ˆæœ‰å½±éŸ¿åŠ›ã€æœ‰å¾Œæœï¼‰  
2. âœ… **å•é¡Œå¾ˆé›£**ï¼ˆç¾æœ‰æ–¹æ³•éƒ½ä¸å¤ å¥½ï¼‰  
3. âœ… **ä½ è§£æ±ºäº†é€™å€‹å•é¡Œ**ï¼ˆæœ‰å¯¦é©—è­‰æ˜ï¼‰

ğŸ’¡ **æ•™å­¸å»ºè­°**ï¼šè®“å­¸ç”Ÿåœ¨å¯«è«–æ–‡å‰å…ˆç”¨ä¸‰å¥è©±å›ç­”é€™ä¸‰å€‹å•é¡Œã€‚å¦‚æœç­”ä¸å‡ºä¾†ï¼Œé‚„æ²’æº–å‚™å¥½å¯«ã€‚

---

## ç¬¬äºŒèª²ï¼šAbstract æ€éº¼å¯«

### å…¬å¼ï¼šå•é¡Œ â†’ ä¸è¶³ â†’ æ–¹æ³• â†’ çµæœï¼ˆ4-6 å¥ï¼‰

| å¥å­ | åŠŸèƒ½ | ä¿¡è™Ÿè© |
| :---- | :---- | :---- |
| ç¬¬ 1-2 å¥ | èƒŒæ™¯ \+ ç¾æœ‰æ–¹æ³• | "X methods aim to..." |
| ç¬¬ 3 å¥ | Gapï¼ˆä¸è¶³ï¼‰ | "However," "Nevertheless," "Despite," |
| ç¬¬ 4 å¥ | ä½ çš„æ–¹æ³• | "We propose X, which..." / "In this work, we..." |
| ç¬¬ 5 å¥ | çµæœ | "Experiments show..." \+ å…·é«”æ•¸å­— |
| ç¬¬ 6 å¥ï¼ˆé¸å¡«ï¼‰ | é¡å¤–äº®é» | "Notably," "Furthermore," |

### å¥½ç¯„ä¾‹åˆ†æï¼šEFFUSE (Interspeech 2024 Best Paper)

\[èƒŒæ™¯\] SSL models have demonstrated exceptional performance in various speech tasks.

\[è¶¨å‹¢\] Recent works show that fusing diverse SSL models could achieve superior performance.

\[Gap\] However, fusing models increases the overall parameter size, leading to higher computational costs.

\[æ–¹æ³•\] We propose EFFUSE, ...uses a single SSL model to mimic the features of multiple SSL models via prediction.

\[çµæœ\] Our best model achieves an average SUPERB score increase of 63.5 (6.3%)...

      while decreasing parameter size by 317M (49%).

**ç‚ºä»€éº¼å¥½ï¼Ÿ** ä¸‰æ®µè«–æ³•ï¼ˆå¥½â†’æ›´å¥½â†’ä½†è²´â†’æˆ‘å€‘çš„è§£æ³•ï¼‰ï¼›çµæœåŒæ™‚å ± absolute \+ relativeã€‚

### å¸¸è¦‹éŒ¯èª¤

- âŒ å¤ªé•·ï¼ˆ\>250 å­—ï¼‰â†’ æ‡‰ 150-200 å­—  
- âŒ æ²’æœ‰å…·é«”æ•¸å­—  
- âŒ æ”¾ referencesï¼ˆabstract è¦èƒ½ç¨ç«‹å­˜åœ¨ï¼‰  
- âŒ èªª "In this paper"ï¼ˆSchulzrinneï¼šé‚„èƒ½æ˜¯ä»€éº¼ paperï¼Ÿï¼‰  
- âŒ åªèªª "significant improvement" ä¸çµ¦æ•¸å­—

ğŸ’¡ **ç·´ç¿’**ï¼šæ‰¾ä¸‰ç¯‡ Interspeech best paperï¼Œæ¨™è¨˜æ¯å¥è©±çš„åŠŸèƒ½ï¼ˆèƒŒæ™¯/gap/æ–¹æ³•/çµæœï¼‰ã€‚

---

## ç¬¬ä¸‰èª²ï¼šIntroduction æ€éº¼å¯«

### çµæ§‹ï¼šå€’ä¸‰è§’ \+ Contributions

**ç¬¬ 1 æ®µï¼šå¤§èƒŒæ™¯**ï¼ˆç‚ºä»€éº¼é€™å€‹é ˜åŸŸé‡è¦ï¼‰

- 1-2 å¥å³å¯ï¼Œä¸è¦å¯«å¤ªå¤š general motivation

**ç¬¬ 2-3 æ®µï¼šå»ºç«‹ Gap**

- æè¿°ç¾æœ‰æ–¹æ³•ï¼ˆcite ç›¸é—œå·¥ä½œï¼‰  
- ç”¨è½‰æŠ˜è©æŒ‡å‡ºä¸è¶³ï¼šHowever, Despite, Unfortunately  
- Gap è¦å…·é«”åˆ°è®“è®€è€…è¦ºå¾—ã€Œå°ï¼Œé€™ç¢ºå¯¦æ˜¯å•é¡Œã€

**ç¬¬ 4 æ®µï¼šä½ çš„æ–¹æ³•ï¼ˆoverviewï¼‰**

- "In this work, we propose X, which..."  
- çµ¦ high-level intuitionï¼Œä¸è¦é€²å…¥æŠ€è¡“ç´°ç¯€

**ç¬¬ 5 æ®µï¼ˆæˆ–ç¬¬ 4 æ®µæœ«ï¼‰ï¼šContributions**

- ç”¨ bullet points æ˜ç¢ºåˆ—å‡º  
- æ¯å€‹ contribution è¦å…·é«”ã€å¯é©—è­‰  
- âŒ "We propose a novel method" â†’ âœ… "We propose X that achieves Y by doing Z"

### é—œéµæŠ€å·§

- **Gap æ±ºå®šè«–æ–‡åƒ¹å€¼**ï¼šGap ä¸ convincing \= è«–æ–‡ä¸ convincing  
- **ä¸è¦åœ¨ Introduction å¯«å¤ªå¤š related work**ï¼ˆSPJ \#5ï¼‰  
- **Introduction çµå°¾ \= è®€è€…çš„ mental map**ï¼šå‘Šè¨´ä»–å€‘æ¥ä¸‹ä¾†æœƒçœ‹åˆ°ä»€éº¼

ğŸ’¡ **ç·´ç¿’**ï¼šè®“å­¸ç”Ÿåªå¯« Introduction çš„æœ€å¾Œä¸€æ®µï¼ˆcontributionsï¼‰ï¼Œç„¶å¾Œäº’ç›¸è©•ã€‚

---

## ç¬¬å››èª²ï¼šMethod æ€éº¼å¯«

### ä¸‰å±¤çµæ§‹

1. **Problem Formulation**ï¼šç”¨æ•¸å­¸æˆ–æ¸…æ™°èªè¨€å®šç¾©å•é¡Œ  
2. **Key Insight**ï¼šç‚ºä»€éº¼ä½ çš„æ–¹æ³•èƒ½è§£æ±ºé€™å€‹å•é¡Œï¼Ÿï¼ˆ**æœ€å®¹æ˜“è¢«è·³éï¼**ï¼‰  
3. **Technical Details**ï¼šå…·é«”å¯¦ç¾

### DiffATR ç¤ºç¯„

- Layer 1: å®šç¾© discriminative (p(y|x)) vs generative (p(x,y))  
- Layer 2: "å¦‚æœæˆ‘å€‘æŠŠ retrieval é‡æ–°å»ºæ¨¡ç‚º joint distributionï¼Œå°±è‡ªç„¶è€ƒæ…®äº† data distribution"  
- Layer 3: Diffusion processã€loss functionã€training procedure

### å¸¸è¦‹éŒ¯èª¤

- âŒ åªå¯« **how**ï¼Œä¸å¯« **why**ï¼ˆå­¸ç”Ÿæœ€å¸¸çŠ¯ï¼ï¼‰  
- âŒ è·³é intuition ç›´æ¥é€² formalism  
- âŒ æ²’æœ‰ pipeline åœ–ç¤º  
- âŒ æ•¸å­¸ç¬¦è™Ÿä¸ä¸€è‡´

ğŸ’¡ **æ•™å­¸å»ºè­°**ï¼šè®“å­¸ç”Ÿå¯«å®Œ Method å¾Œï¼Œå•ä»–å€‘ï¼šã€Œå¦‚æœå»æ‰æ‰€æœ‰æ•¸å­¸ï¼Œç”¨ä¸‰å¥è©±è§£é‡‹ä½ çš„æ–¹æ³•ï¼Œä½ æœƒæ€éº¼èªªï¼Ÿã€é€™ä¸‰å¥è©±å°±æ˜¯ Key Insightã€‚

---

## ç¬¬äº”èª²ï¼šExperiments æ€éº¼å¯«

### å¿…å‚™å…ƒç´ 

1. **Setup**ï¼šæ•¸æ“šé›†ã€baselineã€è©•ä¼°æŒ‡æ¨™ã€å¯¦ç¾ç´°ç¯€  
2. **Main Results**ï¼šèˆ‡ baseline çš„æ¯”è¼ƒï¼ˆtable \+ æ–‡å­—è§£è®€ï¼‰  
3. **Ablation Study**ï¼šæ¯å€‹ component çš„è²¢ç»  
4. **Analysis**ï¼šerror analysisã€case studyã€visualization

### å¯«ä½œæŠ€å·§

- æ¯å€‹ table/figure éƒ½è¦æœ‰æ–‡å­—è§£è®€  
  - âŒ "As shown in Table 1"  
  - âœ… "Table 1 shows that X outperforms Y by 3.2% on WER, suggesting that \[insight\]"  
- Bold æœ€å¥½çš„çµæœ  
- å ±å‘Š mean Â± stdï¼ˆå°¤å…¶æ˜¯å·®è·å°çš„æ™‚å€™ï¼‰  
- ä¸»å‹•è¨è«–è² é¢çµæœï¼ˆä½ çš„æ–¹æ³•åœ¨å“ªè£¡ä¸è¡Œï¼Ÿç‚ºä»€éº¼ï¼Ÿï¼‰

### Ablation çš„é‚è¼¯

- é€ä¸€å»æ‰ componentï¼Œçœ‹ performance drop  
- è®“è®€è€…çŸ¥é“ biggest drop åœ¨å“ªè£¡ \= æœ€é‡è¦çš„ component  
- å¦‚æœæŸå€‹ component å»æ‰å¾Œæ²’å·®ï¼Œè¦è§£é‡‹ï¼ˆæˆ–è€ƒæ…®ç§»é™¤ï¼‰

ğŸ’¡ **ç·´ç¿’**ï¼šçµ¦å­¸ç”Ÿä¸€å¼µ results tableï¼Œè®“ä»–å€‘å¯«ä¸€æ®µè§£è®€ã€‚ç„¶å¾Œæ¯”è¼ƒèª°çš„è§£è®€æœ€æœ‰ insightã€‚

---

## ç¬¬å…­èª²ï¼šRelated Work æ€éº¼å¯«

### æ ¸å¿ƒåŸå‰‡

- **ä¸æ˜¯ listï¼Œæ˜¯å°æ¯”**  
- è¦è§£é‡‹ä½ çš„æ–¹æ³•èˆ‡æ¯é¡ related work çš„å·®ç•°  
- è¦ generousï¼ˆæ‰¿èªä»–äººè²¢ç»ï¼‰ä½† clearï¼ˆèªªæ˜å·®ç•°ï¼‰

### çµæ§‹å»ºè­°

- æŒ‰ä¸»é¡Œåˆ†çµ„ï¼ˆä¸æ˜¯æŒ‰æ™‚é–“ï¼‰  
- æ¯çµ„ 2-3 æ®µï¼Œæœ€å¾Œä¸€å¥é»å‡ºèˆ‡ä½ çš„æ–¹æ³•çš„å·®ç•°  
- âŒ "X et al. proposed..." æµæ°´å¸³  
- âœ… "While X approaches this problem from \[angle\], our work differs in \[aspect\]"

---

## ç¬¬ä¸ƒèª²ï¼šå¯«ä½œé¢¨æ ¼

### ä¾†è‡ªå››å¤§å°å¸«çš„å…±åŒå»ºè­°

1. **æ¯æ®µç¬¬ä¸€å¥ \= topic sentence**ï¼ˆreviewer å¸¸åªè®€ç¬¬ä¸€å¥ï¼‰  
2. **ç”¨ active voice**ï¼š"We train the model" è€Œé "The model is trained"  
3. **é¿å… puffery words**ï¼šnovel, clearly, obviously, significantï¼ˆç„¡æ•¸æ“šæ”¯æŒçš„ï¼‰  
4. **å…¨ç¯‡è¡“èªä¸€è‡´**ï¼šä¸€å€‹æ¦‚å¿µåªç”¨ä¸€å€‹è©  
5. **å…ˆèªªçµè«–ï¼Œå†çµ¦è­‰æ“š**ï¼ˆgive away the punchlineï¼‰  
6. **Ruthlessly cut**ï¼šä¸æ”¯æŒä¸»è«–é»çš„å°±åˆª

### éæ¸¡è©é€ŸæŸ¥è¡¨

| åŠŸèƒ½ | è©å½™ |
| :---- | :---- |
| è½‰æŠ˜ | However, Nevertheless, Despite, Yet, Unfortunately |
| å› æ­¤ | Therefore, Consequently, As a result, Thus |
| æ­¤å¤– | Furthermore, Moreover, Additionally, In addition |
| å…·é«”ä¾†èªª | Specifically, In particular, Concretely |
| å°æ¯” | In contrast, Unlike, While, Whereas |

---

## ç¬¬å…«èª²ï¼šæäº¤å‰ Checklistï¼ˆç²¾ç°¡ç‰ˆï¼‰

### å¿…æŸ¥é …ç›®

- [ ] Abstract æœ‰å…·é«”æ•¸å­—å—ï¼Ÿ  
- [ ] Introduction æœ‰æ˜ç¢ºçš„ contributionsï¼ˆbullet pointsï¼‰å—ï¼Ÿ  
- [ ] Method æœ‰è§£é‡‹ **why** è€Œä¸åªæ˜¯ **how** å—ï¼Ÿ  
- [ ] æ¯å€‹ table/figure éƒ½æœ‰æ–‡å­—è§£è®€å—ï¼Ÿ  
- [ ] æœ‰ ablation study å—ï¼Ÿ  
- [ ] Related work æ˜¯å°æ¯”è€Œé list å—ï¼Ÿ  
- [ ] å…¨æ–‡æ²’æœ‰ "novel"ã€"clearly"ã€"obviously" ç­‰ pufferyï¼Ÿ  
- [ ] æ ¼å¼ç¬¦åˆç›®æ¨™æœƒè­°è¦æ±‚ï¼Ÿ  
- [ ] é æ•¸æ²’è¶…éé™åˆ¶ï¼Ÿ  
- [ ] æœ‰ Limitations sectionï¼Ÿï¼ˆACL ç³»åˆ—å¿…å‚™ï¼‰

---

## ç¬¬ä¹èª²ï¼šå¾å¯©ç¨¿è§’åº¦åçœ‹å¯«ä½œ

### Reviewer çœ‹ä»€éº¼ï¼Ÿï¼ˆACL Rolling Review äº”å¤§ç¶­åº¦ï¼‰

1. **Soundness**ï¼šä½ çš„ claims æœ‰è¶³å¤ çš„ evidence æ”¯æŒå—ï¼Ÿ  
2. **Excitement**ï¼šé€™å€‹å·¥ä½œæœ‰å¤šä»¤äººèˆˆå¥®ï¼Ÿå¤šæœ‰å½±éŸ¿åŠ›ï¼Ÿ  
3. **Reproducibility**ï¼šåˆ¥äººèƒ½é‡ç¾ä½ çš„çµæœå—ï¼Ÿ  
4. **Overall**ï¼šè©²æ¥å—å—ï¼Ÿ

### å¸¸è¦‹æ‹’ç¨¿åŸå› ï¼ˆåŠå°æ‡‰å¯«ä½œç­–ç•¥ï¼‰

| æ‹’ç¨¿åŸå›  | å¯«ä½œå°ç­– |
| :---- | :---- |
| Novelty ä¸è¶³ | Introduction æ¸…æ¥šå»ºç«‹ gapï¼›explicitly å°æ¯” prior work |
| å¯¦é©—ä¸å……åˆ† | å¤š datasetã€ablationã€error analysis |
| å¯«ä½œä¸æ¸…æ¥š | Topic sentencesã€pipeline åœ–ã€consistent terminology |
| Motivation ä¸ convincing | ç”¨å…·é«”ä¾‹å­èªªæ˜å•é¡Œçš„åš´é‡æ€§ |
| Overclaiming | ç”¨ "our results suggest" è€Œé "our method proves" |

---

## Interspeech 4 é ç©ºé–“åˆ†é…æŒ‡å—

### ç©ºé–“é ç®—è¡¨

| Section | ä½”æ¯” | å­—æ•¸ | å‚™è¨» |
| :---- | :---- | :---- | :---- |
| Abstract | å›ºå®š | \~150 å­— | 5 å¥å…¬å¼ |
| Introduction | 25-30% | 800-950 | **æœ€é‡è¦**ï¼Œå« gap \+ contribution |
| Related Work | 10-15% | 300-450 | å¯èå…¥ Intro |
| Method | 25-30% | 800-950 | é‡ insight ä¸é‡ detail |
| Experiments | 25-30% | 800-950 | Setup \+ Results \+ Ablation |
| Conclusion | 5-8% | 150-250 | 3-4 å¥å³å¯ |

### ç‰ˆé¢æ¨¡æ¿

Page 1: Title \+ Abstract \+ Intro(å‰åŠ) \+ Figure 1

Page 2: Intro(å¾ŒåŠ) \+ Related Work \+ Method(å‰åŠ)

Page 3: Method(å¾ŒåŠ) \+ Exp Setup \+ Main Results(Table 1\)

Page 4: Ablation(Table 2\) \+ Analysis \+ Conclusion

Page 5: References only

### ä¸‰å€‹éµå¾‹

1. **Figure 1 å¿…é ˆåœ¨ç¬¬ä¸€é ** â€” reviewer 30 ç§’å…§è¦çœ‹æ‡‚ä½ çš„æ–¹æ³•  
2. **å¯§å¯ç  Method æ–‡å­—ï¼Œä¸å¯ç  Figure 1 å’Œ Results Table**  
3. **Related Work ä¸è¶…éåŠæ¬„** â€” èå…¥ Intro çš„ gap-building æ›´å¥½

### ç©ºé–“ä¸å¤ æ™‚çš„ç ç¨¿é †åº

1. ç  Related Workï¼ˆèå…¥ Introï¼‰  
2. ç  Training Detailsï¼ˆç§»åˆ° footnoteï¼‰  
3. ç  Baseline Descriptionsï¼ˆè®“ table è‡ªæ˜ï¼‰  
4. ç  Conclusionï¼ˆå£“åˆ° 3 å¥ï¼‰  
5. âŒ çµ•ä¸ç ï¼šIntro gapã€Main Resultsã€Figure 1

### Reviewer æ³¨æ„åŠ›åˆ†é…ï¼ˆæŒ‰æ­¤é †åºå„ªåŒ–ï¼‰

Abstract(30s) â†’ Figure 1(30s) â†’ Results Table(60s) â†’ Intro(2min) â†’ Method(2min)

---

## ğŸ“Š å¯¦é©—çµæœå‘ˆç¾ï¼ˆExperiments Sectionï¼‰

### MOS å¿…å‚™æ¸…å–®

- [ ] è©•åˆ†äººæ•¸ â‰¥ 15ï¼ˆå¯«é€²æ–‡ä¸­ï¼‰  
- [ ] 95% CI æˆ– error barï¼ˆä¸èƒ½åªå ±å¹³å‡ï¼‰  
- [ ] è©•åˆ†é‡è¡¨èªªæ˜ï¼ˆå¼•ç”¨ ITU-T P.800 æˆ–è‡ªè¿°ï¼‰  
- [ ] è©•å¯©è€…ä¾†æºï¼ˆMTurk/lab/Prolificï¼‰åŠ screening æ–¹å¼  
- [ ] æ¯ç³»çµ±æ¸¬è©¦å¥æ•¸ï¼ˆé€šå¸¸ 20-50ï¼‰  
- [ ] Ground truth MOSï¼ˆä¸Šç•Œæ ¡æº–ï¼‰  
- [ ] çµ±è¨ˆé¡¯è‘—æ€§æª¢é©—ï¼ˆWilcoxon / Mann-Whitneyï¼Œæ¨™ \*p\<0.05ï¼‰

### WER/CER å¿…å‚™æ¸…å–®

- [ ] â‰¥ 2 å€‹ test setï¼ˆå« in-domain \+ out-of-domainï¼‰  
- [ ] Relative improvement %ï¼ˆä¸åªå ± absoluteï¼‰  
- [ ] ç²—é«”æ¨™ç¤ºæœ€ä½³çµæœ  
- [ ] èªªæ˜ LMã€beam size ç­‰å½±éŸ¿çµæœçš„è¨­å®š  
- [ ] çµ±è¨ˆé¡¯è‘—æ€§ï¼ˆpaired bootstrap / MAPSSWEï¼‰

### Ablation Study è¨­è¨ˆ

- æ¯è¡Œç§»é™¤ä¸€å€‹ componentï¼ˆone-at-a-timeï¼‰  
- 3-6 è¡Œç‚ºä½³ï¼ˆå¤ªå°‘ä¸ convincingï¼Œå¤ªå¤šä½”ç©ºé–“ï¼‰  
- åŠ  Î” æ¬„ä½é¡¯ç¤ºè²¢ç»é‡  
- å¥½çš„ ablation å›ç­”ã€Œç‚ºä»€éº¼æœ‰æ•ˆã€ï¼Œä¸åªæ˜¯ã€Œæ¯å€‹éƒ½æœ‰ç”¨ã€  
- è¨­è¨ˆ ablation æ™‚å›ç­”ï¼šä½  propose ä»€éº¼ï¼Œå°± ablate ä»€éº¼

### åœ–è¡¨ 10 æ¢è¦å‰‡

1. Table caption ä¸Šæ–¹ã€Figure caption ä¸‹æ–¹  
2. æ¯å€‹åœ–è¡¨éƒ½è¦åœ¨æ­£æ–‡ reference  
3. åœ–ä¸­å­—é«” â‰¥ æ­£æ–‡ 80%  
4. ç”¨å‘é‡åœ–ï¼ˆPDFï¼‰ï¼Œspectrogram é™¤å¤–  
5. Colorblind-friendly é…è‰²  
6. Bar chart y-axis å¾é›¶é–‹å§‹ï¼ˆé™¤éæ¨™ç¤º breakï¼‰  
7. Spectrogram æ¨™ axis labelï¼ˆTime, Freq, dBï¼‰  
8. MOS bar chart é™„ error bar  
9. ç”¨ â†“â†‘ æ¨™ç¤º metric æ–¹å‘ï¼ˆWERâ†“ã€PESQâ†‘ï¼‰  
10. Radar chart é©åˆå¤šç¶­åº¦æ¯”è¼ƒ

### Baseline é¸æ“‡åŸå‰‡

- å¿…å«ï¼š(1) ç¶“å…¸æ–¹æ³• (2) ç•¶å‰ SOTA (3) ä½ æ–¹æ³•çš„ç°¡åŒ–ç‰ˆ  
- åªè·Ÿè‡ªå·±èˆŠæ–¹æ³•æ¯” \= contribution ä¸æ˜ç¢º  
- Speech å¸¸ç”¨ baselineï¼š  
  - TTS: Tacotron2, VITS, FastSpeech2, NaturalSpeech  
  - ASR: Whisper, wav2vec2.0, HuBERT, Conformer-T  
  - SE: DCCRN, MetricGAN+, CMGAN  
  - VC: AutoVC, VQVC+, kNN-VC

---

## ç¬¬åèª²ï¼šBefore/After æ”¹å¯«ç¯„ä¾‹é›†

ğŸ’¡ **æ•™å­¸å»ºè­°**ï¼šé€™äº›ç¯„ä¾‹å¯ç›´æ¥ç”¨æ–¼èª²å ‚æ•™å­¸ã€‚è®“å­¸ç”Ÿå…ˆåˆ¤æ–· Before ç‰ˆæœ‰ä»€éº¼å•é¡Œï¼Œå†çœ‹ After ç‰ˆï¼Œæœ€å¾Œè®“ä»–å€‘ç”¨åŒæ¨£åŸå‰‡æ”¹è‡ªå·±çš„ç¨¿å­ã€‚

### A. Abstract æ”¹å¯«

**Before âŒ**

In this paper, we propose a novel method for speech enhancement. Our method uses a deep learning model to process noisy speech signals. We conduct extensive experiments on various datasets. The experimental results demonstrate that our proposed method significantly outperforms existing methods and achieves state-of-the-art performance.

**Problems**: "In this paper" å†—é¤˜ã€"novel" ç©ºæ´ pufferyã€"significantly" ç„¡æ•¸æ“šæ”¯æŒã€ç„¡ gapã€ç„¡å…·é«”æ•¸å­—ã€ç„¡ insightã€‚

**After âœ…**

Diffusion-based speech enhancement methods have achieved strong denoising performance, but their iterative sampling process incurs high latency, limiting real-time deployment. We propose FastDiff-SE, which replaces the standard reverse diffusion with a one-step consistency distillation, reducing inference time by 50Ã— while preserving enhancement quality. On VoiceBank-DEMAND, FastDiff-SE achieves 3.42 PESQ and 0.95 STOI with a real-time factor of 0.03, outperforming DCCRN (3.27 PESQ) and matching CMGAN (3.41 PESQ) at 1/20th the computational cost.

**Why better**: æœ‰å…·é«” gapï¼ˆlatencyï¼‰ã€æœ‰ insightï¼ˆconsistency distillationï¼‰ã€æœ‰æ•¸å­—ï¼ˆPESQã€RTFï¼‰ã€æœ‰ baseline å°æ¯”ã€‚

---

### B. Introduction Gap æ”¹å¯«

**Before âŒ**

Many methods have been proposed for text-to-speech synthesis. However, there are still some problems. Therefore, we propose our method to address these issues.

**Problems**: Gap å®Œå…¨ä¸å…·é«”â€”â€”ä»€éº¼ problemsï¼Ÿå“ªäº› issuesï¼ŸReviewer çœ‹å®Œä¸çŸ¥é“ä½ è¦è§£ä»€éº¼ã€‚

**After âœ…**

Recent zero-shot TTS systems (VALL-E, NaturalSpeech 2\) achieve impressive speaker similarity from a 3-second prompt, but they rely on autoregressive token generation, requiring 200+ decoding steps per utterance. This makes them impractical for interactive applications where latency below 500ms is expected. We address this gap by reformulating zero-shot TTS as a single-step flow matching problem, reducing synthesis latency from 4.2s to 0.08s while maintaining comparable speaker similarity (cosine sim: 0.82 vs. 0.85).

**Why better**: å…·é«”æŒ‡å‡ºèª°åšäº†ä»€éº¼ï¼ˆVALL-E, NaturalSpeech 2ï¼‰ã€ç‚ºä»€éº¼ä¸å¤ å¥½ï¼ˆ200+ steps, high latencyï¼‰ã€gap çš„å¾Œæœï¼ˆimpractical for interactive appsï¼‰ã€ä½ çš„è§£æ³•å’Œæ•¸å­—ã€‚

---

### C. Contribution æ”¹å¯«

**Before âŒ**

Our contributions are as follows:

- We propose a novel framework for automatic speech recognition.  
- We conduct comprehensive experiments.  
- We achieve state-of-the-art results.

**Problems**: æ¯ä¸€æ¢éƒ½æ˜¯ activityï¼ˆæˆ‘åšäº†ä»€éº¼ï¼‰ï¼Œä¸æ˜¯ claimï¼ˆæˆ‘è­‰æ˜äº†ä»€éº¼ï¼‰ã€‚"Novel" å’Œ "comprehensive" æ˜¯ç©ºè©±ã€‚

**After âœ…**

Our contributions are:

- We propose CTC-Align, a non-autoregressive ASR model that decouples alignment prediction from token prediction, enabling 12Ã— faster inference than autoregressive baselines with \<1% WER degradation.  
- We show that alignment-conditioned decoding eliminates the "peaky" CTC distribution problem, improving WER by 8.3% relative on LibriSpeech-other compared to vanilla CTC.  
- We release pretrained models and training code for reproducibility.

**Why better**: æ¯æ¢éƒ½æœ‰ What \+ How much \+ Why it mattersã€‚Reviewer å¯ä»¥ç›´æ¥é©—è­‰ã€‚

---

### D. Method æè¿°æ”¹å¯«ï¼ˆWhy \> Howï¼‰

**Before âŒ**

We first extract features using a pretrained model. Then we feed the features into a transformer encoder. The output is passed through a linear layer to get the final prediction.

**Problems**: åªæœ‰ howï¼ˆæ­¥é©Ÿæµæ°´å¸³ï¼‰ï¼Œæ²’æœ‰ whyã€‚Reviewer çœ‹å®Œä¸çŸ¥é“ç‚ºä»€éº¼é€™æ¨£è¨­è¨ˆã€‚

**After âœ…**

We build on the observation that pretrained SSL features capture complementary information at different layers: lower layers encode acoustic details while upper layers encode linguistic content (Pasad et al., 2021). Rather than using only the final-layer representation, we introduce a learnable weighted sum across all layers, allowing the model to adaptively select the most relevant features for each downstream task. The fused representation is processed by a 6-layer Transformer encoder, chosen to balance model capacity with the 4-page Interspeech constraint on training compute.

**Why better**: æ¯å€‹è¨­è¨ˆé¸æ“‡éƒ½æœ‰ç†ç”±ã€‚è®€è€…ç†è§£ why weighted sumï¼ˆcomplementary infoï¼‰ã€why Transformerï¼ˆcapacityï¼‰ã€why 6 layersï¼ˆcompute constraintï¼‰ã€‚

---

### E. Results è§£è®€æ”¹å¯«

**Before âŒ**

As shown in Table 1, our method achieves the best performance on all datasets.

**Problems**: æ²’æœ‰ insightã€‚å“ªäº› datasetï¼Ÿå¥½å¤šå°‘ï¼Ÿç‚ºä»€éº¼å¥½ï¼Ÿ

**After âœ…**

Table 1 shows that CTC-Align outperforms all baselines on LibriSpeech test-clean (2.8% WER, âˆ’12.5% relative vs. Conformer-T) and test-other (6.9% WER, âˆ’11.5% relative). The improvement is more pronounced on the noisier test-other split, suggesting that the explicit alignment module is particularly beneficial when acoustic conditions are challenging. On WSJ, the gap narrows to 4.5% vs. 5.1% for the baseline, likely because WSJ's read speech provides clearer alignment cues even without our module.

**Why better**: æœ‰æ•¸å­—ã€æœ‰ relative improvementã€æœ‰ insightï¼ˆnoisier \= bigger gapï¼‰ã€ä¸»å‹•è§£é‡‹ negative/weaker resultï¼ˆWSJ gap narrowsï¼‰ã€‚

---

### F. Related Work æ”¹å¯«ï¼ˆå¾åˆ—èˆ‰åˆ°å®šä½ï¼‰

**Before âŒ**

Smith et al. (2023) proposed method A. Jones et al. (2024) proposed method B. Lee et al. (2024) proposed method C. Our method is different from these methods.

**Problems**: æµæ°´å¸³ã€æ²’æœ‰æ¯”è¼ƒã€"different" æ²’èªªæ€éº¼ differentã€‚

**After âœ…**

SSL-based ASR methods can be categorized by how they leverage pretrained representations. Fine-tuning approaches (Baevski et al., 2020; Hsu et al., 2021\) adapt the entire model to downstream data, achieving strong results but requiring substantial compute per task. Adapter-based methods (Thomas et al., 2022; Chen et al., 2023\) reduce trainable parameters by inserting lightweight modules, but they still depend on autoregressive decoding, limiting inference speed. Our work combines the parameter efficiency of adapters with non-autoregressive decoding, achieving comparable accuracy at 12Ã— lower latencyâ€”a trade-off not explored in prior work.

**Why better**: æŒ‰ä¸»é¡Œåˆ†çµ„ï¼ˆfine-tuning vs adapterï¼‰ã€æ¯é¡æŒ‡å‡ºå„ªç¼ºé»ã€æœ€å¾Œæ¸…æ¥šå®šä½è‡ªå·±çš„å·®ç•°å’Œç¨ç‰¹è²¢ç»ã€‚

---

### G. æ¨¡ç³Šè¡¨é”æ”¹å¯«ï¼ˆPrecisionï¼‰

| Before âŒ | After âœ… | å•é¡Œ |
| :---- | :---- | :---- |
| significantly outperforms | outperforms by 3.2% WER (p\<0.01) | ç„¡æ•¸æ“š â†’ æœ‰æ•¸æ“š |
| various datasets | 4 benchmark datasets (LS, WSJ, AISHELL, CV) | æ¨¡ç³Š â†’ å…·é«” |
| a large improvement | \+6.3 SUPERB score (12.8% relative) | å½¢å®¹è© â†’ æ•¸å­— |
| our method is efficient | RTF=0.03 on a single V100 GPU | ç©ºè©± â†’ å¯é‡åŒ– |
| we use a deep model | 12-layer Conformer (110M params) | æ¨¡ç³Š â†’ ç²¾ç¢º |
| extensive experiments | experiments on 4 datasets with 3 metrics | ç©ºè©± â†’ ç¯„åœ |
| SOTA results | lowest WER among non-AR models on LS test-other | éåº¦å®£ç¨± â†’ scoped claim |
| promising results | competitive with SOTA (within 0.3% WER) with 5Ã— speedup | æ¨¡ç³Š â†’ trade-off |
| our novel approach | our approach (åˆªæ‰ novel) | puffery â†’ è®“çµæœèªªè©± |
| clearly demonstrates | the results indicate / the data suggest | overclaim â†’ hedged |

---

### H. Transition æ”¹å¯«

**Before âŒ**

Our method performs well. And we also test on another dataset. The results are good.

**After âœ…**

While the results on LibriSpeech confirm our method's effectiveness on read English speech, a natural question is whether these gains transfer to more challenging conditions. To investigate this, we evaluate on CommonVoice, which includes accented and noisy recordings. As shown in Table 2, the improvements persist (+2.1% WER), though the margin decreasesâ€”suggesting that our alignment module's benefit is partially redundant with the robustness already captured by the SSL backbone.

**Why better**: æœ‰é‚è¼¯é€£æ¥ï¼ˆwhile â†’ natural question â†’ to investigate â†’ as shown â†’ suggestingï¼‰ã€æœ‰ insightã€æœ‰ honest analysisã€‚

---

### I. Conclusion æ”¹å¯«

**Before âŒ**

In this paper, we proposed a method for ASR. The experimental results show our method is effective. In the future, we will extend our method to more languages.

**After âœ…**

We presented CTC-Align, demonstrating that decoupling alignment from token prediction enables non-autoregressive ASR to close the gap with autoregressive models while providing 12Ã— inference speedup. Our analysis reveals that explicit alignment is most impactful under noisy conditions, suggesting a complementary role to SSL pretraining. Current limitations include degraded performance on code-switched speech, where alignment boundaries are ambiguous. Future work will explore multilingual alignment strategies that handle intra-utterance language switches.

**Why better**: æœ‰æ ¸å¿ƒ insightï¼ˆdecoupling \= keyï¼‰ã€æœ‰ findingï¼ˆnoisy \= most impactfulï¼‰ã€limitation å…·é«”ä¸”é™„ reasonã€future work å…·é«”ä¸”æ‰¿æ¥ limitationã€‚

---

## å¾ Best Paper å­¸ç²¾æº–è‹±æ–‡ï¼ˆEFFUSE, Interspeech 2024 Best Paperï¼‰

### å‹•è©ç²¾æº–åº¦ Cheat Sheet

| å ´æ™¯ | âŒ æ¨¡ç³Š | âœ… ç²¾æº– |
| :---- | :---- | :---- |
| æå‡ºæ–¹æ³• | "We use/apply" | "We propose... that employs..." |
| æ¯”è¼ƒçµæœ | "is better than" | "outperforms... while decreasing..." |
| æè¿°è¿‘ä¼¼ | "approximates" | "mimics"ï¼ˆæ›´ç”Ÿå‹•ä¸”ç²¾ç¢ºï¼‰ |
| å»ºç«‹å‡è¨­ | "So we think..." | "Thus, we hypothesize that..." |

### æ•¸å­—è¡¨é”é»ƒé‡‘æ³•å‰‡

- æ°¸é åŒæ™‚çµ¦ **absolute \+ relative** improvement  
- ä¾‹ï¼šâŒ "significantly improves" â†’ âœ… "reduces CER by 4.5 absolute (20% relative)"

### 4 é çµæ§‹æŠ€å·§

- ä¸éœ€è¦ç¨ç«‹ Related Work section â€” æ•´åˆé€² Introduction  
- å¦‚æœ method åŸºæ–¼å‡è¨­ï¼ŒèŠ±åŠé åš pilot study å…ˆè­‰æ˜å‡è¨­ï¼ˆæ¯”å£é ­ "we hypothesize" æœ‰åŠ› 100 å€ï¼‰  
- Table caption è¦å¯«çµè«–ï¼Œä¸åªæè¿°å…§å®¹

### Contribution åˆ—è¡¨å¹³è¡Œçµæ§‹

(1) we extensively explore...    â† èª¿æŸ¥

(2) we propose a novel...       â† æå‡º

(3) we demonstrate that...      â† é©—è­‰

ä¸‰æ¢æ–‡æ³•çµæ§‹ä¸€è‡´ã€æŠ½è±¡ç¨‹åº¦ä¸€è‡´ã€é‚è¼¯éé€²ã€‚

---

## ç¬¬åä¸€èª²ï¼šä¸­æ–‡æ¯èªè€… 30 å€‹å¸¸è¦‹å­¸è¡“è‹±æ–‡éŒ¯èª¤

### ğŸ¯ ç‚ºä»€éº¼é€™å¾ˆé‡è¦

Language polishing ä¸åªæ˜¯ cosmeticâ€”â€”å¯«ä½œå“è³ªå·®æœƒè®“ reviewer è³ªç–‘ soundnessã€‚Schulzrinne èªªï¼šã€ŒConsider the rules as mental rumble strips.ã€

### å…­å¤§é¡éŒ¯èª¤é€ŸæŸ¥è¡¨

**A. å† è©ï¼ˆæœ€å¤§é‡ç½å€ï¼‰**

- å¯æ•¸åè©é¦–æ¬¡å‡ºç¾ç”¨ a/anï¼šâŒ "We propose method" â†’ âœ… "We propose **a** method"  
- å·²å®šç¾©å°è±¡ç”¨ theï¼šâŒ "model achieves" â†’ âœ… "**the** model achieves"  
- æ³›æŒ‡æ¦‚å¿µä¸åŠ  theï¼šâŒ "**The** speech recognition is..." â†’ âœ… "Speech recognition is..."  
- ç¸®å¯«ä¸åŠ  theï¼šâŒ "**The** ASR model" â†’ âœ… "ASR model"ï¼ˆé™¤éæŒ‡ the ASR-based modelï¼‰

**B. å‹•è©**

- ä¸»å‹• \> è¢«å‹•ï¼šâŒ "It is shown that..." â†’ âœ… "We show that..." / "Our results show..."  
- å¼·å‹•è© \> å¼±åè©ï¼šâŒ "make an assumption" â†’ âœ… "assume"ï¼›âŒ "perform training" â†’ âœ… "train"  
- et al. ç”¨è¤‡æ•¸å‹•è©ï¼šâŒ "Smith et al. shows" â†’ âœ… "Smith et al. **show**"  
- æ™‚æ…‹ï¼šæè¿°æœ¬æ–‡ \= ç¾åœ¨å¼ï¼›å¼•ç”¨ä»–äººå·²å®Œæˆå·¥ä½œ \= éå»å¼

**C. å¥å‹**

- ä¸€å¥ä¸è¶…é 30 å­—ï¼Œå¦å‰‡æ‹†é–‹  
- Dangling modifierï¼šâŒ "Using X, the WER was reduced." â†’ âœ… "Using X, **we** reduced the WER."  
- é¿å… "There is/are" é–‹é ­  
- åè©å †ç–Š â‰¤3ï¼šâŒ "speech recognition error rate reduction method" â†’ âœ… "a method for reducing the error rate"

**D. ç”¨è©**

- åˆª "novel"ï¼ˆè®“æ–¹æ³•è‡ªå·±å±•ç¾æ–°ç©æ€§ï¼‰  
- "significantly" å¿…é ˆé™„ p-value æˆ–å…·é«”æ•¸å­—  
- "utilize" â†’ "use"  
- "etc." â†’ å…·é«”åˆ—èˆ‰  
- "various/several" â†’ å…·é«”æ•¸å­—  
- whichï¼ˆéé™å®šï¼ŒåŠ é€—è™Ÿï¼‰vs thatï¼ˆé™å®šï¼Œä¸åŠ é€—è™Ÿï¼‰

**E. æ¨™é»æ ¼å¼**

- Oxford commaï¼šâœ… "A, B\*\*,\*\* and C"  
- æ•¸å­— â‰¤10 æ‹¼å‡ºä¾†ï¼ˆé™¤äº†è·Ÿå–®ä½ä¸€èµ·ï¼‰  
- æ•¸å­—å’Œå–®ä½ä¹‹é–“æœ‰ç©ºæ ¼ï¼šâœ… "16 kHz"ã€"0.5 s"  
- kHz çš„ k å°å¯«  
- å¼•ç”¨ç”¨ä½œè€…åï¼šâŒ "\[1\] shows" â†’ âœ… "Smith et al. \[1\] show"

**F. é‚è¼¯é€£æ¥**

- é€£çºŒå¥å­å¿…é ˆæœ‰é‚è¼¯é€£æ¥è©ï¼ˆhowever / thus / in contrast / specificallyï¼‰  
- ä¸ä»¥ "And" é–‹é ­  
- this/that å¾Œé¢è·Ÿåè©ï¼šâŒ "This is important." â†’ âœ… "This **finding** is important."

### ğŸ”§ 5 åˆ†é˜è‡ªæŸ¥ï¼ˆç”¨æœå°‹ï¼‰

grep \-i "novel" â†’ è€ƒæ…®åˆªé™¤

grep \-i "utilize" â†’ æ”¹æˆ use

grep \-i "etc\\." â†’ æ”¹æˆå…·é«”åˆ—èˆ‰

grep \-i "significant" â†’ ç¢ºèªæœ‰æ•¸å­—

grep \-i "there is\\|there are" â†’ æ”¹ä¸»å‹•å¥

grep \-i "in this paper" â†’ ä¸è¶…é 2 æ¬¡

ğŸ’¡ **æ•™å­¸å»ºè­°**ï¼šè®“å­¸ç”Ÿäº¤ç¨¿å‰è·‘ä¸€éé€™ 6 å€‹ grepï¼Œ5 åˆ†é˜å…§å¯ä»¥æŠ“å‡º 80% å¸¸è¦‹å•é¡Œã€‚å† è©å•é¡Œå»ºè­°å¦å¤–è·‘ä¸€éå°ˆé–€çš„ã€Œå† è©æ ¡ç¨¿ã€â€”â€”è®€æ¯å€‹åè©ï¼Œå•è‡ªå·±ï¼šéœ€è¦ a/the/é›¶å† è©ï¼Ÿ

---

## é™„éŒ„ï¼šæ¨è–¦è³‡æº

1. Simon Peyton Jones, "How to Write a Great Research Paper" â€” [Microsoft Research](https://www.microsoft.com/en-us/research/academic-program/write-great-research-paper/)  
2. Derek Dreyer, "How to Write Papers So People Can Read Them" â€” [YouTube](https://www.youtube.com/watch?v=PM1Atui30qU)  
3. Henning Schulzrinne, "Writing Technical Articles" â€” [Columbia CS](https://www.cs.columbia.edu/~hgs/etc/writing-style.html)  
4. Michael Ernst, "How to Write a Technical Paper" â€” [UW](https://homes.cs.washington.edu/~mernst/advice/write-technical-paper.html)  
5. ACL Rolling Review Reviewer Guidelines â€” [ARR](https://aclrollingreview.org/reviewerguidelines)

---

## âš ï¸ Interspeech 2026 æ–°åˆ¶åº¦ï¼šLong Paper Track

Interspeech 2026ï¼ˆSydney, 9/28-10/1ï¼‰é¦–æ¬¡å¼•å…¥ Long Paper trackï¼š

- **Regular Paper**ï¼š4 pages \+ 1 refï¼ˆå‚³çµ±æ ¼å¼ï¼‰  
- **Long Paper**ï¼š8 pages \+ 2 ref/ackï¼ˆ**æ–°å¢**ï¼Œç›®æ¨™æ¥å—ç‡ \<30%ï¼‰  
- æˆªæ­¢æ—¥ç›¸åŒï¼š2/25 AoEï¼ˆPaper Update: 3/04 AoEï¼‰  
- Long Paper å®šä½ï¼šextended, high-impact contributions

### å»ºè­°

- å¤šæ•¸å­¸ç”Ÿæ‡‰æŠ• **Regular 4-page**ï¼ˆé¦–æ¬¡æŠ•ç¨¿ï¼Œ4 é é™åˆ¶åè€Œæ˜¯ä¿è­·ï¼‰  
- åªæœ‰ç ”ç©¶æ·±åº¦è¶³å¤ ï¼ˆæœ‰ pilot study \+ å®Œæ•´ ablation \+ error analysisï¼‰æ‰è€ƒæ…® Long Paper  
- æ–°å¢ä¸»é¡Œ "Generative AI for Speech and Language Processing" é©åˆ LALM ç›¸é—œç ”ç©¶

